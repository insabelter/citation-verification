{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Model Prompting\n",
    "\n",
    "This notebook provides code for manual batched prompting of the OpenAI models. \n",
    "The script 'prompt_openai_model.py' provides a shortened version of this code which can be run via console (compatible with nohup command)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%run ./04_prompt_creation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-3.5-turbo-0125\", \"gpt-4.1-nano-2025-04-14\", \"gpt-4.1-mini-2025-04-14\", \"gpt-4.1-2025-04-14\"]\n",
    "model = models[3]\n",
    "\n",
    "chunking = \"1024_20\"\n",
    "only_text = True\n",
    "ai_prompt = False\n",
    "suit_prompt = True\n",
    "\n",
    "path = f\"../data/dfs/{'only_text_' if only_text else ''}{chunking}/ReferenceErrorDetection_data_with_chunk_info.pkl\"\n",
    "print(path)\n",
    "\n",
    "# read the dataframe from a pickle file\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Batch Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chars = []\n",
    "\n",
    "def create_batch_files(df, model, number_files=1, ignore_ids=[], ai_prompt=False, suit_prompt=False):\n",
    "    global prompt_chars\n",
    "\n",
    "    output_dir = f\"../data/batch_files/{'only_text_' if only_text else ''}{chunking}/{model}{'/AI_prompt/' if ai_prompt else ''}{'/suit_prompt/' if suit_prompt else ''}\"\n",
    "    # Empty the folder if it exists\n",
    "    if os.path.exists(output_dir):\n",
    "        for filename in os.listdir(output_dir):\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_files = []\n",
    "    for i in range(number_files):\n",
    "        output_file = os.path.join(output_dir, f\"prompt_batch_{i}.jsonl\")\n",
    "        # If the file already exists, empty it\n",
    "        open(output_file, \"w\").close()\n",
    "        output_files.append(output_file)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['Reference Article Downloaded'] == 'Yes' and index not in ignore_ids:\n",
    "            prompt = create_prompt(row)\n",
    "            prompt_char = len(prompt)\n",
    "            prompt_chars.append(prompt_char)\n",
    "\n",
    "            json_sequence = {\n",
    "                \"custom_id\": f\"request-{index}\", \n",
    "                \"method\": \"POST\", \n",
    "                \"url\": \"/v1/chat/completions\", \n",
    "                \"body\": {\n",
    "                    \"model\": model, \n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    \"temperature\": 0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "            output_file = output_files[index % number_files]\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(json.dumps(json_sequence) + \"\\n\")\n",
    "                \n",
    "    # Remove empty output files from list\n",
    "    output_files = [file for file in output_files if os.path.getsize(file) > 0]\n",
    "    \n",
    "    return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"../data/batch_responses/{'only_text_' if only_text else ''}{chunking}{'/AI_prompt/' if ai_prompt else ''}{'/suit_prompt/' if suit_prompt else ''}\", exist_ok=True)\n",
    "responses_dict_path = f\"../data/batch_responses/{'only_text_' if only_text else ''}{chunking}{'/AI_prompt/' if ai_prompt else ''}{'/suit_prompt/' if suit_prompt else ''}/{model}_responses_dict_batch.json\"\n",
    "\n",
    "responses_dict = {}\n",
    "try:\n",
    "    with open(responses_dict_path, 'r') as file:\n",
    "        responses_dict = json.load(file)\n",
    "    ids_to_ignore = [int(key) for key in responses_dict.keys()]\n",
    "except FileNotFoundError:\n",
    "    ids_to_ignore = []\n",
    "\n",
    "print(ids_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file_paths = create_batch_files(df, model, 5, ids_to_ignore, ai_prompt=ai_prompt, suit_prompt=suit_prompt)\n",
    "batch_file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Model Prompting\n",
    "\n",
    "Only one batch request is sent at once and its completion is awaited before starting the next batch, because otherwise the OpenAI API token limit can be easily exceeded which leads to the cancellation of all requests and a required refresh time before trying again.\n",
    "\n",
    "This can make prompting the OpenAI models slow, but this sequential batched processing is still faster (and cheaper) than prompting each citation separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An OpenAI API key needs to be generated and put into a file called \"open_ai_key.txt\" for the following code to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of open_ai_key.txt into a variable\n",
    "with open('../open_ai_key.txt', 'r') as file:\n",
    "    open_ai_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch(batch_id, client):\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    print(f\"{batch_id} - Current status: {batch.status}\")\n",
    "    if (batch.status == 'in_progress'):\n",
    "        print(f\"{batch.request_counts.completed} / {batch.request_counts.total} completed\")\n",
    "\n",
    "    if batch.status == 'completed' or batch.status == 'failed':\n",
    "        return batch\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_files = []\n",
    "batch_creation_responses = []\n",
    "batches = []\n",
    "\n",
    "client = OpenAI(api_key=open_ai_key)\n",
    "\n",
    "def prompt_model_in_batches():\n",
    "    global batch_input_files\n",
    "    global batch_creation_responses\n",
    "    global batches\n",
    "\n",
    "    for batch_file_path in batch_file_paths:\n",
    "        # Creating input file\n",
    "        if os.stat(batch_file_path).st_size == 0:\n",
    "            print(f\"Skipping empty file: {batch_file_path}\")\n",
    "            continue\n",
    "        batch_input_file = client.files.create(\n",
    "            file=open(batch_file_path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        print(batch_input_file)\n",
    "        batch_input_files.append(batch_input_file)\n",
    "\n",
    "        # Starting batch job\n",
    "        batch_input_file_id = batch_input_file.id\n",
    "        batch_creation_response = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(\"Started: \" + batch_creation_response.id)\n",
    "\n",
    "        time.sleep(5)\n",
    "        # Check the status of the created batch until it is completed\n",
    "        while True:\n",
    "            batch_id = batch_creation_response.id\n",
    "            batch = check_batch(batch_id, client)\n",
    "            if batch:\n",
    "                if batch.status == \"failed\":\n",
    "                    return\n",
    "                elif batch.status == \"completed\":\n",
    "                    batches.append(batch)\n",
    "                    break\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt_model_in_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Open Batches\n",
    "\n",
    "The following code can be used to view currently running batches and cancel them if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=open_ai_key)\n",
    "\n",
    "current_millis = int(time.time())\n",
    "recently = current_millis - 24 * 60 * 60\n",
    "\n",
    "open_batches = client.batches.list()\n",
    "relevant_open_batches = [batch for batch in open_batches if batch.created_at >= recently]\n",
    "in_progress_batch_ids = [batch.id for batch in relevant_open_batches if batch.status == 'in_progress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(relevant_open_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.content(relevant_open_batches[0].output_file_id).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.batches.cancel(\"batch_67e3cf592eb081908cd64e5e1dc55fa0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting for Batch Completion\n",
    "\n",
    "The following function can be used to wait for an existing batch to be finished by polling for the batch status with a given interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_batch_completion(batch_id, client, interval=10):\n",
    "    while True:\n",
    "        batch = check_batch(batch_id, client)\n",
    "        if batch != None:\n",
    "            return batch\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = wait_for_batch_completion(\"batch_68efa5f819d4819081b526560a95616b\", client, interval=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = check_batch(\"batch_68efa5f819d4819081b526560a95616b\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model_responses\n",
    "\n",
    "The responses of the completed batches are saved into a json file in the folder 'data/batch_responses'.\n",
    "Additionally, the responses are added to the dataframe to the corresponding data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save responds of completed batches\n",
    "for batch in batches:\n",
    "    if batch.status != \"completed\":\n",
    "        continue\n",
    "    model_responses = client.files.content(batch.output_file_id).text\n",
    "\n",
    "    # Parse the model_responses into a list of objects\n",
    "    responses_list = [json.loads(line) for line in model_responses.splitlines()]\n",
    "    # print(responses_list)\n",
    "\n",
    "    try:\n",
    "        for response in responses_list:\n",
    "            responses_dict[int(response['custom_id'].split('-')[1])] = response\n",
    "            responses_dict = dict(sorted(responses_dict.items(), key=lambda item: int(item[0])))\n",
    "    except NameError:\n",
    "        responses_dict = {int(response['custom_id'].split('-')[1]): response for response in responses_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save responses_dict to a JSON file\n",
    "with open(responses_dict_path, 'w') as file:\n",
    "    json.dump(responses_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save responds to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe to store the responses\n",
    "if 'Model Classification' not in df.columns:\n",
    "    df['Model Classification'] = None\n",
    "\n",
    "# Iterate through the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    if row['Reference Article Downloaded'] == 'Yes':\n",
    "        i = index\n",
    "        if i not in responses_dict:\n",
    "            i = str(i)\n",
    "        model_response = responses_dict[i]['response']['body']['choices'][0]['message']['content']\n",
    "        \n",
    "        # Save the response to the new column\n",
    "        df.at[index, 'Model Classification'] = model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_path = f\"../data/dfs/{'only_text_' if only_text else ''}{chunking}/{model}/{'AI_prompt/' if ai_prompt else ''}\"\n",
    "os.makedirs(dfs_path, exist_ok=True)\n",
    "df.to_pickle(f\"{dfs_path}ReferenceErrorDetection_data_with_prompt_results.pkl\")\n",
    "df.to_excel(f\"{dfs_path}ReferenceErrorDetection_data_with_prompt_results.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
