{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Paper Chunk Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference papers are chunked and indexed to then retrieve the three most relevant (similar) chunks to the citation statement. LlamaIndex is used to easily implement this full process.\n",
    "\n",
    "LlamaIndex:\n",
    "- Github: https://github.com/run-llama/llama_index\n",
    "- Documentation: https://docs.llamaindex.ai/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the xlsx data into a pandas dataframe\n",
    "df = pd.read_excel(f\"../data/ReferenceErrorDetection_data_extended_annotation.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Text from Reference Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'txt' if PBTE was used on TEI documents, otherwise ''\n",
    "extension = \"txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(reference_article_id):\n",
    "    # Construct the file path pattern using the Reference Article ID of the first entry\n",
    "    file_pattern = f\"../data/extractions/{'only_text/' if extension == 'txt' else ''}{reference_article_id}*.{extension}\"\n",
    "\n",
    "    # Find the file that matches the pattern\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    if file_list:\n",
    "        file_path = file_list[0]\n",
    "        return file_path\n",
    "    else: \n",
    "        print(\"No matching file found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_text(reference_article_id):\n",
    "    # Get the file path\n",
    "    file_path = get_file_path(reference_article_id)\n",
    "    \n",
    "    if file_path:\n",
    "        if extension == \"txt\":\n",
    "            # Read the text file\n",
    "            with open(file_path, 'r') as file:\n",
    "                reference_text = file.read()\n",
    "            return reference_text\n",
    "\n",
    "        elif extension == \"xml\":\n",
    "            # Parse the XML file\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Extract the text content from the XML file\n",
    "            reference_text = ''.join(root.itertext())\n",
    "            return reference_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting OpenAI key\n",
    "An OpenAI API key needs to be generated and put into a file called \"open_ai_key.txt\" for the following code to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of open_ai_key.txt into a variable\n",
    "with open('../open_ai_key.txt', 'r') as file:\n",
    "    open_ai_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading or Generating Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = \"text-embedding-3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = OpenAIEmbedding(model=model_embeddings, api_key=open_ai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(reference_text, chunk_size, chunk_overlap):\n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "            OpenAIEmbedding(model=model_embeddings, api_key=open_ai_key)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # run the pipeline\n",
    "    nodes = pipeline.run(documents=[Document(text=reference_text)])\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_index(article_id, reference_text, chunk_size, chunk_overlap, only_checking=False):\n",
    "    index_path = f\"../data/vector_indices/{'only_text_' if extension == 'txt' else ''}{chunk_size}_{chunk_overlap}/{article_id}/\"\n",
    "    index = None\n",
    "    if only_checking:\n",
    "        if os.path.exists(index_path) and os.listdir(index_path):\n",
    "            print(article_id + \": Index exists.\")\n",
    "            return True\n",
    "        \n",
    "    assert reference_text is not None and reference_text != '', \"Reference text cannot be None or empty.\"\n",
    "\n",
    "    try:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "        index = load_index_from_storage(storage_context)\n",
    "        print(article_id + \": Loaded existing index.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(article_id + \": Creating a new index.\")\n",
    "        try: \n",
    "            index = create_index(reference_text, chunk_size, chunk_overlap)\n",
    "            index.storage_context.persist(persist_dir=index_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(article_id + \": Failed to create index.\")\n",
    "            print(reference_text)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Indices for all Reference Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1024\n",
    "chunk_overlap = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    if row['Reference Article Downloaded'] == 'Yes':\n",
    "        reference_article_id = row['Reference Article ID']\n",
    "        if reference_article_id:\n",
    "            reference_text = get_reference_text(reference_article_id)\n",
    "            index = load_or_create_index(reference_article_id, reference_text, chunk_size, chunk_overlap, only_checking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Top 3 Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "def get_top_k_similar_chunks(statement, index, k=3):\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=k,\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(statement)\n",
    "    return retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_similar_chunks(doc_ids, reference_id, chunk_size, chunk_overlap):\n",
    "    file_path = f\"../data/similar_chunks/{'only_text_' if extension == 'txt' else ''}{chunk_size}_{chunk_overlap}/{reference_id}.json\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(doc_ids, file)\n",
    "\n",
    "def load_similar_chunks(reference_id, chunk_size, chunk_overlap):\n",
    "    file_path = f\"../data/similar_chunks/{'only_text_' if extension == 'txt' else ''}{chunk_size}_{chunk_overlap}/{reference_id}.json\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        doc_ids = json.load(file)\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Document IDs and Contents of Retrieved Chunks to the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_ids(response):\n",
    "    doc_ids = []\n",
    "    for node in response:\n",
    "        doc_ids.append(node.dict()['node']['id_'])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_k_chunk_ids(df, chunk_size, chunk_overlap, k=3):\n",
    "    for _, row in df.iterrows():\n",
    "        if row['Reference Article Downloaded'] == 'Yes':\n",
    "            reference_article_id = row['Reference Article ID']\n",
    "            print(f\"------ Starting {reference_article_id} ------\")\n",
    "            \n",
    "            # Try to load similar chunks first\n",
    "            try:\n",
    "                doc_ids = load_similar_chunks(reference_article_id, chunk_size, chunk_overlap)\n",
    "                print(\"Loaded similar chunks successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                # Load reference text and create chunks\n",
    "                reference_text = get_reference_text(reference_article_id)\n",
    "                \n",
    "                # Load or create index\n",
    "                index = load_or_create_index(reference_article_id, reference_text, chunk_size, chunk_overlap)\n",
    "                \n",
    "                # Get the statement and retrieve top chunks\n",
    "                statement = row[\"Corrected Statement\"]\n",
    "\n",
    "                print(\"Receiving top chunks\")\n",
    "\n",
    "                try:\n",
    "                    response = get_top_k_similar_chunks(statement, index, k)\n",
    "                    doc_ids = get_doc_ids(response)\n",
    "                    \n",
    "                    # Save the top chunks\n",
    "                    print(\"Saving top chunks\")\n",
    "                    save_similar_chunks(doc_ids, reference_article_id, chunk_size, chunk_overlap)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\"Failed to get top chunks.\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_top_k_chunk_ids(df, chunk_size, chunk_overlap, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"../data/dfs/{'only_text_' if extension == 'txt' else ''}{chunk_size}_{chunk_overlap}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# df2 = pd.read_pickle(os.path.join(output_dir, f\"ReferenceErrorDetection_data_with_chunk_info.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_k_chunk_ids_and_texts_to_df(df, chunk_size, chunk_overlap, k=3):\n",
    "    doc_ids_list = []\n",
    "    doc_texts_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['Reference Article Downloaded'] == 'Yes':\n",
    "            reference_article_id = row['Reference Article ID']\n",
    "            \n",
    "            print(f\"------ Starting {reference_article_id} ------\")\n",
    "\n",
    "            # load index\n",
    "            index_path = f\"../data/vector_indices/{'only_text_' if extension == 'txt' else ''}{chunk_size}_{chunk_overlap}/{reference_article_id}/\"\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "\n",
    "            # load similar chunks\n",
    "            doc_ids = load_similar_chunks(reference_article_id, chunk_size, chunk_overlap)\n",
    "            doc_texts = [index.docstore.docs[doc_id].text for doc_id in doc_ids]\n",
    "\n",
    "            # add to lists\n",
    "            doc_ids_list.append(doc_ids)\n",
    "            doc_texts_list.append(doc_texts)\n",
    "        else:\n",
    "            doc_ids_list.append(None)\n",
    "            doc_texts_list.append(None)\n",
    "    \n",
    "    df[f'Top_{k}_Chunk_IDs'] = doc_ids_list\n",
    "    df[f'Top_{k}_Chunk_Texts'] = doc_texts_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = add_top_k_chunk_ids_and_texts_to_df(df, chunk_size, chunk_overlap, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "output_dir = f\"../data/dfs/{'only_text_' if extension == 'txt' else ''}{chunk_size}_{chunk_overlap}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to a pickle file\n",
    "df2.to_pickle(os.path.join(output_dir, f\"ReferenceErrorDetection_data_with_chunk_info.pkl\"))\n",
    "\n",
    "# Save the DataFrame to a excel file\n",
    "df2.to_excel(os.path.join(output_dir, f\"ReferenceErrorDetection_data_with_chunk_info.xlsx\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
