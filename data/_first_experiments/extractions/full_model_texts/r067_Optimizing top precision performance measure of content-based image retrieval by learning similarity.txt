I. INTRODUCTION
In the problem of content-based image retrieval, the two basic steps of the content-based image retrieval are image visual feature extraction, and the similarity calculation. Recently, the similarity learning problem has been a hot topic in the field of machine learning. This problem tries to learn a parameterized similarity function from a training set, so that a targeted performance can be optimized  [1] . To evaluate the performance of a retrieval system, common measures for retrieval performance are taken, including the area under the receiver operating characteristic (ROC) curve, and the top precision value  [2] ,  [3] ,  [4] . The top precision measure is defined as the portion of the top-ranked relevant images out of all the relevant database images. The top-ranked relevant images are further defined as the database images which are ranked before the first ranked irrelevant database image. Top precision is an important performance measure for retrieval problems, however, all the existing similarity learning methods ignore the performance measures, and the resulted similarity function cannot give an optimal performance measure of top precision. To fill the gap between similarity learning and top precision optimization in the problem of content-based image retrieval, we propose a problem of top precision-optimal similarity learning, together with a corresponding solution using a novel algorithm. Our contributions in this paper are divided into two parts, which are given as followed. The proposed problem is to learn a similarity function to compare a query image against a database image. With the similarity scores between query and database images, when retrieval task is performed, we can obtain a maximum top precision measure. The problem is to adjust the parameters of the similarity function, so that the top precision can be maximized. The proposed novel similarity function learning algorithm is to maximize the top precision over a training set of query images and a database of images. We design the similarity function as a linear function of the two feature vectors of a query image and a database image. To learn the parameter matrix of the similarity function, we propose to maximize the top precision for each query, and parallelly minimize the squared Frobenius norm of the parameter matrix in order to prevent the emergence of the over-fitting problem. The optimization problem is modeled by a hinge loss and solved as a quadratic programming. The rest parts of this paper are organized as followed. In section II, we introduce the proposed similarity learning algorithm. In section III, we give the experimental results of the proposed method. In section IV, we give the conclusion of this paper with future works.

II. PROPOSED SIMILARITY LEARNING METHOD
We assume we have an image database of m images, D = {x 1 , • • • , x m }, where x j ∈ R d is the feature vector of the jth database image, a set of query images Q = {z 1 , • • • , z n }, where z i ∈ R d is the feature vector of the i-th query image, and a relevance matrix Y = [y ij ] ∈ {1, 0} n×m , where y ij = 1 if z i is relevant to x j , and 0 otherwise. We define a similarity function for a query z and a database image x parameterize by a matrix W ∈ R d×d , s(z, x) = z ⊤ W x. (1) The first ranked irrelevant database image of a query z i , denoted as top irrelevant, is defined as the irrelevant database image with the largest similarity to the query, x φi , where φ i = arg max k:y ik =0 s(z i , x k ). (2) Top precision measure of a query z i is defined as the portion of the relevant images ranked before the top irrelevant image, T op P recision(z i ) = |{x j : y ij = 1, and s(z i , x j ) > s(z i , x φi )}| |{x j : y ij = 1}| . (3) To obtain a good performance measure of top precision, we hope that for any pair of query and relevant database image, (z i , x j ) i,j:yij =1 , their similarity (z i , x j ) is larger than the similarity to the top irrelevant database image plus a margin, s(z i , x φi ) + 1, s(z i , x j ) > max k:y ik =0 s(z i , x k ) + 1, ⇒ 0 > max k:y ik =0 s(z i , x k ) -s(z i , x j ) + 1, ∀ i, j : y ij = 1. (4) To maximize the top precision, we propose to learn the similarity function parameter matrix W by penalizing the case which doesn't satisfies this condition, 0 ≤ max k:y ik =0 s(z i , x k ) -s(z i , x j ) + 1.∀ i, j : y ij = 1. ( 5 ) Thus the loss function for each (z i , x j ) i,j:yij =1 is, ℓ(z i , x j ; W ) = max 0, max k:y ik =0 s(z i , x k ) -s(z i , x j ) + 1 = max 0, max k:y ik =0 z ⊤ i W (x k -x j ) + 1 . (6) The optimization problem is modeled as, min W,ξij |i,j:y ij =1    1 2 W 2 F + C i,j:yij =1 ξ ij    , s.t. ∀i, j : y ij = 1, ξ ij ≥ 0, ξ ij ≥ z ⊤ i W (x k -x j ) + 1, (7) where ξ ij is slack variable of (6), 1 2 W 2 F is the squared Frobenius norm of W to prevent the over-fitting problem, and C is the tradeoff parameter. The Lagrange function of (  7 ) is L = 1 2 W 2 F + C i,j:yij =1 ξ ij - i,j:yij =1 α ij ξ ij - i,j,k:yij =1,y ik =0 β ijk ξ ij -z ⊤ i W (x k -x j ) -1 , (8) where α ij ≥ 0 is the Lagrange multiplier of the constraint ξ ij ≥ 0, and β ijk ≥ 0 is the Lagrange multiplier of the constraint ξ ij ≥ z ⊤ i W (x k -x j ) + 1. The dual form of the optimization problem of (  7 ) is given as follows, max αij |i,j:y ij =1 ,β ijk | i,j,k:y ij =1,y ik =0 min W,ξij |i,j:y ij =1 L s.t.α ij ≥ 0, ∀i, j : y ij = 1, β ijk ≥ 0, ∀i, j, k : y ij = 1, y ik = 0. (9) By setting the derivative of L with regard to ξ ij to zero, we have ∂L ∂ξ ij = C -α ij - k:y ik =0 β ijk = 0, ⇒ α ij = C - k:y ik =0 β ijk ≥ 0, ⇒ C ≥ k:y ik =0 β ijk . (10) By substituting  (10)  to L, we have L = 1 2 W 2 F - i,j,k:yij =1,y ik =0 β ijk z ⊤ i W (x j -x k ) + i,j,k:yij =1,y ik =0 β ijk . (11) By setting the derivative of L with regard to W to zero, we have ∂L ∂W = W - i,j,k:yij =1,y ik =0 β ijk z i (x j -x k ) ⊤ = 0 ⇒ W = i,j,k:yij =1,y ik =0 β ijk z i (x j -x k ) ⊤ . (12) By substituting W of (  12 ) to L of (11), we have L = - 1 2 i,j,k:yij =1,y ik =0 i ′ ,j ′ ,k ′ :y i ′ j ′ =1,y i ′ k ′ =0 β ijk β i ′ j ′ k ′ z ⊤ i z i ′ (x j ′ -x k ′ ) ⊤ (x j -x k ) + i,j,k:yij =1,y ik =0 β ijk . (13) This is a quadratic function of the multiplier variables, β ijk | i,j,k:yij =1,y ik =0 . The optimization problem of (  9 ) is transferred to max β ijk | i,j,k:y ij =1,y ik =0    - 1 2 i,j,k:yij =1,y ik =0 i ′ ,j ′ ,k ′ :y i ′ j ′ =1,y i ′ k ′ =0 β ijk β i ′ j ′ k ′ z ⊤ i z i ′ (x j ′ -x k ′ ) ⊤ (x j -x k ) + i,j,k:yij =1,y ik =0 β ijk    , s.t. C ≥ k:y ik =0 β ijk , and β ijk ≥ 0, ∀ i, j, k : y ij = 1, y ik = 0. ( 14 ) This is a quadratic programming (QP) problem, and we can use a standard active set algorithm to solve this problem. After the β ijk | i,j,k:yij =1,y ik =0 are solved, we can recover W from  (12) .

III. EXPERIMENTS


A. Experimental protocol
In this experiment, we used two benchmark image databases, which are Indoor image database  [5] , and Cal-tech256 image database  [6] . Then total number of images/ number classes of these two databases are 15,620/ 67 and 30,670/ 256. Given a database, we first split it to a set of queries and a set of database images, and these queries are further split into a training set and a test set. We apply the proposed similarity learning algorithm to the training set of queries and the database images to learn the similarity function. Then we use the similarity function to conduct the retrieval task for the test set of queries. The performance is evaluated by top precision measure.

B. Experimental results
We compare our proposed maximum top precision similarity learning method (MTPS), to several state-of-the-art similarity learning methods, online algorithm for scalable image similarity (OASIS)  [7] , online multiple kernel similarity (OMKS)  [8] , Boosted distance (BD)  [9] , stochastic intersection kernel machine (SIKMA)  [10] , and visuality-preserving distance metric (VPDM)  [11] . The results of the comparison over the four benchmark databases are given in Figure  1 . It is obvious that the proposed algorithm, MTPS, outperforms the compared similarity learning algorithms over both the two benchmark databases. In the results over the Caltech256, the top precisions of almost all the compared algorithms are below 0.16, while our algorithms, MTPS, have top precisions higher than 0.18. In addition, we test the running time of the proposed algorithm over the two benchmark databases, displayed in Figure  2 . Our proposed algorithm MTPS exhibits shorter running time than others, except for the OASIS and OMKS, suggesting that MTPS provides a comparably efficient way against the problem, and shows a promising improvement space for the future work.

IV. CONCLUSION AND FUTURE WORKS
In this paper we study the problem of similarity learning for content-based image retrieval. The critical novelty of this work is it learns the similarity to maximize the top precision measure over the training set. We model the learning problem as a support top irrelevant database image weighting problem to obtain the optimal similarity function. The experiments over some benchmark databases show its advantages over other similarity learning methods. In the future, we will investigate to use some other similarity function as similarity measure instead of linear function, such as Bayesian network  [12] ,  [13] ,  [14] , and also to develop novel algorithms of other machine learning problems and applications besides similarity learning, to maximize top precision measure, such as importance sampling  [15] ,  [16] ,  [17] , portfolio choices  [18] ,  [19] , multimedia technology  [20] ,  [21] ,  [22] ,  [23] ,  [24] ,  [25] ,  [26] ,  [27] ,  [28] ,  [29] , computational biology  [30] ,  [31] ,  [32] ,  [33] ,  [34] , big data processing  [35] ,  [36] ,  [37] ,  [38] ,  [39] , computer vision  [40] ,  [41] ,  [42] ,  [43] ,  [44] ,  [45] ,  [46] ,  [47] ,  [48] ,  [49] ,  [50] ,  [51] ,  [52] ,  [53] , information security  [54] ,  [55] ,  [56] ,  [57] ,  [58] ,  [59] , and medical imaging  [60] ,  [61] ,  [62] , etc.