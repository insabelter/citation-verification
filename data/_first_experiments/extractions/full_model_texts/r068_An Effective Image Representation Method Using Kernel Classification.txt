I. INTRODUCTION
In modern times, countless digital photos are being taken everyday, and the problem of how to store, represent and retrieval them raises  [1] -  [5] . In recent years, the digital management system of images has been proposed to effectively handle this problem  [6] ,  [7] . Many online systems are developed to store, manage and share the images. For example the google image engine  [8] ,  [9]  can that allows users to search the Web for image content. Using such services, we can classify the images correctly according to the concept of them. In figure  1 , we shown some examples of classified image using google image engine. These classified images can provide effective information of image concept, by containing more themes information. Furthermore, Flickr groups can represent richer themes  [10] ,  [11] . In this paper, an effective representation is learned for an image by using these Google image class memberships. If two images share a set of similar class memberships, we regard them as a pair of similar images. Thus we use the class memberships to construct the new representation of the image. Based on this new presentation, we could gain an accurate retrieval of images from a image database  [12] -  [14] . A kernel classifier is used to measure the membership of an image to a special image class. The number of images in each class is always tremendous, thus the training of kernel classifier may takes a long time  [15] -  [18] . To solve this problem, an efficient algorithm is needed. In this paper, we develope a new iterative algorithm to learn the kernel classifier, which is an efficient fast implementation of support vector machine (SVM)  [19] -  [21] . We try to develop the fast algorithm by using both the stochastic learning strategy  [22] -  [24]  and the support vector approximation technology  [25] ,  [26] . The developed classifier is fast and robust, which makes the learning from a large number of images possible. Because the training images are input into the algorithm one by one, thus it does not takes much memory, and it is also a online algorithm. Moreover, we use the Histogram Intersection Kernel (HIK)  [27] -  [29] , which is also easy and fast to calculate for histogram features vectors of multi-instance based image representation  [30] -  [32] . Using the proposed kernel classifiers, and with a candidate image, we firstly input it into the classifiers of different classes and obtain the classification responses. Then the classification responses are connected to form a classification response vector. Finally, a pair of images are compared by consulating the Euclidean distance  [33] -  [36]  between show that this method is both efficient and effective for the kernel classifier learning. However, the goal of kernel classification is not to learn good classifiers themselves, but to use the classifier for the representation of images. We also show that the proposed kernel classification response vector performs better than the original histogram features. We use the proposed method to retrieve images from large scale images. It is shown that it outperforms other methods. The contribution of this paper is summarized as follows: • In this paper, we demonstrate a new approach to use the image of different classes to represent a new image. • Moreover, we develop a novel iterative algorithm to learn the kernel classifier by using the training samples one by one. It could also be used to learn other types of classifiers.

II. REPRESENTATION BY KERNEL CLASSIFICATION


A. Overview
The overview and framework of the proposed method is shown in figure  2 . We first download many images of different classes from internet and then train SVM classifiers to separate the images of each class to other classes using the one-against-all rule [37]-  [40] . Given a test image, we apply the SVM classifiers to compute its response, and then the response vector is used as the new representation. Finally, the image dissimilarity is measured using the Euclidean distance between the response vectors  [41] .

B. Kernel Classifier Learning
For each image class, we train a kernel classifier to distinguish the images of this class against the images of other classes. To this end, we first download a large number of images of this class, and also a great deal of images of different classes. Then, we extract the histogram feature vector from each image. The set of extracted image histogram vectors are presented as {x i } N i=1 , where x i = [x i1 , • • • , x id ] ∈ R d is the d dimensional vector of the i-th image, and N is the number of images. A HIK kernel is used to match a pair of histograms as K(x i , x j ) = k min(x ik , x jk ) (1) Finally, based on this kernel, we design a classifier to predict if an image x belongs to this class or not. f (x) = N i=1 α i φ(x i ) φ(x) = N i=1 α i K(x i , x) =α K(•, x) (2) where φ(x ) is an implicit nonlinear mapping function so that φ( x i ) φ(x) = K(x i , x), α i is the weight for i- th training image, α = [α 1 , • • • , α N ] , and K(•, x) = [K(x 1 , x), • • • , K(x N , x)] . The kernel classifier parameter α is learned by minimizing the objective function as min α 1 2 α 2 2 + C 1 N N i=1 (f (x i ), y i ) (3) where is a loss function for the classifier, y i ∈ {+1, -1} is the class label of i-th image. Here we use the hinge-loss (f (x i ), y i ) = max(0, 1 -y i f (x i )) (4) α 2 2 is introduced to control the complexity of the classifier, so that the structural loss can also be minimized  [42] . We write the objective function in  (3)  as O(α) = 1 2 α 2 2 + C 1 N N i=1 (f (x i ), y i ) = 1 2 α α + C 1 N N i=1 β i 1 -y i α K(•, x i ) (5) β i is defined as β i = 1, if 1 -y i α K(•, x i ) ≥ 0 0, e l s e (6) We try to use the gradient descent method  [43] -  [45]  to update the classifier parameter α in an iterative algorithm. However, for the hinge loss has no close form of derivative with regarding to α, thus we fix β i as the previous one. In the t-th iteration, we first compute β t-1 i as β t-1 i = 1, if 1 -y i α t-1 K(•, x i ) ≥ 0 0, e l s e (7) then fix it and compute the derivative of O(α) with regarding to α, as ∂O(α) ∂α = α -C 1 N N i=1 β t-1 i y i K(•, x i ) (8) Then α is updated as follows where τ t is the descent step at t-th iteration. It can be concluded that in each iteration, all the training images are used to compute ∂O(α)   ∂α α=α t-1 . When the training image number N is large, the computational cost could be very expensive. α t = α t-1 -τ t ∂O(α) ∂α α=α t-1 = α t-1 -τ t α t-1 -C 1 N N i=1 β t-1 i y i K(•, x i ) = 1 -τ t α t-1 -τ t C 1 N N i=1 β t-1 i y i K(•, x i ) (9) 1) Fast updating method: To accelerate the updating procedure, we develop a novel and efficient algorithm for this problem. The training images are given to the algorithm one by one. In each iteration, only one image is used. We assume that in t-th iteration, x i t is used, and the objective function is constructed using only x i t as O(α) = 1 2 α 2 2 + C (f (x i t ), y i t ) = 1 2 α α + Cβ i t 1 -y i t α K(•, x i t ) (10) The derivative of O(α) with regarding to α is as ∂O(α) ∂α = α -Cβ i t y i t K(•, x i t ) (11) where β i t is also computed using the previous α t-1 as β i t = 1, if 1 -y i t α t-1 K(•, x i t ) ≥ 0 0, e l s e (12) In the t-th iteration, α is updated as α t = α t-1 -τ t ∂O(α) ∂α α=α t-1 = α t-1 -τ t α t-1 -Cβ i t y i t K(•, x i t ) = 1 -τ t α t-1 -τ t Cβ i t y i t K(•, x i t ) (13)

C. Classification Response Vector
Given M classes of images, we train M SVM classifiers denoted as {f m (x)} M m=1 using the developed efficient algorithm. When a test image comes, its feature vector is extracted as x, and its response to M classifiers are given as f m (x), m = 1 • • • M . These classification responses are further organized in a vector f = [f 1 (x), • • • , f M (x)] . When a pair of images x i and x j are available, they are firstly represented as f i = [f 1 (x i ), • • • , f M (x i )] and f j = [f 1 (x j ), • • • , f M (x j )] , then we compute the Euclidean distance to compare the similarity between them Dist(f i , f j ) = m (f m (x i ) -f m (x j )) 2 (14) III. EXPERIMENTS Moreover, we collected a large scale database of 10,000 images from google image engine which are classified according to google keywords. Some example images of our database are given in figure  3 . We test our algorithm on the database retrieval task of the collected data set. We randomly selected 1,000 images as queries and leave the remaining ones as the database images. We define an image in the database as a relevant image to the query image if they belong to the same class. We compare our method to the method based on original histogram feature and HIK as similarity measure. The compared method is denoted as "Histogram+HIK". Our method is denoted as "Kernel Classification Representation". The average retrieval precision (ARP)  [46] -  [48]  of different retrieved images is given in figure  4 .  The retrieval ARP of the proposed method and compared method. In the experiment, a precision value is calculated for each query image, an then they are averaged to obtain the final ARP. The original histogram feature, the precision value at one retrieved image is 0.4620. But when the kernel classifier is used to represent the images, it is improved to 0.7164. Such a significant improvement is archived by including the discriminant information provided by the classifier. A much more accurate representation of image is provided by using the kernel classifier trained from the images of different  In figure  5 , the top nine nearest neighbor images of a query image is shown. On the top of the figure, the query image is presented, and the retrieved nearest neighbors are presented below. It is clear that the proposed method can relevant images from the database. We also compare the proposed fast updating algorithm with the original updating one. The training time is given in figure  6 . It is indicated that the proposed algorithm can reduce the training time significantly.

IV. CONCLUSION
In this paper, we propose a novel method to represent image using the kernel classifiers. The motivation behind this is that two images are regarded as similar to each other if their response to a classifier are similar. We developed a fast and effective iterative algorithm to train the kernel classifier when the training image number is tremendous. The presented experiment results show the proposed presentation method outperforms the original features. In addition, our methodfig2 reduce the training time eminently.