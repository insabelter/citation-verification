Introduction
Cyberattacks have become a popular topic in recent years as an increasing number of terminal devices are connected to the Internet. They are highly threatening to the information security of individuals because of the large number of computer viruses spreading in the network, and various types of cyberattacks have emerged in endless streams. Examples of such attacks include disclosure, modification and deletion of specific data, attempts at unauthorized access, manipulation of information, and other malicious behaviors that make a system unreliable and unstable. This damage produces incalculable and disastrous consequences for users of the system  (Gong et al., 2019) . It is becoming increasingly important to secure users' data through the popularization of networking applications. However, it has always been difficult to develop an effective and compatible method for complex high-dimensional data  (Rezvy, Petridis, Lasebae, & Zebin, 2018) . With the rapid development of artificial intelligence, methods based on deep learning are envisioned as competent approaches for cyberattack detection. Some scholars have therefore adopted an unsupervised machine learning method with dimension reduction functions, known as the autoencoder framework  (Andresini, Appice, Di Mauro, Loglisci, & Malerba, 2019; Xu, Qian, & Hu, 2019) , but this kind of method is limited to performing robust anomaly detection for network intrusion data with high dimensionality and sparsity without supervision. This is because when the dimension of the input data increases, it is more difficult to estimate the density of multidomain data in the original feature space. Additionally, an input sample may easily be regarded as an abnormal event that has a low probability of being observed  (Shone, Ngoc, Phai, & Shi, 2018) . To address the performance degradation caused by high dimensions, many works have concentrated on dimension reduction, which searches for a lower-dimensional representation by reducing the number of variables in the data  (Kim, Kwon, Chang, & Paik, 2020; Zong et al., 2018) . The major drawback is the weak assumption of homogeneous parameters of a specific Gaussian distribution with respect to each domain of data, which significantly deteriorates the original data properties. Depending on the type of cyberattack, it is usually possible to classify network traffic into different domains. For example, the classic KDDcup'99 network intrusion data set has been found to have four domains: denial of service (DoS), remote to local (R2L), user to root (U2R) and Probe  (Chen et al., 2018) . The size of each domain is different, and they do not follow a homogeneous distribution. Therefore, treating attack samples indiscriminately will affect the results of anomaly detection and may even mislead the entire machine learning process. Many customized neural network models that are used in a single domain can achieve good anomaly detection performance, but the effect on multidomain data is poor because of the limited ability to acquire the complex information of high-dimensional distributions. One major solution in recent decades to yield enriched features is to decouple the learning process following  Dromard, RoudiÃ¨re, and Owezarski (2016)  and  Zhou and Paffenroth (2017) , but this method is still unable to obtain sufficient features of anomalous attack samples and thereby fails to obtain accurate detection results. Cyberattack data are usually collected from a large number of heterogeneous network devices that exhibit uncertainties and severe skewness, which is fatal for conventional machine learning. The prospect is that the multidomain machine learning methods developed in recent years can better address abnormal network intrusion problems of multidomain data by making full use of the features of multidomain data to achieve optimal performance while simultaneously saving training time and model resources. The core of multidomain machine learning is to obtain a model with the smallest average risk in multiple domains and to treat different cyberattacks as independent domains  (Hu, Li, Liu, & Li, 2020; Qian et al., 2019) . Therefore, the key problem to be solved in this paper is to use hidden information in multidomain data to improve the performance of the anomaly detection model using multidomain machine learning. For this purpose, a multichannel ensemble autoencoder combined with an attention mechanism is devised. To maintain the different distributions of multidomain data in a low-dimensional hidden space, the minimum reconstructed error of the multichannel autoencoder is fed back to GMM, and the expectation maximization (EM) algorithm is used to estimate the mixing probability of the components. When the estimated sample density exceeds the learning threshold obtained in the training phase, the sample is identified as an attack anomaly. Because the robust optimization algorithm enables multidomain machine learning models to obtain common feature representations and the models can adapt to different domains, this paper further develops an optimized combination of ensemble autoencoders and GMM so that the model used in each domain is optimal. Based on the above idea, the main contributions of this study lie in the following three areas: â€¢ An ensemble framework of multichannel network anomaly detection called the ensemble multichannel mixed model (EM 3 ) is proposed that combines deep autoencoders and the GMM model. â€¢ A robust optimization version of EM 3 for multiple domains, namely, EM 3+ , is proposed, which transforms the optimization problem of the objective function into a Lagrangian dual problem. â€¢ A series of experiments are conducted on two classic data sets and a newly published data set from 2020, which to the best of our knowledge is the first work that performs algorithms on both differentiated data domains and data distributions. The rest of this paper is organized as follows. Related works are presented in Section 2. Section 3 provides detailed descriptions of the proposed framework, which includes parameter learning and the realization of a robust optimization model. The results of a series of experiments are reported in Section 4, followed by the conclusions drawn in Section 5.

Related work
Internet attackers constantly improve their intrusion methods, resulting in the existence of various types of abnormal data in network traffic. It is therefore becoming increasingly urgent to devise effective approaches that are able to contend with increasingly complex cyberattacks  (Qian et al., 2019; Zong et al., 2018) . The existing library has documented many works with respect to anomaly detection, and the most popular method is the integration of autoencoders in a specific computing framework. Autoencoders are a type of artificial neural network and are used in both feature engineering and anomaly detection. There are two major types of research on the applications of autoencoders in network intrusion anomaly detection. One type of research mainly uses the autoencoder and a variant of it that incorporates a deep network to recognize distributional heterogeneity by constantly learning and reconstructing normal and abnormal samples. For example, some studies have used automatic variational encoders for network intrusion anomaly detection and have proposed a unified paradigm conversion system based upon unsupervised Gaussian mixture variational autoencoders  (Liao, Guo, Chen, & Li, 2018) . This method first generates sample distributions through training and extracts reconstructed features and then uses a deep simplified network to estimate the mixing probability of the components through the potential distribution. The authors of  Gong et al. (2019)  used an autoencoder extended with a storage module. Given an input, the code is first obtained from the encoder and is then used as a query sentence to retrieve the most relevant storage items to reconstruct the features.  Rezvy et al. (2018)  applied a dense neural network algorithm based on a deep autoencoder in cyberattack detection and evaluated the algorithm with the benchmark data set NSL-KDD. The results show that the deep autoencoder can effectively distinguish the differences in low-dimensional spaces for both normal and abnormal samples in low dimensions so that this algorithm can achieve excellent detection performance. The other type of research focuses on a combination of an autoencoder and functions based on distance metrics or probability distributions.  Kim et al. (2020)  established an anomaly detection algorithm based on an admissibility attribute, which includes an objective function of integral probability measurement and a type I autoencoder called Lipschitz. The proposed Wasserstein distance metric achieves Lipschitz continuity by minimizing the approximate Wasserstein distance and penalty functions. Other relevant work includes the stacked asymmetric deep autoencoder  (Majumdar & Tripathi, 2017; Wang, Xu, Huang, Wang, & Lai, 2018) , used for unsupervised feature learning, and the deep autoencoder-based GMM  (Zong et al., 2018) , which uses a deep autoencoder to generate a low-dimensional representation and inputs the reconstructed error of the input data points into the GMM, which can not only maintain the original deep autoencoder but also discover high-quality and nonlinear features  (Zhou & Paffenroth, 2017) . In summary, recent studies based on deep autoencoders indicate that a scheme that combines dimensionality reduction and density estimation is effective in network attack anomaly detection. However, the obvious disadvantage is that the joint optimization of dimensionality reduction and density estimation is usually computationally difficult under a generic framework  (Andresini et al., 2019; Liu et al., 2020) , and the performance of the model is mainly affected by two limitations. The first is that they cannot handle the heterogeneity of data from different domains and cannot capture the subtle differences in different data distributions. The second is that they cannot obtain the variable information of samples in a low-dimensional space but simply estimate the sample density, which restricts the complex training process of multidomain data  (Injadat, Moubayed, Nassif and Shami, 2020) . Although deep autoencoders achieve good performance in single-domain tasks, in many practical situations, a unified model for multidomain data is required for shared tasks. Therefore, multidomain machine learning technology is used to obtain cross-domain adaptability and inherit the advantages of multitask learning at the same time  (Hu et al., 2020) . Early work  (Peng & Dredze, 2016)  with regard to multidomain machine learning focused on deep neural network models, which shared training weights in the early layers and used special weights in the later layers. For example, solutions were proposed in  Fourure et al. (2017)  that shared all core parameters except those in the batch and instance normalization layers, where different domains were modeled separately in individual neural networks. Based on this,  Vaca and Niyaz (2018) , as an extension, proposed a new parameterization method for the standard residual network architecture, which aimed to increase the number of parameters in a limited way in order to improve the level of parameter sharing between domains. However, these early works did not consider the risk of increased computational complexity caused by the large number of model parameters, which makes such models weak with respect to convergence  (Injadat, Moubayed and Shami, 2020) . In more recent works inspired by the success of transfer learning,  Berriel et al. (2019)  and  Ren and Lee (2018)  proposed a combination of serial and parallel network adapters for multidomain data. They obtained domain-related models with adjustable budgets in terms of the number of parameters and computational complexity. To adjust the computational complexity of the network, they adopted a pretrained architecture to derive a specialized deep model for each domain and then incorporated a budget-aware adapter to select the most relevant feature channel to better process the data from the new domain. Among recent works, there are also some studies that improve domain adaptability through adversarial learning. They decompose a deep network into feature extractor and classifier components and then train each component by tuning its partner  (Xu, Chen, Zuo, Yan, & Lin, 2018) . They either use a deep cocktail network (DCTN) to deploy multidirectional adversarial learning to minimize the difference between the targets and multidomain data  (Ganin et al., 2016)  or use domain-specific representation learning  (Schoenauer-Sebag et al., 2019) , where the data come from similar but heterogeneous distributions. Although multidomain machine learning solves the problem of domain adaptability to some extent through assumptions about specific data distributions, the performance of most models is poor in practice because they ignore the nature of data skewness and lack a process that effectively determines the robustness of multidomain data. It is claimed that ignoring the inconsistency of the data size and the heterogeneity of the data distribution generated by network traffic will have a serious impact on the detection results and may even mislead the entire machine learning process. In contrast to existing works, the current study focuses on extracting the hidden relationship between data that are not independent and identically distributed (non-IID) and unbalanced data and performing robust optimization on data from different domains so that the proposed scheme can further improve domain adaptability without relying on assumptions about the data distribution.

Proposed framework


System overview
The purpose of this research is to perform effective anomaly detection on network intrusion samples from different domains. The key problem to be solved is the feature learning and reconstruction of high-dimensional multidomain data. However, most of the current studies using single-channel networks cannot capture the hidden spatial information of data in different domains, and the features of the reconstructed errors do not include the differential representation of heterogeneous distributions. Therefore, EM 3 is proposed, as shown in Fig.  1 . Overall, EM 3 consists of two parts: the ensemble network and estimation network. The ensemble network uses a deep autoencoder to reduce the dimensionality of the input multidomain samples, and the estimation network feeds back the latent distribution learned from the ensemble autoencoder and the reconstructed error features to the GMM, using the potential distribution and the EM algorithm to estimate the sample density. To avoid the decoupling and suboptimal performance of feature learning for multidomain data, the system performs joint optimization on the ensemble autoencoder and GMM. The use of the GMM protects the ensemble of autoencoders from a local suboptimum, and the use of an autoencoder provides a prior  distribution of samples for the GMM. When the estimated sample density exceeds the learning threshold obtained in the training phase, the samples are identified as outliers. The major notation used in this study is listed in Table  1 . In what follows, each part of the system is described in three steps. First, an ensemble of deep autoencoders is utilized to reduce the dimensionality of multidomain data. The deep autoencoder is a popular deep learning model that is constructed with several autoencoders. Each autoencoder minimizes the reconstructed error between the input data and output data. Given an unlabeled ğ‘›-dimensional sample set ğ± = [ğ‘¥ 1 , ğ‘¥ 2 , â€¦ , ğ‘¥ ğ‘› ] âˆˆ â„œ 1Ã—ğ‘› , the autoencoder uses an activation function such as sigmoid to transform the input data set into a hidden representation and then maps it back into a reconstructed vector ğ± â€² . That is, the autoencoder attempts to learn the functions ğ·(ğ¸(ğ±)) = ğ± â€² â‰ˆ ğ±, where ğ· (â‹…) and ğ¸ (â‹…) are the decoder and encoder function, respectively. To measure the reconstructed error, the mean square error is used in this paper. And we utilized cross entropy as the cost function. Due to the difficulty of parameter selection, an individual deep autoencoder will probably show a low generalization ability. An ensemble of multiple deep autoencoders is developed to enhance the generalization performance by combining a series of activation functions  (Zhang, Li, Gao, Chen, & Li, 2020) . Let h = ğœ“( wx + b) denote the hidden representation, where ğœ“ is the combination of the activation functions of the hidden layer, w is the weight matrix with respect to the parameters and b is the bias vector. The proposed system inputs high-dimensional multidomain data into each channel and obtains the hidden space distribution of the reduced dimensionality and reconstructed error features for each channel: ğ‘§ ğ‘™ ğ‘ = ğ¸ ğ‘™ (ğ±), (1) where ğ‘§ ğ‘ is the hidden space of the autoencoder for channel ğ‘™. Assume there are ğ¿ channels in total. The minimum reconstruction error ğ‘§ ğ‘Ÿ is represented as: ğ‘§ ğ‘Ÿ = min { â„ 1 ( ğ±, ğ± â€² ) , â„ 2 ( ğ±, ğ± â€² ) , â€¦ , â„ ğ¿ ( ğ±, ğ± â€² )} , ( 2 ) where â„(â‹…) is the set of reconstructed error function of the autoencoder, which contains one of the relative absolute square error, Euclidean distance and cosine similarity  (Pratama & Kang, 2020) . Some existing studies use a single autoencoder as input, which largely ignores the fine discrimination of the reconstruction error features of multidomain data, resulting in suboptimal performance. Reconstructed error features in multiple autoencoders can provide richer information for the estimation network, thereby improving the model's reasoning ability. Second, to balance the complexity and expressive ability of the model, we use the attention model to optimize the hidden space of each channel so that important information is selected. An attention model based upon the encoder-decoder framework is widely adopted in natural language processing, image classification and speech recognition tasks because of its ability to optimize the memory information of hidden space in multiple channels. The encoder projects ğ± onto a hidden vector representation ğ¡. The decoder uses a recurrent network to maintain an internal hidden state ğ³ and uses the attention model to weight the feature vectors based on a score function: ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘¡ = ğ‘“ ğ‘ (ğ³ ğ‘¡-1 , ğ¡ ğ‘¡ ), (3) where ğ‘¡ is the number of iterations and ğ‘“ ğ‘ is a fully connected neural network with a single hidden layer  (Zhang et al., 2020) . Clearly, this equation takes both the previous decoder hidden state and the feature vectors at the ğ‘¡'th iteration as input. The two most commonly used attention functions are additive attention and dot-product attention. Here, we choose additive attention because it computes a compatibility function using a feed-forward network with a single hidden layer. The attention weight is computed using the following function: ğš ğ‘™ ğ‘¡ = sof tmax ( ğ‘‘ ğ‘™ ğ‘¡ ğ° ğ‘™ ğ‘¡ ğ‘§ ğ‘™ ğ‘ ) , ( 4 ) where ğ‘‘ ğ‘¡ is the state of the encoder at the ğ‘¡'th iteration, ğ‘™ âˆˆ ğ¿ is the channel, and ğš is the attentive weight after using the softmax function. With the obtained attentive weights, the weighted sum of all the hidden vectors ğ‘§ ğ‘ is represented by: ğ‘§ ğ‘ = ğ¿ âˆ‘ ğ‘™=1 ğš ğ‘™ ğ‘¡ ğ‘§ ğ‘™ ğ‘ . (5) Finally, the proposed system combines ğ‘§ ğ‘ with the minimum error feature of the ensemble network into a new hidden space vector ğ³ = [ ğ‘§ ğ‘ , ğ‘§ ğ‘Ÿ ] .

Parameter learning
EM 3 jointly optimizes the parameters of the ensemble autoencoder and GMM in an end-to-end manner and uses a separate estimation network to promote parameter learning. Joint optimization can balance the reconstruction of multiple autoencoders in the ensemble network and contribute to density estimation and regularization, thereby avoiding local optimization and reducing the reconstructed errors. To make the probability model effective, we optimize the model with the addition of objective functions and reparameterization techniques. The GMM converts high-dimensional data into a mixture of single-mode Gaussian distributions, which can solve mathematical problems in high-dimensional spaces. When training samples, the EM algorithm is used to solve the parameters  (Zong et al., 2018) . Given the latent space distribution ğ‘§ and the number of mixed components ğ‘Ÿ, the GMM uses the softmax function to generate an ğ‘Ÿ-dimensional vector for each sample, represented as Î“ğ‘¥ = [ Î³ğ‘¥1 , Î³ğ‘¥2 , â€¦ , Î³ğ‘¥ğ‘Ÿ ] , where Î³ğ‘¥ğ‘– , 1 â‰¤ ğ‘– â‰¤ ğ‘Ÿ, denotes the probability that the ğ‘–'th component of the GMM produces ğ±. In the maximization stage of the EM algorithm, the parameters of the GMM are estimated using ğ‘ samples and the corresponding mixed probability Î“ğ‘¥ as in Eq. (  6 ): Ï†ğ‘Ÿ = ğ‘ âˆ‘ ğ‘–=1 Î³ğ‘–ğ‘Ÿ ğ‘ , Î¼ğ‘Ÿ = âˆ‘ ğ‘ ğ‘–=1 Î³ğ‘–ğ‘˜ ğ³ ğ‘– ğ‘ âˆ‘ ğ‘ ğ‘–=1 Î³ğ‘–ğ‘Ÿ , Ïƒğ‘˜ = âˆ‘ ğ‘ ğ‘–=1 Î³ğ‘–ğ‘Ÿ (ğ³ ğ‘– -Î¼ğ‘Ÿ ) 2 âˆ‘ ğ‘ ğ‘–=1 Î³ğ‘–ğ‘Ÿ . ( 6 ) where Î¼ğ‘Ÿ is the average of GMM component probabilities and Ïƒğ‘˜ is the corresponding variance. In the prediction stage, we utilize the sample energy as the anomaly score. The sample energy characterizes the degree to which a sample deviates the trained distribution. When the anomaly score exceeds a user-defined threshold, it is recognized as an abnormal value. The energy of the samples ğœ‰(ğ³) can be formulated as follows: ğœ‰(ğ³) = -log â› âœ âœ â ğ‘Ÿ âˆ‘ ğ‘˜=1 Ï†ğ‘˜ exp(-1 2 (ğ³ -Î¼ğ‘Ÿ ) T )Ïƒ -1 ğ‘Ÿ (ğ³ -Î¼ğ‘Ÿ ) âˆš |2ğœ‹Ïƒ ğ‘Ÿ | â âŸ âŸ â  , ( 7 ) where | â‹… | is the determinant of the matrix.

Robust optimization for multiple domains
To make the model perform well in various domains, it is necessary to improve the performance of the domain with a small number of samples to prevent the samples in this domain from reducing the overall model performance. In this section, a robust optimization method, namely, EM 3+ , based on the EM 3 framework is proposed. Multidomain learning aims to obtain common behaviors across related problems, so the core idea is to learn domain-specific parameters guided by shared parameters. Multidomain learning algorithms are simple to implement and scale to very large data sets. They process multiple streams of data from many different sources, transferring knowledge between domains through a shared model  (Luong, Pham, & Manning, 2015)   min ğ° { ğ‘˜ âˆ‘ ğ‘š=1 max ğ‘ ğ©ğŸ(ğ° ğ‘š ) + ğ›½ ğ‘˜ âˆ‘ ğ‘š=1 1 ğ‘˜ â€– â€– Åµ -ğ° ğ‘š â€– â€–ğ¹ }, (8) where { ğ© âˆˆ ğ‘˜ | | | âˆ‘ ğ‘˜ ğ‘š=1 ğ‘ ğ‘š = 1; âˆ€ğ‘˜, ğ‘ ğ‘š â‰¥ 0 } is the dual management factor and ğ›½ > 0 is the penalty factor. ğ© can also be treated as an adversarial distribution of different domains, and its default value is calculated as ğ‘ ğ‘š = 1 ğ‘˜ . Then, we have Eq. (  9 ): ğŸ (ğ°) = [ğ‘“ 1 (ğ°) , ğ‘“ 2 (ğ°) , â€¦ , ğ‘“ ğ‘š (ğ°)] ğ‘‡ , ( 9 ) where ğ‘“ ğ‘– (ğ°) = â„(ğ‘¥ ğ‘– ğ‘š , ğ‘¦ ğ‘– ğ‘š ;ğ°). The second term of Eq. (  8 ) narrows the difference in the parameters of the multidomain model through the average value ğ°. Obviously, it is a minimax problem that satisfies the Karush-Kuhn-Tucker (KTT) condition according to  Yu (2020) . Therefore, the optimization problem of the objective function can be considered equivalent to the Lagrangian dual problem. It is known that a key to minimax optimization is that the formula is very sensitive to outliers. If there is a domain with significantly lower performance than other domains, the objective function of the domain with poor performance will dominate the entire multidomain learning process, which will seriously affect the performance of the training model in different domains. For this reason, this paper normalizes the loss of each domain to prevent the ğ© of a poor domain from being larger and to ensure that each domain has a smooth training process. To this end, we use the Lagrangian relaxation mode, as given in Eq. (  10 ): max ğ© ğ‘‡ ğ‘š ğŸ ğ‘š (ğ°) + ğœ‚(ğ‘ 1 + ğ‘ 2 + â‹¯ + ğ‘ ğ‘˜ -1), (10) where ğœ‚ â‰¥ 0 and ğ‘ ğ‘˜ â‰¥ 0. The dual problem is formulated as Eq. (  11 ): max ğ‘ min ğ° ğ© ğ‘‡ ğ‘š ğŸ ğ‘š (ğ°) + ğœ‚(ğ‘ 1 + ğ‘ 2 + â‹¯ + ğ‘ ğ‘˜ -1) (11) To solve the minimax optimization problem presented in Eq. (  11 ), we use the gradient descent method to learn the model and the gradient ascent method to update the adversarial distribution. The ğ‘ value can be obtained using the approximate gradient descent algorithm  (MariÃ±o & MÃ­guez, 2006)  in Eq. (  12 ): ğ‘ ğ‘˜ = ğ‘ğ‘Ÿğ‘œğ‘¥ ğ‘¡â„ (ğ‘ ğ‘˜-1 -ğ‘¡ ğ‘˜-1 âˆ‡ğ‘“ (ğ°)) ( 12 ) where ğ‘¡ is the step size of each gradient. Let ğœ’(ğ‘) = ğœ‚ ( ğ‘ 1 + ğ‘ 2 + â‹¯ + ğ‘ ğ‘˜ -ğŸ ) ; according to the definition of ğ‘ğ‘Ÿğ‘œğ‘¥ ğ‘¡â„  (Li, Zou, & Zhong, 2020) , Eq. (  12 ) is expanded to obtain Eq. (  13 ): ğ‘ + = argmin ğ‘ ( ğœ’ (ğ‘) + 1 2ğ‘¡ ğ‘ + ğ‘¡âˆ‡ğ‘“ (ğ°) 2 2 ) = arg min ğ‘ ( ğœ’ (ğ‘) + ğ‘“ (ğ°) + âˆ‡ğ‘“ (ğ°) ğ‘‡ ğ‘ + 1 2ğ‘¡ ğ‘ 2 2 ) (13) The expression in parentheses is a second-order expansion of ğ‘“ near ğ‘, and ğ‘ + is the minimum value of the approximate function. Furthermore, we have Eqs. (  14 ) and (  15 ): 0 âˆˆ ğ‘¡ğœ•ğœ’ ( ğ‘ + ) + ( ğ‘ + -ğ‘ + ğ‘¡âˆ‡ğ‘“ (ğ°) ) (14) ğº ğ‘¡ (ğ°) âˆ¶= ğ‘ -ğ‘ + ğ‘¡ âˆˆ ğœ•ğœ’ ( ğ‘ + ) + âˆ‡ğ‘“ (ğ°), (15) where ğº ğ‘¡ (ğ°) = ğœ•ğœ’ ( ğ‘ + ) + âˆ‡ğ‘”(ğ°) is approximated as the subgradient of the function ğ‘“ and ğ‘ + can be simplified as ğ‘ + = ğ‘ -ğ‘¡ğº ğ‘¡ (ğ°) so that the dual management factor ğ‘ is obtained for each domain. To find the shared model parameters ğ°, the robust optimization algorithm is used as follows: The multidomain machine learning model initializes the model parameters of each domain. It randomly samples p*n samples for iteration to obtain the loss function of each domain and then uses the gradient descent algorithm to update the ğ‘ value. When the update is complete, the average value of the model parameters Åµ is reassigned to the model parameters in the corresponding domain until the end of training.

Experiments
The purpose of this study is to detect network intrusion in different domains using the proposed EM 3 framework. In this section, a series of experiments are conducted to test whether the proposed EM 3 is competitive with respect to specific measures compared to other algorithms. The data sets used in the experiment are detailed in Section 4.1, followed by a Section 4.2 that describes the baselines based on frequently used algorithms in the network intrusion anomaly detection area. Section 4.3 presents the experimental settings used in the experiment, and the results are reported in Section 4.4. All of the experiments are performed with PyTorch and the Python SK-learning library on a RHEL7.5 server with an Intel Xeon E5-2687w 3.1 GHz CPU and NVIDIA Quadro P4000 GPU. P.  An et al.  

Data sets
Three public data sets are used in this experiment. The first is KDDcup'99  (Zong et al., 2018) , which comprises network connection and system audit data collected over 9 weeks by MIT using TCPdump. This data set simulates data samples of different user types, network traffic and attack methods. Each sample is described with 41 features. The data set includes 14,258 normal samples and 713 abnormal samples in total. The multiple domains can be divided into DoS, R2L, U2R and Probe domains. For the experiment, we randomly sample 20,000 data points for each of the DoS, R2L, and Probe types, where anomalies account for 10%. Since the size of the U2R data is small, we set 1000 U2R data points, and the abnormal proportion is 5%. For the nominal features in this data set, we coded them according to numerical categories. Taking protocol as an example, the Transmission Control Protocol (TCP) was coded as 0, the User Datagram Protocol (UDP) was coded as 1 and the Internet Control Message Protocol (ICMP) was coded as 2. The second data set is CICMalDroid 2020  (Mahdavifar, Kadir, Fatemi, Alhadidi, & Ghorbani, 2020) . This data set is for Android malware that contains the most complete static and dynamic network connection features among publicly available data sets, including the latest and most complex Android samples as of 2018. In this experiment, the top 9 features were selected for experimentation through principal component analysis (PCA). The multiple domains of this data set are Adware, Banking Malware, Short Message Service (SMS) Malware and Mobile Riskware. Each type contains 20,000 entities, of which abnormal data account for 10%. The last data set is AWID  (Vaca & Niyaz, 2018) . It is collected from a small network environment for 802.11 networks, which typically involves 11 client computers. The data are a wireless local area network (WLAN) data stream captured in a packet-based format, and the multiple domains of this data set are Flooding, Impersonation, and Injection. Each domain contains 20,000 entities, where abnormal data account for 10%. Since the KDDcup'99 and CICMalDroid 2020 data sets are structured numerical data, there are no garbled codes or symbols in the data, so this experiment normalizes them and encodes them with one hot encoding. However, the missing, garbled, and duplicated data in the AWID data set cannot be directly input to the model. We perform data cleaning, data transformation, and feature selection on the original data set. During data cleaning, this experiment uses replacement and zero padding to convert dirty data into quality data. Constants are used to fill the missing data in the set, and the existing special symbols and garbled codes are cleared or replaced. Since the AWID data set contains 154 features with both numerical and character data, we convert nonnumerical features such as wlan.ra and wlan.da into numerical features.

Baselines
To compare the performance difference of the proposed model with and without robust optimization, the experiment uses the following baselines: â€¢ The one-class support vector machine (OCSVM)  (Zhou & Paffenroth, 2017 ) is a commonly used kernel-based outlier detection method. When making predictions, the model searches for a hyperplane; the samples marked on this hyperplane are considered positive samples, and vice versa. All tasks in this experiment use radial basis function kernels. â€¢ The deep autoencoding Gaussian mixture model (DAGMM)  (Zong et al., 2018 ) combines a GMM with a deep autoencoder. The model uses the deep autoencoder to generate a low-dimensional representation and reconstructed error for each input data point and then inputs it to the GMM. The parameters of the deep autoencoder and the hybrid model are optimized simultaneously in an end-to-end manner, and a separate estimation network is used to promote the parameter learning of the hybrid model. The estimated density of the sample is used as an anomaly detection criterion. â€¢ Adversarially learned anomaly detection (ALAD)  (Zenati, Romain, Foo, Lecouat, & Chandrasekhar, 2018 ) combines an autoencoder with the standard generative adversarial network (GAN) algorithm and uses representative learning to measure the similarity in the data space. By combining the autoencoder with the GAN, the feature representation can be used as the basis for the reconstruction target in the GAN discriminator.

Settings
The learning rate of the models used on the KDDcup'99 and AWID data sets is set to 0.001, and the learning rate of the models used on the CICMalDroid 2020 data set is set to 0.0007, while the batch sizes are 256 and 128, respectively. The hyperparameters ğœ† 1 and ğœ† 2 of the regularized objective function are set to 0.1 and 0.0001 based on cross-validation experiments. To test the multichannel neural network in this study, the number of channels is set to 3 for simplification. The channels are the deep simple networks of  [100, 64, 32, 16, 1, 16, 32, 64, 100] ,  [112, 82, 56, 28, 1, 28, 56, 82, 112] and [60, 30, 10, 10, 30, 60] , where the digits represent the number of neurons in the network. The experiment consists of two parts. The first part of the experiment was conducted on data from the undifferentiated domain. In this setting, we did not divide the data set into domains following traditional solutions to show whether the performance of EM 3 proposed in this paper is competitive in a single domain. In the second part of the experiment, we divided the data set into domains to show whether the performance of the EM 3 framework proposed in this paper is competitive in multiple domains. We used the F1 value as the evaluation standard of the model performance based on the calculation of the accuracy and recall rate because it is a commonly used evaluation index for abnormality detection problems. The accuracy rate is the most intuitive performance measure; it is the ratio of the correctly predicted observations to the total observations. The recall rate is the ratio of correctly predicted positive observations to all observations in the actual class, and the F1 value is the weighted average of the accuracy rate and the recall rate. When the F1 value is higher, the model is better. P. An et al. 

On an undifferentiated domain
Table  2  shows the result matrix for running the model on different data sets when no domain distinction is made on the data sets, as measured by F1. EM 3 has the highest F1 value on all data sets, followed by ALAD. This is because ALAD does not use the information represented by dimensionality reduction in the autoencoder hidden space. Although DAGMM uses a GMM and a deep autoencoder, its F1 value is reduced significantly, by approximately 1.6%, compared with EM 3 because EM 3 utilizes an attention model to select the optimal information of the multichannel autoencoder hidden space, and the ensemble autoencoder is more effective than the single autoencoder for data dimensionality reduction. The F1 value of the OCSVM model is the smallest. Compared with EM 3 , the performance is reduced by approximately 16%, and the performance is reduced by approximately 15.2% compared with ALAD. In addition, the model has a higher F1 value on the KDDcup'99 data set and a lower F1 value on the AWID data set, which is attributed to the fact that there is less available information in the AWID data set. To the best of our knowledge, previous work has mainly applied this data set to classification tasks, and for the first time, we are using it for unsupervised anomaly detection testing. In summary, the EM 3 proposed by this research obtained the best F1 value on the selected data set. The ensemble deep autoencoder produces a low-dimensional representation of data and the reconstructed errors, which effectively utilizes the hidden information in the network intrusion data. Furthermore, the attention mechanism is used to optimize the latent space distribution, which is an advantage that support vector machines do not have. In addition, EM 3 uses the reconstructed minimum error based on the Euclidean distance and cosine similarity measure to expand the difference distribution between normal samples and abnormal samples so that GMM can better learn the sample distribution. The normal and abnormal samples in the hidden space of EM 3 are visualized in Fig.  2 , which shows a low-dimensional representation of the hidden space of different channels in an ensemble network. The normal and abnormal samples can be better separated in low dimensions under the EM 3 framework because their samples overlap very little. EM 3 dynamically selects the autoencoder with the smallest reconstructed error for different data sets and jointly optimizes the autoencoder and GMM parameters in an end-to-end manner, which helps the autoencoder eliminate local optimization and obtain better compression results. In addition, the use of the GMM provides the model with more meaningful sample distributions for different domains. Fig.  3  presents the convergence of the models on different data sets. Clearly, the loss of each model decreases as the epoch increases. EM 3 decreases most rapidly compared to the other algorithms, which means that EM 3 has the best convergence ability. Specifically, the loss of EM 3 becomes constant after the algorithm runs approximately three epochs for all the selected data sets. However, the loss of the other algorithms depends on the data sets. SVM, DAGMM and ALAD have nearly the same loss changes for the KDD data set, and they reach stability when the algorithms run for more than eight epochs. For the Android data set, DAGMM is superior to SVM and ALAD. The loss of DAGMM reaches stability when the epoch number reaches two, while SVM and ALAD become stable when the epoch number reaches eleven and thirteen, respectively. For the AWID data set, the curve changes in different ways. SVM and ALAD converge faster than DAGMM, which reaches stability after 21 epochs. Moreover, the enlarged local charts indicate that EM 3 has the smallest loss value compared to the other algorithms, although all of them converge after several epochs. When running the algorithm on the CICMalDroid 2020 data set, the model is stable after 3 epochs. The fast model learning speed mainly depends on two properties of the proposed method. First, we use 9 features of the data and a three-channel deep autoencoder, which can quickly learn the features of the data. Second, the attention mechanism chooses the hidden space so that the GMM can better learn the sample distribution of the data. 

On differentiated domains
Table  3  presents the results of the algorithms running on different domains. Clearly, the proposed EM 3 is superior to the other algorithms when the domains of the data sets are differentiated. For the KDDcup'99 data set, OCSVM is not able to recognize the abnormal samples of the U2R domain (F1 = 0.4887), while it obtains acceptable accuracy on the DoS domain (F1 = 0.7414), which means that the performance of the OCSVM algorithm depends on the distribution of each domain and is not robust to the data set. DAGMM and ALAD have relatively similar performances because they both use a combination of autoencoders and other models. For example, the DAGMM model combines an autoencoder and a GMM, and ALAD combines an autoencoder and a GAN. Moreover, nearly all of the algorithms follow a similar trend when they are used on the domains of the CICMalDroid 2020 data set except that DAGMM outperforms ALAD in terms of specific domains such as Adware and Banking. On average, algorithms running on the domains of KDDcup'99 obtain the highest values, and algorithms running on the AWID data set have the lowest values. The reason for this is that the amount of data becomes small after it is cleaned, resulting in insufficient model training. In addition, some features are filled, which makes the model unable to learn the changes in each feature. EM 3 is optimal on each domain of the AWID data set. Fig.  4  shows the hidden space of the KDDcup'99 data, which shows that EM 3 can better distinguish the abnormal and normal samples in various domains of the data set. It is worth noting that the performance of EM 3+ is improved in each domain of the data set relative to that of EM 3 . EM 3+ improves by at least 0.2% on KDDcup'99 and at least 2% on CICMalDroid 2020. EM 3+ is an updated version based on the robust      4 . The number of parameters of EM 3 and DAGMM is very close, but far less than that of ALAD. This indicates that EM 3 costs less than ALAD. Considering time and parameters together, overall EM 3 has a more competitive time complexity and fewer model parameters, illustrating the conclusion that EM 3 can be deployed flexibly and quickly on current hardware.   