I. INTRODUCTION
Bug tracking systems, such as Bugzilla and JIRA, play an important role in the process of managing software bugs. According to statistics, software companies spend over 45 percent of development time in repairing bugs in software development  [9] ,  [21] . Along with the increasing scale and complexity of software projects, a large-scale of bug reports are received daily by bug tracking systems. Due to bug repairing is a time-consuming and limited human resources, it is often difficult for developers to take care of all bug reports. In order to repair the bugs of software projects quickly, developers often need to prioritize the severe bug reports. Therefore, an automated technique to help developers to identify the severity of bug reports, would be more preferable to augment productivity. Two challenges often arise during the automated technique to identify the severity of bug reports: (1) high-dimension (both bug report dimension and word dimension) is found in most bug repositories, and (2) class imbalance occurs for those datasets. High-dimension is caused by the bug reports are submitted by testers from all over the world, and for each tester, the understanding and natural language description of bugs are different, which could result in large-scale and low-quality bug reports in bug repositories  [1] . A number of problems may arise due to the large-scale and low-quality, such as extensive computation and a decline in predictive performance. Class imbalance occurs when the number of bug reports in one class (majority-class) is obviously more than the other class (minority-class). This problem is more prevalent in bug repository (such Eclipse, Mozilla, and GNOME), where the proportion of severe bug reports is relatively larger than nonsevere bug reports. The primary drawback of imbalanced dataset is that traditional classification algorithms tend to misclassify minority-class bug reports as majority-class bug reports. Some investigators have try to solve these problem  [9] ,  [17] -  [19] . For the high-dimension problem, Gao et al.  [17]  presents six filter-based feature ranking techniques to reduce the number of available software metrics. Xuan et al.  [9]  combines feature selection algorithms with instance selection algorithms to reduce the scale of bug datasets as well as improve the data quality. For the class imbalance problem, Lamkanfi et al.  [18]  selected an equal number of reports for each class for inclusion in the training and evaluation sets. However, the manual selection of bug reports from the original dataset may be lossy and result in weak generalizations of the trained classifier. Against imbalanced distribution of training set problem, Yang et al.  [19]  investigated four widely used imbalanced learning strategies (ILS) to solve the imbalance distribution of bug reports from four different open source projects. However, few literatures consider both high-dimension and class imbalance problems that building an automated technique model without any affect by low-quality and imbalance distribution of bug reports. In this study, we present a process of using two data preprocessing steps, data reduction (for addressing large-scale and low-quality problem) and data sampling (for addressing class imbalance problem) together in the context of identifying the severity of bug reports. In data reduction step, we employ the combination of feature selection (FS) and instance selection (IS) to get small-scale and high-quality set of bug reports and improve the performance of our approach to identify the severity of bug reports. To avoid the bias of a single algorithm, we consider four commonly used feature selection algorithms, namely One Rule (OneR), Information Gain (IG), ChiSquared attribute selection (CHI), and Relief-F attribute selection (RF) and four instance selection algorithms, namely Condensed Nearest Neighor (CNN), Minimal Consistent Set (MCS), Edited Nearest Neighbor (ENN), and Iterative Case Filter (ICF)  [1] -  [3] . In data sampling step, we propose a RSMOTE approach to balance the imbalance distribution of bug reports with respect to their severities. We select a bug report from minority-class as a center point, and then choose another bug report from minority-class with the minimum Euclidean distance as the edge point. Then, we randomly sample new bug report points in a multi-dimensional sphere, which is generated among the multi-dimensional rectangle with the center point and edge point as diagonal. In addition, the number of new synthetic bug reports is constrained by the imbalance degree of the original datasets. Furthermore, in order to solve the uncertainty of random sampling, we propose a multiple sampling strategy that applies RSMOTE approach to multiple sampling, then we train classifiers with the balanced datasets obtained from multiple samplings, respectively. Finally, we use Choquet fuzzy integral  [20]  to integrate the trained classifiers to classify bug reports. Comprehensive experiments have been conducted on public datasets obtained from real-world bug repositories, and the experimental results indicate that our approach could efficiently improve the distribution of bug reports the quality of training datasets. In addition, our approach could efficiently improve the uncertainty of random sampling caused by RSMOTE, which could improve the classification accuracy of identifying the severity of bug reports with an imbalanced distribution. The main contributions of this paper are as follows: 1) We propose a combination of FS and IS to addressing the problem of data reduction. This can be viewed as a combination approach in bug repositories to reduce high-dimension (both bug report dimension and word dimension) and get small-scale and high-quality training dataset. 2) We propose a RSMOTE approach to balance the imbalanced distribution of bug reports. The new synthetic minority bug reports are generated in the neighbourhood of the remaining minority-class samples by RSMOTE. Several experimental shown that RSMOTE can effectively improve the generalization ability of classifiers to identify the severity of bug reports. 3) We propose a multiple sampling mechanism using RSMOTE to solve the random sampling uncertainty of RSMOTE. Firstly, we train the classifiers respectively with the balanced datasets obtained from RSMOTE. Then, we use fuzzy integral to integrate the trained classifiers to obtain the ultimate prediction results. To our knowledge this is the first study exploring to fusion of multi-RSMOTE with fuzzy integral to classify bug reports with an imbalanced distribution. 4) We evaluate our approach using data on ten components from three bug repository datasets (Eclipse, Mozilla, and GNOME). Several experiments demonstrate that our approach yields improved classification performance for identifying the severity of bug reports with imbalanced distribution.

II. METHODOLOGY
In this section, we present a detailed description of our approach to identify the severe bug reports with an imbalanced distribution.

A. MODEL DESCRIPTION
In this section, we propose a model for identifying severe bug reports with an imbalanced severity distribution, as shown in Figure  1 . The framework consists of three main phases. Firstly, employ the combination of FS and IS to get small-scale and high-quality set of bug reports (cf. Subsection III.B). Secondly, we use the RSMOTE approach to address the class imbalance problem (cf. Subsection III.C). Finally, duo to the new synthetic bug reports generated via RSMOTE are randomly generated in a certain area, which could cause the new synthetic bug reports to be noisy. To solve this problem, we combine of classifiers and fuzzy integral, while the classifiers are trained by training sets consisting of bug reports generated via RSMOTE (cf. Subsection III.D).

FIGURE 1.
The framework of our model for identifying severe bug reports with an imbalanced distribution.

B. DATA REDUCTION ALGORITHM
Since bug reports are submitted by people from all over the world and each person's description of natural language and understanding of bugs are different, resulting in excessive noise that affects classification performance. By preprocessing bug reports (Tokenization, Stopword removal, Stemming), we convert bug reports into a text matrix with two dimensions, namely the bug report dimension and the word dimension  [9] ,  [18] . FS and IS are widely used techniques to remove the noisy or non-informative. For a given dataset in a certain application, FS aims to reduce the word dimension, which can obtain a subset of relevant words. Instance selection aims to reduce the bug report dimension, which can obtain a subset of relevant bug reports  [9] . In our study, we employ combination of FS and IS to get small-scale and high-quality training reduced set. The reduced set is considered as the representative of the original dataset, which can be handled more easily by automatic techniques than the original dataset. In our study, the orders of applying FS and IS are viewed as two different orders of bug reports reduction. We use FS IS to denote the bug data reduction, which first applies FS and then IS; on the other hand, IS FS denotes first applying IS and then FS. We briefly present how to reduce the bug reports based on FS IS in Algorithm 1, and the IS FS has the same process. To avoid the bias from a single algorithm, we examine results of four typical algorithms of feature selection algorithms (OneR, IG, CHI, and RF) and instance selection algorithms (CNN, MCS, ENN, and ICF), respectively. In Algorithm 1, we briefly present how to reduce the bug data based on FS IS. Lines 1-5, FS is applied to reduce the number of bug reports. When all the words of bug reports are removed, we remove the bug reports from dataset (lines 6-7). Lines 8-10, IS is applied to reduce the number of words, and we get the reduction dataset by line 11.

C. RSMOTE
In this section, we briefly present RSMOTE how to balance the imbalanced distribution of bug reports. As RSMOTE is an improve synthetic minority over-sampling Algorithm 1 FS→IS Input: T , original dataset, m I , the final number of bug reports, n F , the final number of words, Output: T FI , the reduction dataset. 

technique (SMOTE) to deal with imbalance distribution.
As can be seen in Figure  2 , SMOTE uses linear interpolation between two points to generate a new minority class sample, which limits the range of the sample generation. In order to solve this problem, the new synthetic minority activities are generated in the neighbourhood of the remaining minority-class examples in RSMOTE. Finally, two specified constraints control the new synthetic samples can be generated in robust manner. Compared with SMOTE, it has been improved obviously that the generalization ability of several classifiers by using the RSMOTE. The RSMOTE algorithm is explained as follows (Algorithm 2). Lines 1-4, we initialization parameters and calculate the imbalance degree of dataset (Im_D). For each bug report, we use the Euclidean distance to find the k nearest neighbours' bug reports, and randomly select Im_D bug reports from the k nearest neighbours' bug reports (lines 5-7). We generate new synthetic minority-class bug reports from high-dimensional space (lines 8-17 NN←nearsetNeighbors(s, k) //Use the Euclidean distance to find the k nearest neighbours. 

For each m in
M 9. newSample ← ∅ 10. newValues ← ∅ 11. For each a in A  12.  low ← value(s, a)-0.5 * abs(value(m, a)value(s, a))// abs is used to solve absolute value.  newValues ← newValues∪{(a, newValue)} 

D. FUZZY INTEGRAL
In this section, we first introduce the notation and definitions used in our work and then present an approach of fusing multi-classifiers with fuzzy integrals (FC-FI). Let Tr = {x|x ∈ R m } denote a training set, let Te = {x|x ∈ R m } denote a testing set, and let La = {La 1 , La 2 , . . . , La C } be a set of class labels, where C is the total number of class labels. Let E = {E 1 , E 2 , . . . , E L } be a set of classifiers trained on different training sets subTrs(subTrs = {Tr 1 , Tr 2 , . . . , Tr i , . . . , Tr L }), where L is the total number of training sets extended using the RSMOTE and E i is a basic classifier trained on the Tr i that has been extended using the RSMOTE approach (i ∈ [1, L]). For all x ∈ R m , E i assigns a class label from La to x. We define the classifier output as a C-dimensional vector consisting of support degrees for the classes, as follows  [26] . E i (x) = e i1 (x) , e i2 (x) , . . . , e ij (x) , . . . , e iC (x) (2.1) where e ij (x) ∈ [0, 1] (1 ≤ i ≤ L, 1 ≤ j ≤ C) denotes the support degree assigned by classifier E i that x belongs to class La j . In this paper, e ij (x) is an estimate of the posterior probability p(La c |x). In the following, we will present some related definitions. Definition 1: Given E = {E 1 , E 2 , . . . , E L }, La = {La 1 , La 2 , . . . , La C }, and Te = {x|x ∈ R m }, for each x ∈ Te, the decision profile matrix is shown as follows: DP (x) =         e 11 (x) • • • e 1j (x) • • • e 1C (x) . . . . . . . . . . . . . . . e i1 (x) • • • e ij (x) • • • e iC (x) . . . . . . . . . . . . . . . e L1 (x) • • • e Lj (x) • • • e LC (x)         (2.2) where the ith row of the matrix is the output of classifier E i and the jth column of the matrix consist of the support degrees from all classifiers E 1 , E 2 , . . . , E L for class La j . Definition 2: Given E = {E 1 , E 2 , . . . , E L }, E i E, i [1, L], let g i = g({E i }) . g i is called the fuzzy density of classifier E i . We use the following formula to calculate g i : g i = p(E i ) L k=1 p(E k ) × d sum (2.3) where p(E i ) is the validation accuracy of E i and d sum is the desired sum of fuzzy densities. Definition 3: Given E = {E 1 , E 2 , . . . , E L }, let P(E) be the power set of E. The fuzzy measure on E is a set function g: P(E) → [0, 1] such that g(∅) = 0, g({E}) = 1. (2.4) For ∀A, B ⊆ E, if A ⊂ B, then g(A) ≤ g(B). (2.5) Definition 4: Given E = {E 1 , E 2 , . . . , E L }, g is called a λ-fuzzy measure. The any subset (A k , k [1, L]) of g could be calculated by the following formulas. g (A 1 ) = g ({E 1 }) = g 1 , g ({E k }) = g k , λg (A k ) = g k + g (A k-1 ) + ×g k × g (A k-1 ) (2.6) where λ > -1 and λ = 0. The value of λ can be determined using the following formula: λ + 1 = L i=1 1 + ×g i (2.7) Definition 5: Given E = {E 1 , E 2 , . . . , E L }, g is the fuzzy measure on E, the Choquet fuzzy integral of function f j : E → [0, 1] with respect to g is defined as follows  [44] . The probability of the test sample in Te belongs to La j (u j ), j ∈ [1, C], which is calculated by combine the results of each sub-classifier with fuzzy integrals. u j = (C) fdg = f j (E 1 ) + L i=2 f j (E i-1 ) -f j (E i ) ×g(A i-1 ) (2.8) where 0 ≤ f (E 1 ) ≤ f (E 2 ) ≤ . . . ≤ f (E L ) ≤ 1, f (E 0 ) = 0, A i ⊆ E, A i = {E 1 , E 2 , . . . , E i }, g(A 0 ) = 0. In Algorithm 3, we briefly present how to use Chouqet fuzzy integral to integrate multi-RSMOTE. The algorithm 3 has two main stages: training process and integrated process, which is explained in detail in the follow. In training process, lines 1-5, we apply RSMOTE approach to multiple sampling, then we train classifiers respectively with the balanced dataset obtained from multiple samplings. Then, we calculate the fuzzy densities based on the classification results of each classifier (lines 6-9). In integrated process, Lines 10-16, we calculate a decision profile (DP), which is based on fuzzy densities of the corresponding classifiers. We sort each line of DP in descending order to obtain a new decision profile matrix DP . Then, lines 17-19, we calculate the fuzzy measure based on DP'. Finally, we calculate the probability of the test bug reports for each category, and select the max value as the category of bug report (lines 20-23).

III. EXPERIMENTAL DESIGN
The experimental design used to validate the performance of our approach is described in this section.

A. EXPERIMENTAL DATASET
We perform experiments on datasets from three open source projects, which are Eclipse  [39] , Mozilla  [40] , and GNOME  [41] . The projects are selected based on three criteria. Firstly, all the projects have a large number of reported issues, which is essential for a good research in the topic. Secondly, all the projects use Bugzilla  [42]  as an issue tracking system, which leads to an easier manual labeling process. Thirdly, the projects are different from one another in the application domain, which is essential for a general investigation since the distribution of high-impact bugs can be very different in different application domains. We selected ten components from three bug repositories in this study to validate FC-FI approach are presented in Table  1 . Severe bug reports include high-severity (e.g., 'blocker', 'critical', 'major') that represents critical errors and non-Severe (e.g., 'minor', 'trivial') that denotes unimportant bugs  [18] .  For each c in C 20. U c ← U c ∪ calculateUc// calculating the probability for each category by using equation (2.8),  21 .

end for


22.
K ← K ∪ arg max 1≤c≤C {U c } // determine the category of te 23. end for  24.  return K According to the results of  [18] , summary attribute of the bug reports contains useful and precise information of the bug, thus, in our study, we select the summary as the text content for classification  [28] .

B. EXPERIMENTAL SETUP
In our study, we compare the performance of RSMOTE to deal with the imbalance distribution of bug reports with four well-known strategies (RUS, ROS, SMOTE, CMA)  [2] . To evaluate the classifiers ensemble performance of FC-FI, we used well-known standard ensemble methods mentioned in the literature, including AdaBoost, bagging, and majority voting  [44] ,  [45] . We used stratified 3-fold cross-validation to validate the performance of our approach. In stratified 3-fold crossvalidation, each dataset is divided into 3 folds, with each fold containing the same proportion of sample instances belonging to each class. Next, 3 evaluation rounds are performed; in each round, two folds are used as the training dataset, and the remaining fold is used as the testing dataset. The results of all 3 evaluation rounds are aggregated to report the overall performance. Stratified cross-validation is a standard evaluation setting that is widely used in software engineering studies  [46] ,  [47] . In our experiments, we let the value of k be 5, and let N = round (M ) -1, where round (M ) represents the rounded imbalance degree M of the original bug reports. Our approach was implemented in the JAVA programming language for JDK 1.8 and executed on a machine with the following configuration: Intel R Xeon R CPU E5-2620 v3 @ 2.40 GHz, 16 G of RAM, running Windows 10. Many classification algorithms have been studied in the data mining field, each of which behaves differently in the same situation depending on its specific characteristics. In this study, we used six classifiers, namely, Naïve Bayes (NB), Naïve Bayes multinomial (NBM), Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Decision Tree (J48), and Random Tree (RT), and, as implemented in the Weka toolkit  [48] .

C. EVALUATION METRICS
We use accuracy,precision, recall and the F-measure as our evaluation metrics. These metrics are commonly used measures for evaluating classification performance  [23] . They can be derived from the confusion matrix, which captures all four possible classification results, as presented in Table  2 . The number of true positives (TP) is the number of verified surprise test reports that are correctly classified. The number of false positives (FP) is the number of verified non-surprise test reports that are incorrectly classified as surprise test reports. The number of false negatives (FN) is the number of verified surprise test reports that are incorrectly classified as non-surprise test reports. The number of true negatives (TN) is the number of verified non-surprise test reports that are correctly classified as non-surprise test reports. Based on the values of TP, FP, FN, and TN, the precision, recall and F-measure are calculated as follows. • Accuracy: The accuracy of the model is the number of correct classifications divided by the total number of classifications. The accuracy is defined as follows: Accuracy = TP + TN TP + FP + TN + FN × 100%. (5.1) • Precision: Precision is the proportion of correctly predicted surprise bug reports to all bug reports predicted as surprise. We formally define the precision as follows: Precision = TP TP + FP × 100%. (5.2) • Recall: Recall is the proportion of the number of correctly predicted surprise bug reports to the actual number of surprise bug reports. Mathematically, recall is defined as: Recall = TP TP + FN × 100%. (5.3) • F-measure: F-measure is a summary measure that combines both precision and recall. It evaluates if an increase in precision (recall) outweighs a reduction in recall (precision). Mathematically, the F-measure is defined as follows: F-measure = 2 × Precision × Recall Precision + Recall × 100%. (5.4)

IV. EXPERIMENTAL RESULTS
In this section, the experimental results are discussed in relation to the specific research questions.

RQ1. Which variants of FS and IS with classifiers perform the best performance for identifying severity of bug reports?
In the first research question, we consider four feature selection (OneR, IG, CHI, RF) and four instance selection (CNN, MCS, ENN, ICF) approaches with six classification algorithms (RT, NB, NBM, KNN, SVM, J48), which can have totally 48 variants (i.e., combinations of one of the FS or IS approach with one of the classification algorithms). For each FS and IS approach, we select 30%, 50%, 70%, and 90% as the ratio of the final number of words, respectively. The ratio value set up is based on the study of text instance selection  [16] . We use two evaluation metrics mentioned above (accuracy, and F-measure) to compare the totally 48 variants. Tables 3-6 presents the performance of the FS and IS approaches with six classifiers to identify the severity of bug reports, respectively. We bold the best results of FS and IS for each variant. From Tables  3 4 5 6 , it can be found that the performance of identifying severity of bug reports by using four FS and four IS approaches all works much better than the original experiment, except identifying severity of DS-E4 by four IS approaches. For example, as shown in Table  3  and 4 , the accuracy and F-measure of CHI with 70% as the ratio of the final number of words to identify severity of bug reports for DS-E1 is 0.79 and 0.79. And, the accuracy and F-measure of original bug reports to identify severity of bug reports for DS-E1 is 0.78 and 0.75. The average best performance of four feature selection (OneR, IG, CHI, RF) are better than four instance selection (CNN, MCS, ENN, ICF) approaches to identify the severity of bug reports. From Table  3 -6, we can see that the performance of identify the severity of bug reports is relatively stable, while FS under different reduction scales (30%, 50%, 70%, and 90%), however, the performance of identify the severity of bug reports becomes very poor, while IS under the reduction scale of 30%. This is caused by the bug reports removed after IS includes large number of words, and the available information to build the classification model  is too small. For all datasets, the highest accuracy of identifying the severity by using four FS approaches (OneR, IG, CHI, RF) is higher than original dataset 1.50%. Identifying the severity by using four IS approaches (CNN, MCS, ENN, ICF) and original datasets can get almost the same performance. In addition, the scales of datasets (including the number of words and the number of bug reports) has decreased. From Tables  3 4 5 6 , we can get that using FS and IS with classifiers perform the better performance than original datasets for identifying severity of bug reports. Furthermore, for each FS and IS approach, it is possible that different ratios can get different results. Thus, in the follow experiments, we set the percentages of selected words and bug reports based on the percentages, which get best performance in this experiment.

RQ2. Compare with individual FS and IS, what the performance of data reduction (combine FS and IS) to identify the severity of bug reports?
According to RQ1, we can see that FS can reduce the number of words and IS can reduce the number of bug reports, and both FS and IS can improve the performance of identify the severity of bug reports. Compare with individual FS and IS, we want to reduce bug report dimension and the word dimension at the same time. Thus, in this research question, we employ combination of FS and IS with six classification algorithms (RT, NB, NBM, KNN, SVM, J48) to get smallscale and high-quality training reduced set, which is based on the basis results of RQ1. We use two evaluation metrics mentioned above (accuracy, and F-measure) to compare the performance of FS, IS, FS→IS, and IS→FS. We bold the best results of feature selection and instance selection for each variant. From Tables  7  and 8 , we can see that the best performance of identify the severity of bug reports by using FS→IS and IS→FS are better than performance of using individual FS, IS, and original datasets, except DS-E3, and the accuracy of DS-E3 by using FS→IS and IS→FS is 0.68, and 0.67, and the accuracy of original DS-E3 is 0.70. In addition, for most datasets, the performance of using FS→IS is better than IS→FS, except DS-E4, and the accuracy of DS-E4 by using FS→IS is 0.80, and the accuracy of DS-E4 by using IS→FS is 0.82. The average performance of identifying the severity of bug reports from high to low is FS→IS, FS, IS, and IS→FS, respectively. However, for DS-E4, IS→FS could get the highest performance of identify the severity of bug reports, the accuracy of IS→FS is 0.82 and the F-measure of IS→FS is 0.78. From the Tables  7  and 8 , we can get that, FS→IS could not only reduce bug report dimension and the word dimension at the same time, but also improve the performance of identify the severity of bug reports. Compare to FS→IS, FS, and IS, the IS→FS get the worse performance, which is caused by the bug reports removed after IS includes large number of words, and the available information to identify the severity of bug reports is too small after FS. RQ3. Can RSMOTE improve the performance of identifying the severity of bug reports with an imbalanced distribution? Typical classification techniques may be adversely affected when the distribution of the bug reports is imbalanced. In order to solve this problem, in this study, we propose RSMOTE to weaken the imbalanced degree of bug reports. Thus, in this research question, we want to investigate which variants of imbalanced learning strategy and classifier perform the best for identifying the severity of bug reports. We consider five imbalanced learning strategies (RSMOTE, RUS, ROS, SMOTE and CMA) and six classification algorithms (NB, NBM, SVM, KNN, RT and J48), we can have totally 30 variants (i.e., combinations of one of the imbalanced learning strategies and one of the classification algorithms). Therefore, in this research question, we want to investigate which variants perform the best for identifying the severity of bug reports. Tables 9-10 present the performance of RSMOTE and the imbalanced learning strategies with six classifiers, respectively. We use the two evaluation metrics mentioned above (accuracy, and F-measure) to compare the totally 30 variants. In Tables 9-10, to show the variants with the best performance, we highlight the highest result values in bold based on the results of the six classifiers for each imbalanced learning strategy. The highest classification results for each imbalanced learning strategy are duplicated in the MAX_ACC column, and the highest results in the MAX_ACC column are further highlighted in bold. To illustrate the overall improvement enabled by each imbalanced learning strategy for all six classifiers, the average of the results obtained by all six classifiers using each imbalanced learning strategy is reported in the AVG_ACC column, and the highest results in the AVG_ACC column are further highlighted in bold. From Tables 9-10, we observe that RSMOTE approach could get maximum performance (MAX_ACC column) of identify the severity of bug reports with imbalance distribution than RUS, ROS, SMOTE and CMA. The RSMOTE could achieve the average maximum accuracy is 0.81, which was higher than imbalanced learning strategies (SMOTE, ROS, RUS, CMA, Original) by 3.83%, 5.02%, 8.93%, 2.19%, and 3.10%, respectively. The AVG_ACC column shows the generalization ability of ILS to balance the imbalanced distribution of bug reports. We can observe that the RSMOTE could achieve the average accuracy is 0.76, which was higher than imbalanced learning strategies (SMOTE, ROS, RUS, CMA, Original) by 3.01%, 5.10%, 11.84%, 3.01%, and 2.83%, respectively. Thus, it could represent that the balanced dataset by RSMOTE approach have better generalization capabilities than Original, SMOTE, RUS, and ROS. From Tables 9-10, we can find that RUS get the worse performance to balance the imbalance distribution. This is due to RUS removes the majority-class samples in dataset, which could loss information of original dataset. From Tables 9-10, we can see that SVM achieves the highest average performance and NB get the worse performance to identify the severity of bug reports after ILS balance the imbalance distribution. The highest average accuracy is 0.78, which was higher than other classifiers (RT, NB, NBM, KNN, J48) 4.87%, 4.27%, 1.20%, 11.16%, and 4.97%, respectively. Furthermore, different variants have significantly different performance of identifying the severity bug reports. For example, RSMOTE+J48, CMA+(NB or J48), and ROS+J48 are the top-3 best performing variants for identifying the severity of bug reports for DS-M6. RSMOTE+KNN, CMA+KNN, and Original+NBM are the top-3 best performing variants for identifying the severity of bug reports for DS-G8. From these results, we observe that in most cases, the classifiers achieved higher performance with RSMOTE than with the other ILS (RUS, ROS, SMOTE, and CMA). Therefore, RSMOTE approach could effectively improve the performance of identifying surprise bug reports of camel and wicket. RQ4. Can combine RSMOTE with data reduction could improve the performance of identifying the severity of bug reports. Due to original dataset are imbalance distribution, which could affect the basic classification performance of identifying the severity of bug reports. Based on RQ3, we can get that RSMOTE approach has better generalization capabilities than ILS (SMOTE, RUS, CMA and ROS). However, the random sampling uncertainty of RSMOTE and original dataset could contain large amounts of noise data. In order to solve this problem, according to RQ2, we could observe the best scale of data reduction (FS and IS) for each dataset, which could effectively reduce the noise and improve the performance of identify the severity of bug reports. Thus, in this research question, we employ combination of data reduction with RSMOTE to identify the severity of bug reports. To answer this question, Firstly, we use the RSMOTE approach to balance the imbalance distribution of original datasets. Then, we use data reduction (combine FS and IS) to get small-scale and high-quality training reduced set from balanced datasets by RSMOTE, which could not only reduce bug report dimension and the word dimension at the same time, but also improve the performance of identify the severity of bug reports. We select the scale of data reduction based on the previous research questions (RQ2). Finally, we use the classification algorithms, which can be any one of six popular text classification algorithms (RT, NB, NBM, KNN, SVM, J48), to build classifiers. We use the two evaluation metrics mentioned above (accuracy and F-measure) to verify the performance of combining RSMOTE with data reduction to identify the severity of bug reports. We bold the best results for each variant. From Tables  11  and 12 , we can see that the best performance of identify the severity of bug reports by using combine RSMOTE with data reduction (FS→IS) are better than performance of using individual data reduction, RSMOTE, RSMOTE with data reduction (IS→FS), and original datasets, except DS-E2 and DS-M6. In addition, for most datasets, the performance of RSMOTE with data reduction (FS→IS) and RSMOTE is better than RSMOTE with data reduction (IS→FS). This is caused by the bug reports removed after IS includes large number of words, and RSMOTE has little available information to balance the imbalance distribution after FS. From Tables  11  and 12 , we can find that using RSMOTE with data reduction (FS→IS) to identify the severity of bug reports, most variants achieve the average values of 0.74-0.89 in terms of accuracy, and 0.73-0.88 in terms of F-measure. From the Tables  11  and 12 , we can get that, using RSMOTE with data reduction (FS→IS) to identify the severity of bug reports could not only balance the imbalance distribution of bug reports, but also bug report dimension and the word dimension at the same time. There could reduce the impact of noise on identifying the severity of bug reports, while reducing reduce bug report dimension and the word dimension. RQ5. Can the multi-classifier fuzzy-integral with RSMOTE approach outperform state-of-the-art approaches? As discussed in regard to RQ4, RSMOTE with data reduction (FS→IS) can effectively improve the performance of identifying the severity of bug reports. in order to solve the random sampling uncertainty of RSMOTE, we use FC-FI approach to ensemble multi-RSMOTE. To better demonstrate the superiority of FC-FI approach, in this experiment, we compared the fusion of multi-RSMOTE with fuzzy integral approach with three classic classifier ensemble approaches: voting, bagging, and AdaBoost. In order to compare the performance of integrated methods, we use the balanced dataset obtained from RSMOTE approach. We used the accuracy and F-measure evaluation metrics defined above to verify the performance of the multi-classifier fuzzy-integral RSMOTE approach. We bold the best results for each variant. In terms of the accuracy and F-measure values, the average performance of FC-FI approach is better than the performances of the RSMOTE with data reduction, major voting, bagging, and AdaBoost approaches with RSMOTE. In Tables  13 14 , which shows the severity prediction results  for the Eclipse bug reports, the average accuracy of fusion of multi-RSMOTE with fuzzy integral approach is higher than the average accuracies of the RSMOTE with data reduction, major voting, bagging, and AdaBoost approaches by 3.00%, 4.82%, 6.62%, and 4.70% respectively, and the average F-measure is similarly higher by 2.00%, 5.24%, 6.25%, and 3.59%, respectively. These experiments show that the performance of the proposed fusion of multi-RSMOTE with fuzzy integral approach achieves better performance than all three classic classifier ensemble approaches (voting, bagging, and AdaBoost).

V. RELATED WORK
Automatic classification technology to support the bug reports could minimize the latent damage of software project in software maintenance activities. Antoniol et al.  [22]  builds automatic text classification techniques to identify whether a bug report contains real bug or not. They find that alternating decision trees, Naive Bayes classifier, and logistic regression could effective identify real bug from other kinds of bug reports. Menzies and Marcus  [23]  proposes an automated approach, namely, SEVERity to help the test engineer assign the severity levels to bug reports, which is based on standard machine learning techniques and text mining. Guo et al.  [38]  transfers the knowledge of labeled bug reports from bug repositories (Eclipse, Mozilla, GNOME) to identify the severity of Android bug reports, which compensates for the lack of labeled information for Android bug reports. Xia et al.  [43]  proposes a ELBlocker to classification the blocker bug reports from bug repositories. According to the imbalance degree of original dataset, ELBlocker divides original dataset into multiple disjoint sets. Then ELBlocker combines the results of multiple classifiers, which are trained by multiple disjoint sets. Xuan et al.  [35]  analyzes the commenters of bug reports, and represents a ranking method to automatic recommend developer to solve the new bug reports in bug repository. Yang et al.  [19]  compares the performance of four imbalance learning strategies and four classification algorithms to identify the high-impact bug reports with imbalanced distribution. In order to reduce the effort of bug report triage, Anvik and Murphy  [25]  proposes an approach to assist triager to recommend the developers, which is based on machine learning. Tian et al.  [49]  considers multiinformation (''temporal,'' ''textual,'' ''author,'' ''relatedreport,'' ''severity,'' and ''product'') to recommend the priority level of bug reports. Zhang et al.  [28]  proposes an automatic method to identify the severity of bug reports and fixer recommendation. Firstly, for a new coming bug report, they find the top k nearest neighbors' bug reports based on K Nearest Neighbor (KNN) classification alogrithm. Then, they identify the severity of bug reports and fixer recommendation by extracting their features (e.g., assignees and similarity). Feng et al.  [30]  proposes a test report prioritization method to assist crowdsourced testing, which is based on three strategies. The three strategies could find the riskiest and most diverse bug reports to assist the developer find bugs, respectively. In their subsequent article, Feng et al.  [31]  considers that the bug reports with screenshots and descriptive text. They could assist inspections of crowdsourced bug reports by using a multi-objective optimization-based prioritization technique. Wang et al.  [32]  proposes a clusterbased classification approach to identify ''good'' bug reports from the crowdsoured testing bug reports. The approach could overcome the local bias of crowdsoured testing bug reports. In their subsequent article, Wang et al.  [33]  proposes a method named Local-based Active Classification (LOAF) to address bug reports with the local bias problem and the lack of labeled information in crowdsoured testing. To improve the data quality, Khoshgoftaar et al.  [34]  and Gao et al.  [17]  use six feature selection approaches to reduce the bug dataset, meanwhile, use three data sampling approaches (RUS, ROS, SMOTE) to handle imbalanced defect data. Shivaji et al.  [6]  proposes a framework to examine multiple feature selection algorithms and remove noise features in classification-based defect prediction. Besides feature selection in defect prediction. Kim et al.  [36]  introduced an approach to measure the noise resistance in defect prediction and how to detect noise data. Xu  [27]  uses seven approaches to reduce the dimension of dataset, which could improve the performance of classification results. Yang et al.  [29]  uses three commonly feature selection, IG, CHI, Correlation Coefficient to reduce the noise from 4 opensource components from Eclipse and Mozilla.  Zhang [3958]  represents a concept profile-based approach to assign the severity of bug reports. They build the concept profiles based on historical bug reports. Then, the new bug reports come, they assign the severity of new coming bug report by measuring the similarity and severity concepts.

VI. CONCLUSION
In the context of software maintenance, a bug with high severity is typically associated with fatal errors and crashes. Therefore, the automatic prediction of the severity of bug reports could considerably reduce manpower requirements and improve the efficiency of bug resolution. However, two challenges often arise during the automated technique to identify the severity of bug reports: (1) high-dimensionality (both bug reports and words) is found in most bug repositories, and (2) class imbalance occurs for those datasets. In this paper, we address the problem of data reduction and data imbalance distribution for identify the severity of bug reports. We combine FS with IS to simultaneously reduce data scale on the bug report dimension and the word dimension to get small-scale and high-quality set of original dataset. Then, we improved random oversampling technique, named RSMOTE, is presented to weaken the imbalancedness degree of severity bug reports. And further, to avoid the uncertainty of random over-sampling, we develop an ensemble learning algorithm, which is based on Choquet fuzzy integral, to combine multiple RSMOTE. We empirically investigate the performance of data reduction on 10 datasets of three large open source projects, namely Eclipse, Mozilla and GNOME. The results show that our approach can effectively reduce the data scale and improve the performance of identify the severity of bug reports.