Introduction
The mixture composed of nanoparticles dispersed through a base liquid, which is called nanofluid, plays a significant role in MEMS & NEMS. It should be mentioned that the nanoparticles concentration have be small to avoid from undesired behaviors such as settlements. The Brownian motions of nanoparticles are able to increase the convection mechanism through the base fluid; however their main important effect would be improve the mixture effective thermal conductivity. That implies the better performance of nanoparticles with more thermal conductivity coefficient. This fact has been reported in many articles for various types of nanoparticles  [1] [2] [3] [4] [5] [6] [7] [8] . Meanwhile, the amount of nanoparticles concentration has an important influence on the mixture thermal behavior especially at different values of working temperature. Other effective thermo-physical properties can be addressed as like the nanoparticles density, specific heat and their shapes and diameters. It is well known that in addition of nanofluid thermal conductivity, the mixture effective viscosity should be evaluated due to its performance in a flow pumping power; so that a large number of works can be referred concerned these aspects of nanofluid properties. The evaluations of nanofluid thermo-physical properties through various experimental studies, have been followed by the numerical works; which had tried to predict the properties by using the suitable correlations. These correlations outputs might show some detour versus experimental ones; however less costs of researchers encouraged researchers to follow that field to present more accurate correlations at various working conditions  [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] . The expensive steps of stabilize and dispersing the nanoparticles, are able to be ignored through the numerical works; moreover all the nanofluid behaviors can be predicted by a simple optimization approach according to the available empirical results. Among these optimization approaches, the artificial neural network (ANN) was used in many studies to predict the nanofluid properties especially for its thermal conductivity and viscosity. Moreover appropriate accuracy of this method encouraged researchers to develop more new models of ANN to have better consistency with the existence physical conditions of a nanofluid flow . In the following, 24 dissimilar ANN methods are examined at present work by introducing the suitable architectures and training algorithms of these using approaches. To do this, the MSEs between the targets and the ANN outputs of the examined methods are evaluated while the results would be supported by the appropriate sensitivity analysis.

Problem statement
The artificial neural network (ANN) optimization method is evaluated according to the experimental results of Bahrami et al.  [10]  concerned the hybrid non-Newtonian nanofluid of iron (Fe) and copper oxide (CuO) in a binary mixture of water and ethylene glycol (see Fig.  1 ). They reported the mixture dynamic viscosity, µ (mPa.s), versus shear rate, γ (1/s), at different amounts of nanoparticles volume fraction (φ = 0.25 to 1.5%) and temperate (T = 25 to 50 • C). Present work novelty is demonstrated by providing 24 dissimilar ANN methods to introduce the suitable architectures and training algorithms for them . The mean squared errors (MSEs) between the targets and ANN outputs are evaluated to present the best optimization approach among them. Meanwhile the results would be supported by the appropriate sensitivity analysis to have better statistical visual presentation.

Feed-forward multilayer ANN
Artificial Neural Networks (ANNs) are appropriate tools for the function approximation. Due to their approximation capabilities, the ANNs can be employed as universal function estimators. In other words, the ANNs can approximate a wide range of functions with any given precision  [12, 13] . In practice, usually feed-forward multilayer ANNs having sigmoid neurons in the hidden layer and linear neurons in the output layer are used for the curve fitting and interpolation. However, it is usually difficult to select the best architecture and training algorithm. This is due the fact that the best combination of the ANN architecture and the training algorithm depends on several factors such as the complexity of the desired functions, the size of the available input-output datasets and the expected accuracy and precision of the resultant models. In this section, a variety of the ANN is examined in order to determine which architecture and training algorithm is more suitable for estimating the nanofluid features. In this paper, the required model has three attributes (i.e., the solid concentration, the temperature and the shear rate) as the inputs and one attribute (i.e., the nanofluid viscosity) as the target. A set of 204 input-output experimental data is available. It is desired to find the best combination of the ANN architecture and the training algorithm in order to minimize the Mean Squared Errors (MSEs) between the targets and the ANN outputs. To that end, a variety of feed-forward multilayer ANNs having dissimilar number of layers, number of hidden layer neurons and training algorithms are examined including: • One hidden layer with 5, 10, 15 and 20 neurons • Two hidden layers with 5-5, 10-5, 5-10 and 10-10 neurons • The Levenberg-Marquardt (LM), Scaled Conjugate Gradient (SCG) and Bayesian Regulation (BR) backpropagation methods. It should be noted that the selected backpropagation algorithms are the most effective methods for small-size datasets. Suppose that the unknown variables (weights and biases) of an ANN are denoted by the vector x T = [ x 1 x 2 • • • x n ] . The learning rules for the aforementioned algorithms are as follows: • For the LM algorithm: x k+1 = x k -[J T k J k + µ k I ] -1 J T k e k ( 1 ) where J k = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ∂e 1 ∂x 1 ∂e 1 ∂x 2 • • • ∂e 1 ∂x n ∂e 2 ∂x 1 ∂e 2 ∂x 2 • • • ∂e 2 ∂x n . . . . . . . . . . . . ∂e N ∂x 1 ∂e N ∂x 2 . . . ∂e N ∂x n ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ (2) in which e T = [ e 1 e 2 • • • e N ] where e is the error between any real output and its desired value. The parameter µ k controls the algorithm speed and convergence: when the errors are increased, the parameter µ k is magnified to accelerate the learning while when the errors are decreased, the parameter µ k is reduced to guarantee the convergence. • For the SCG algorithm: x k+1 = x k + α k p k (3) in which the α k is the learning rate and p k is the search direction. The search direction is updated as follows: p k = -g k + β k p k-1 (4) with β k = g T k g k g T k-1 g k-1 , β 0 = 0 (5) and g k = ∇F (x)| x k . (6) In every step, the learning rate α k should be selected so that Eq. (  4 ) is minimized along the search direction. • For the BR algorithm, the LM algorithm is employed while the following cost function should be minimized: F (x) = βe T e + α n ∑ i=1 x 2 i . (7) In all cases, the ''tansig'' activation functions are used for the hidden layer(s) and the ''linear'' activation function is used for the output layer. The initial conditions of the weights and bias of the ANN are randomly selected. Therefore, every ANN is trained 20 times, and its average performance is reported. Also, data division is random. A preprocessing scheme is performed on the input and target data that map the data means to 0 and deviations to 1. The LM, SCG and BR training algorithms are selected for this problem because they have the best performance for the function approximation. The training parameters of the LM, SCG and BR algorithms are reported in Table  1 . For more information about these training algorithms, one can refer to Ref.  [27] .

Results and discussions
In this paper, 24 dissimilar ANNs are examined. The architectures and training algorithms of these ANNs are listed in Table  2 . Also, the MSEs between the targets and the ANN outputs of the examined ANN are reported in Table  2 . It should be noted that the time required for the network training has not been taken into consideration because the size of the dataset is not so high that there are concerns about the computational time and cost.

Table 1
The training parameters of the LM, SCG and BR algorithms. The results indicate that the LM and BR have better performance than the SCG in all of the investigated architectures. The performance of the LM and BR are comparable; however, the BR outperforms the LM in several cases. The simple architectures provide acceptable models. Three-layer ANNs do not necessarily have better performance than two-layer ANNs. Also, increasing the hidden neurons may slightly improve the ANN performance; nevertheless, it can cause over-fitting when the architecture is more complex. In the investigated problem, the combination of the two-layer ANN with 15 hidden neurons and the BR training algorithm provide the best results. The regression plot for the resultant model is illustrated in Fig.  2 . It indicates a perfect fit. Also, the convergence of the training and test processes are depicted in Fig.  3 . Hence, it can be concluded that a two-layer ANN with moderate hidden neuron number and the BR training algorithm is preferred for similar problems. Once the desired model is obtained, it can be employed to interpolate throughout the trained envelope. For example, the results of the aforementioned ANN for the non-trained solid concentration of 0.25%, 0.5%, 0.75%, 1%, 1.25% and 1.5% are illustrated in Fig.  4 . Moreover, the error percentages between the experimental data and numerical results of the ANN for some solid concentrations are presented in Table  3 . The results indicate that the model can predict the nanofluid properties, properly. Finally, the trained ANN can be employed for the sensitivity analysis. For example, the sensitivities of the output (i.e., viscosity) with respect to the inputs (i.e., solid fraction, temperature and shear rate) at φ = 0.5 (%) are illustrated in Fig.  5 .  

Table 3
The error percentages between the experimental data and numerical results of the ANN for some solid concentrations. φ = 0.25 (%)  T ( • C) γ ( 1 s ) 25

Conclusion
Artificial neural network optimization method was evaluated according to the experimental results concerned hybrid non-Newtonian nanofluid of Fe/CuO in a binary mixture of water/Eg. Present work novelty was demonstrated by providing 24 dissimilar ANN methods to introduce the suitable architectures and training algorithms for them. The mean squared  errors (MSEs) between the targets and ANN outputs were evaluated to present the best optimization approach among them. Meanwhile the results were supported by the appropriate sensitivity analysis to have better statistical visual presentation.