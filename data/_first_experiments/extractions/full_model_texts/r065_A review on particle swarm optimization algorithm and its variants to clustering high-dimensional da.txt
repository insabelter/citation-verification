Introduction
In recent years there has been an explosive growth in the generation, acquisition and storage of data. This large amount of stored data contains valuable and important hidden knowledge, which could be used to improve the decision-making process of an organization. Extracting information and knowledge from this huge and continuously increasing amount of available data, has become a very complex task and has outpaced our capability to process, analyze, and understand it. Therefore, a process for converting large amounts of data to knowledge will become invaluable. The area of Knowledge Discovery in Databases (KDD) has arisen over the last decade to address this challenge. Data mining is a part of the KDD process. Technically, data mining is the process of extracting useful patterns or mining knowledge from large amounts of data through the application of appropriate algorithms, tools, and techniques. Clustering is one of the important and well-known data mining tasks  (Han and Kamber 2001; Pang-ning et al. 2006) . Clustering means the process of partitioning an unlabeled dataset into groups of similar objects. Each group, called a cluster, consists of objects that are similar to each other with respect to a certain similarity measure and which are dissimilar to objects of other groups. The applications of cluster analysis have been used in a wide range of different areas, including artificial intelligence, bioinformatics, biology, computer vision, data compression, image analysis, information retrieval, machine learning, marketing, medicine, pattern recognition, spatial database analysis, statistics, recommendation systems and web mining  (Kriegel et al. 2009; Luxburg 2007) . The challenge of clustering high-dimensional data has emerged in recent years. Clustering high-dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions. Such high-dimensional data spaces are often encountered in areas such as medicine, biology, bioinformatics, and the clustering of text documents, where, if a word-frequency vector is used, the number of dimensions equals the size of the dictionary. However, high-dimensional data poses different challenges for clustering algorithms that require specialized solutions. In particular, in high-dimensional data, traditional similarity measures, as used in conventional clustering algorithms, are usually not meaningful  (Kriegel et al. 2009; Steinbach et al. 2003) . Almost all research on specialized approaches to clustering high dimensional data is relatively new. Recently, some surveys have given overviews of some approaches. In the survey by  Parsons et al. (2004)  the problem is introduced in a very illustrative way and some approaches are sketched out. In  Kriegel et al. (2009)  the survey took a more systematic approach to the problem and focused on the different tasks and sub-problems (axis-parallel, pattern-based, and arbitrarily-oriented clustering). Common approaches for high dimensional cluster analysis either employ dimension reduction at a data preprocessing step or iteratively carry out dimension reduction and clustering. In particular, dimension reduction based on variable selection aims at identifying a subspace spanned by a small set of variables that include most of the relevant variables that generate the nature data clusters and excludes most irrelevant variables  (Steinbach et al. 2003; Parsons et al. 2004) . Variable selection in clustering has been approached from different perspectives: density-based methods such as  (Agrawal et al. 1998; Chan and Hall 2010; Raftery and Dean 2006) , model-based methods such as  (Pan and Shen 2007; Wang ans Zhu 2008) , and criterion-based methods such as  (Witten and Tibshirani 2010; Raftery and Dean 2006) . Many algorithms for large data sets have been proposed in the literature using different techniques. However, conventional algorithms have some shortcomings such as the slowness of the convergence and the sensitivity to initialization values, etc. and they still require much investigation to improve performance and efficiency. Clustering methods such as the K-means algorithm, soft clustering based on mixed models, hierarchical clustering, and spectral clustering have been developed and have proven to be successful in many real world applications  (Kriegel et al. 2009; Luxburg 2007; Steinbach et al. 2003; Parsons et al. 2004; Zhou and Shi 2011) . The K-means (KM) algorithm is one of the most popular and widespread partitioning clustering algorithms because of its superior feasibility and efficiency in dealing with a large amount of data. The main drawback of the KM algorithm is that the cluster result is sensitive to the selection of the initial cluster centers and may converge to the local optima  (Cui et al. 2005; Kao et al. 2008) . To overcome the drawbacks of these algorithms several new algorithms have been introduced  (Sandeep et al. 2011) . A Genetic Algorithm (GA) based on a mutation operator for clustering analysis was proposed by  Maulik and Bandyopadhyay (2002) . Another approach based on Simulated Annealing (SA) for data clustering was proposed by  Selim and Alsultan (1991)  and more recently the Particle Swarm Optimization (PSO) and hybrid PSO algorithms were proposed (Van der Merwe and Engelhrecht 2003;  Omran et al. 2006; Esmin et al. 2008; Sandeep et al. 2011) . Particle Swarm Optimization is a population-based stochastic algorithm proposed by  Kennedy and Eberhart (1995) , which is inspired by the social behaviors of animals like fish schooling and bird flocking. As a stochastic search scheme, PSO has characters of simple computation and rapid convergence capability. PSO has been successfully applied in several areas such as clustering problems  (Paterlini and Krink 2006; Chen and Ye 2004; Esmin et al. 2008; Ahmadi et al. 2010; Sandeep et al. 2011) , and image processing  (Niu and Shen 2006; Omran et al. 2006; Esmin and Matwin 2012) . Also the PSO has proved to be competitive with genetic algorithms in several tasks, mainly in optimization areas  (Silva et al. 2002; Kennedy 2002; Esmin and Lambert-Torres 2012; Zhao et al. 2005) . Recently, some surveys have given overviews of PSO and hybrid evolutionary algorithms for data cluster analysis. In  Sandeep et al. (2011)  the survey has given a general literature review of the PSO application in data clustering. In Hasan and Ramakrishnan (2011) the survey has given a review of hybrid evolutionary algorithms including PSO for cluster analysis. Both surveys focus on a general cluster analysis problem and most of the reviewed papers focus on the application of PSO to cluster simple data with low dimensional numbers. The main advantage of PSO is that it has fewer parameters to adjust. PSO finds the best value for interaction of particles, but when the search space is high its convergence speed becomes very slow near the global optimum. It also shows poor quality results when it deals with large and complex data sets. PSO often failed in searching for a global optimal solution in the case where the objective function has a large number of dimensions. The reason for this phenomenon is not only the existence of the local optimal solutions, but that the velocities of the particles sometimes lapsed into degeneracy, so that the successive range was restricted in the sub-plain of the whole search hyper-plain  (Kao et al. 2008; Gao and Xu 2011; Thangaraj et al. 2012) . The issue of local optima in PSO has been studied and different variants of PSO algorithm were proposed. Some of these variants have been proposed in order to incorporate the capabilities of other evolutionary algorithms, such as hybrid versions of PSO or the adaptation of PSO parameters, thereby creating the adaptive PSO versions. Many authors have considered incorporating selection, mutation, and crossover, as well as differential evolution, into the PSO algorithm. As a result, hybrid versions of PSO have been created and tested, including the hybrid of a genetic algorithm and PSO, a genetic programming-based adaptable evolutionary hybrid PSO, and evolutionary PSO  (Løvbjerg et al. 2001; Miranda and 123 Fonseca 2002; Esmin et al. 2005; Rashid and Baig 2010; Thangaraj et al. 2012) .  Hiqushi and Iba (2003)  proposed the PSO with Gaussian mutation (GPSO), whose global searching ability is more efficient than the standard PSO.  Ling et al. (2008)  proposed a hybrid PSO with wavelet mutation named HWPSO. Using another approach,  Sun et al. (2004a, b)  introduced quantum theory into PSO and proposed a quantum-behaved PSO (QPSO) algorithm, which can be guaranteed theoretically to find optimal solutions in a search space, and  Marinakis et al. (2008)  introduced an improved quantum-behaved particle swarm optimization algorithm with better convergence, called weighted QPSO (WQPSO). Although such improvements work well and have the ability to avoid a fall in the local optima, the problem of early convergence by the degeneracy of some dimensions still remains, even if there are no local optima. Hence the PSO algorithm does not always work well for the high-dimensional function as well as for the cluster of high-dimensional data  (Gao and Xu 2011; Thangaraj et al. 2012) . From this point of view, different variants of PSO have been proposed to improve the performance of the PSO algorithm in the case of high-dimensional functions. Therefore, the aim of this work is to give a survey by reviewing PSO algorithm and its variants as applied to clustering large and high-dimensional data. The rest of this paper is organized as follows: In the next section a brief description of the PSO is presented. In Sect. 3 the PSO variants are described. Section 4 offers a survey about the previous work done on the particle swarm optimization algorithm and its variants for clustering high-dimensional data. A brief discussion is also provided at the end of this section. Finally, conclusions are presented in Sect. 5.

Particle swarm optimization algorithm
The PSO algorithm is a population-based optimization method that tries to find the optimal solution using a population of particles  (Kennedy and Eberhart 1995; Shi and Eberhart 1998; Kennedy and Shi 2002) . Each particle is an individual, and the swarm is composed of particles. Some of the attractive features of the PSO include ease of implementation and the fact that no gradient information is required. In PSO, the solution space of the problem is formulated as a search space. Each position in the search space is a potential solution of the problem. Particles cooperate to find the best position (best solution) in the search space (solution space). Each particle moves according to its velocity. At each iteration, the particle movement is computed as follows: x i (t + 1) ← x i (t) + v i (t), (1) v i (t + 1) ← ωv i (t) + c 1 r 1 ( pbest i (t) -x i (t)) + c 2 r 2 (gbest (t)) -x i (t)) (2) In Eqs. (1) and (2), x i (t) is the position of particle i at time t, v i (t) is the velocity of particle i at time t, pbest i (t) is the best position found by particle itself so far, gbest(t) is the best position found by the whole swarm so far, ω is an inertia weight scaling the previous time step velocity, c 1 and c 2 are two acceleration coefficients that scale the influence of the best personal position of the particle (pbest i (t)) and the best global position (gbest(t)), and r 1 and r 2 are random variables within the range of 0 and 1  (Tsai and Chiu 2008) . Equations (  3 ) and (4) define how the personal and global best values are updated at time t, respectively. It is assumed below that the swarm consists of s particles and the objective function f is used to calculate the fitness of the particles with a minimization task.   Thus, i ∈ 1 . . . s pbest i (t + 1) = pbest i (t) if f ( pbest (t)) ≤ f (x i (t + 1)) x i (t + 1) if f ( pbest (t)) > f (x i (t + 1)) (3) gbest (t + 1) = min { f (y), f (gbest (t))} where, y ∈ { pbest 0 (t), pbest 1 (t), . . . , pbest s (t)} (4) The process of PSO is shown as Fig.  1 . Equation (2) consists of three terms. The first term is the current speed of the particle, which shows its present state and has the ability to balance the whole and search a local part. The second term is the cognition term, which expresses the "thought" of the particle itself and causes the swarm to have a strong ability to search the whole and avoid a local minimum. The third term is called the social term; it reflects the information sharing among the swarm and among the particles, leading the particles toward known good solutions. Under the influence of these three terms, the particles can reach an effective and best position. Figure  2  illustrated the schematic representation of updating the velocity (Eq. 2) of a particle as presented in  Ahmadi et al. (2010) . Two basic approaches to PSO exist based on the interpretation of the neighborhood of particles. Equation (2) reflects the global best (gbest) version of PSO where, for each particle, 123 the neighborhood is simply the entire swarm. The social component then causes particles to be drawn toward the best particle in the swarm. In the local best (lbest) PSO model, the swarm is divided into overlapping neighborhoods, and the best particle of each neighborhood is determined  (Shi and Eberhart 1998; Kennedy and Shi 2002) . The stopping criterion (termination conditions) mentioned in the aforementioned algorithm depends on the type of problem being solved. Usually, the algorithm is run for a fixed number of iterations (objective function evaluations) or until a specified error bound is reached. The description of how the algorithm works is as follows: initially, based on particle fitness information, one particle is identified as the best particle. Then, all the particles are accelerated in the direction of this particle but, at the same time, in the direction of their own best previously encountered solutions. Occasionally, the particles will overshoot their target, exploring the search space beyond the current best particles. All particles also have the chance to discover better particles en route, in which case, the other particles will change direction and head toward the new best particle. Because most functions have some continuity, chances are that a good solution will be surrounded by equally good, or better, solutions. By approaching the current best solution from different directions in search space, the chances that these neighboring solutions will be discovered by some of the particles are good  (Kennedy and Shi 2002; Tsai and Chiu 2008) . The PSO algorithm is very fast, simple and easy to understand and implement. It also has few parameters to adjust  (Kennedy et al. 2001) . PSO finds the best value with interaction of particle, but when the search space is high its convergence speed becomes very slow near global optimum. It also shows poor quality results when it deals with large and complex data set. As aforementioned, several PSO variants have been developed to overcome this problem.

PSO variants
There are several variants of the PSO algorithm available in the literature  (Sedighizadeh and Masehian 2009; Hasan and Ramakrishnan 2011; Sandeep et al. 2011) .  Sedighizadeh and Masehian (2009)  discussed the PSO' s Taxonomy and classified these variants according to cooperation, continuity, fuzziness, accordance, attraction, activity, mobility, divisibility, topology, hierarchy, restriction, objective, recursively, uncertainty etc. One of the variant was proposed by  Yao (2008) , a Cooperatively Coevolving Particle Swarm Optimization (CCPSO) algorithm for solving large-scale problem by breaking the problem into some smaller-scaled ones in a way that the internal dependencies of generated particles are in the possible least values. Then, these particles will become cooperated. In multidimensional or large problems the PSO does not work well. For solving these types of problems, a Cooperative Multiple PSO (CMPSO) was proposed by  Felix et al. (2007) . This algorithm has all conductivity and control properties of the PSO. In Orthogonal PSO (OPSO), Intelligent Move Method (IMM) is used for updating the velocity and this method provides the best result when dealing with optimization problem of large-scale parameter  (Ho et al. 2008 ). The Two Swarm-based PSO algorithm helps in escaping from local optimum and avoiding rapid convergence. The two different swarms are flown in space. The particles of these swarms follow different paths, which will enhance the capability of finding global optimum. At the same time other swarm will enhance local discovery by using roulette wheel selection based selection method  (Li et al. 2006a, b) . To achieve multiple optimizations, Niching PSO is used. Initially the individual particles are monitored and those particles whose fitness changes little is separated in each iteration and a sub swarm is formed. These sub swarms are useful in finding all the global and local optimums  (Brits et al. 2005) . In Optimized PSO, swarms within a swarm help in optimizing the free parameter of PSO. It has been shown by test results that this method performs better than other methods  (Meissner et al. 2006) . The balance between discovery and extract is very important in algorithm related to PSO. In Best Rotation PSO (BRPSO), multi-model functions are optimized by dividing single swarm into sub swarms. In BRPSO when best rotation is executed, stagnation on local minima is avoided by forcing populations to move from one local minimum to another one, increasing the exploration of the problem space between different local minima  (Alviar et al. 2007 ). PSO with spatial particle extension (SEPSO) algorithm uses species, which include a collection of similar particle in a sub swarm. The criterion for finding similarity is the Euclidean distance. When particles move in their species, a parallel multi-objective optimization takes place and helps in increasing convergence  (Krink et al. 2002) . In parallel PSO a parallel computation is used to reduce the total time when solving large-scale complex problem  (Chang et al. 2005) . To improve the efficiency of PSO a hybridization of PSO and harmony search is introduced for solving high dimensional problem  (Li and Li 2007) . Parallel synchronous PSO (PSPSO) algorithm performs the position and velocity updating at the end of each iteration using wholly simulating. It uses a constant load balancing in which the assigned task to each processor in the total time is determined  (Koh et al. 2005) . To solve constrained single objective problem, Constrained Optimization by PSO (COPSO) algorithm is used. These algorithm employees a technique to investigate constrained with the help of an external file to save particle information. This algorithm offers improvement in the best version by using an external procedure, which maintains swarm diversity and guidance  (Aguirre et al. 2007 ). Augmented Lagrangian PSO was proposed to solve the optimization problem with equal and unequal constraints  (Sedlaczek and Eberhard 2006) . Adaptive PSO (APSO) algorithm that introduce a solution to adaptively replace the current inactive particles with fresh particles in a way that the existing PSO-based relationships among the particles are kept  (Xie et al. 2002a, b) . In Adaptive PSO Guided by Acceleration Information (AGPSO) the concept of acceleration item was introduced in the position and velocity updating equations to improve the efficiency of PSO for finding the global optima  (Zeng et al. 2006) . Attractive Repulsive Particle Swarm Optimization (ARPSO) algorithm has been developed to remove the PSO's drawback in premature convergence. It includes an attractive phase and a repulsive phase. In the attractive phase, the addition operator is used among the equation terms for velocity updating. In the repulsive phase, the subtraction operator is employed  (Riget and Vesterstroem 2002) . In Trained PSO (TPSO) algorithm, the training of particle helps in reducing completion complexity and convergence time  (Gheitanchi et al. 2008 ). In Self-Organization PSO (SOPSO) algorithm, the inclusion feedback of agent improves the performance of particle in the next iteration by improving the discovery and extract. This algorithm avoids premature convergence of total swarm  (Jie et al. 2006) . In Active Target PSO (APSO) algorithm, in addition to two existing terms; namely the best position and the best previous position for particle velocity updating, a third term called 'Active target' is also utilized. Calculating the third term is complicated and it does not belong to the existing positions. This method maintains the diversity of the PSO as well as not trapping in the local optimum  (Zhang et al. 2008b) . Binary PSO (BPSO)-The difference between PSO and BPSO lies in their defined searching spaces. In the BPSO moving in the spaces means a change in the probability of the fact that the value of position coordinate is zero or one  (Kennedy and Eberhart 1997) . Angle Modulated PSO (AMPSO) algorithm employs a trigonometry function to generate a bit string. Its difference with the Binary PSO (BPSO) algorithm lies in its high computational efficiency. That is, it avoids the generation of high-dimensional binary vector and thus, 123 its discretion process is not complicated. Moreover, it changes all of the high-dimensional problems to a four-dimensional problem  (Pampara et al. 2005) . Combinatorial PSO (CPSO) algorithm is employed to optimize hybrid problems (consisted of continuous and integer variables)  (Jarbouia et al. 2007) . In Comprehensive Learning PSO (CLPSO) algorithm, a new velocity updating function is proposed and employed to construct CLPSO and then the new algorithm is tested using a group of benchmark functions  (Liang et al. 2006) . Dissipative PSO is used to solve the problem of searching the limited area, and this method helps in overcoming swarm's tendency to get equilibrium status. The concept of negative entropy is used for producing craziness among particles  (Xie et al. 2002a, b) . In Dynamic adaptive dissipative PSO (ADPSO algorithm, on the one hand, a dissipative is made for the PSO introducing negative entropy and on the other hand, a mutation operator is utilized to increase the variety in the swarm when it reaches an equilibrium condition in last runs. Thus, it generates an adaptive strategy for inertia weight in order to keep the balance between the local and global optimality  (Shen et al. 2007 ). The Dynamic neighborhood PSO (DNPSO) method has some modifications to the conventional PSO. In this method, instead of using the current Gbest in the PSO, another parameter, called Nbest, is utilized. This term is the best particle among the current particle's neighbors in a specified neighborhood  (Hu and Eberhart 2002) . Evolutionary Programming and PSO (EPPSO) algorithm is a combination of the PSO and EP. The combination of these two algorithms increase the PSO capability in making a balance between local and global search due to the faster convergence of the EP algorithm  (Wei et al. 2002) . Evolutionary Iteration PSO (EIPSO) algorithm is a combination of the PSO and Evolutionary Programming (EP). Thus, it is able to increase the computational efficiency of EP and it can avoid trapping the algorithm in local optimum  (Lee 2007) . In fuzzy PSO an explicit selection procedure is used together with PSO. The parameters are set using the self-adaptive characteristics. Different operators like mutation, replication, reproduction, evaluation and selection are utilized in this algorithm  (Shi and Eberhart 2001) . In Gaussian PSO, to visualize the random nature of PSO, the probability distribution of swarm is used, which allows better understanding of tuning the algorithm and depicting the weakness. The Gaussian distance of swarm moving from global best and local best is utilized in this algorithm  (Secrest and Lamont 2003) . In Fitness-to-Distance Ratio PSO (FDRPSO) algorithm, the particles are moved towards nearby particles of higher fitness instead of attracting each particle towards just the best position discovered so far by any particle. The direction in which each component of particle position needs to be changed is determined by using the ratio of relative fitness and the distance of other particle  (Peram et al. 2003) . In Hierarchical PSO, the neighborhood structure is defined by arranging particle in a dynamic hierarchy. The particle moves up or down this hierarchy, based on quality of their best solution till now  (Janson and Middendorf 2004) . Heuristic PSO derives that in HPSO the method of selecting next particle for updating its velocity and position is different from original PSO. The main objective of this algorithm is to increase the convergence speed  (Lam et al. 2007) . Hybrid particle swarm optimizer with mutation (HPSOM) algorithm is a combination of PSO and GA mutation. This process allows the search to escape from local optima and search in different zones of the search space. As a result, the proposed algorithm has the automatic balance ability between global and local searching abilities to guarantee the better convergence  (Esmin et al. 2005; Esmin and Matwin 2013) . Hybrid PSO with Simulated Annealing (SA) algorithm is a combination of PSO and SA. These two mechanisms have their merits and demerits individually, but when used in combination using SA they overcome the problem of each other  (Wang and Li 2004) . Predator Prey PSO (PPPSO) algorithm uses the definitions of predator and prey. In this algorithm prey tries to avoid predator. This helps in moving towards global optimum and avoiding local optimum  (Jarbouia et al. 2007) . Quadratic Interpolation PSO (QLPSO) algorithm uses a multi-parent, quadratic crossover/reproduction operator. In this algorithm, the swarm leader is determined in each iteration. Then, its partners are selected among other particles. Next, employing a crossover operation called "quadratic crossover", off spring production is performed. The new particle is accepted into the swarm when it is better than the best particle existing in the swarm  (Pant et al. 2007) . A Novel Hybrid PSO (NHPSO) algorithm is proposed to make the PSO more efficient in solving high-dimensional problems. The NHPSO use a hybridization of PSO and harmony search  (Li and Li 2007) . Principal Component PSO (PCPSO) algorithm is proposed to reduce the time complexity of the problem in high dimensions. In this algorithm, particles are flown in an n-dimensional space and contemporarily; they are flown in an m-dimensional space (m is less than n). That is, the particles are flown simultaneously in two separate spaces  (Voss 2005) .

PSO and its variants to cluster high-dimensional data
Clustering is an important problem that often must be solved as part of more complicated tasks in pattern recognition, image analysis, and other fields of science and engineering. Clustering is one of the main tasks in knowledge discovery from databases (KDD) and consists in finding groups within a certain set of data, where each group contains objects similar to each other and different from those of other groups  (Han and Kamber 2001; Pang-ning et al. 2006 ). In the past decade it has been proved by many researchers that the biologically or nature-inspired algorithms (Ant Colony Optimization Algorithm, Artificial Bee Colony Algorithm, Particle Swarm Optimization, Bird Flocking Algorithm, Frog Leaping Algorithm, Genetic Algorithm) are viable tools to solve complex optimization problems and can be successfully implemented for solving the clustering problems. Among them, the particle swarm optimization algorithm is very popular due to its flexibility and self-organization. The PSO combined with other techniques such as dimensionality reduction and subspace can be successfully applied to address the problem of clustering high-dimensional data  (Ahmadi et al 2007; Das et al. 2008; Kiranyaz et al. 2009; Hua and Pei 2010)  .

Literature survey
An implementation of PSO for document clustering was introduced in  Cui et al. (2005) . The PSO algorithm is used to perform a global search in the entire solution space. The PSO-Clustering algorithm can generate more compact clustering results than the K-means clustering algorithm.  Fun and Chen (2005)  introduced the hybrid PSO and K-means algorithm with a novel alternative metric; this is called the Alternative KPSO clustering (AKPSO) and it automatically detects the cluster centers of geometrical structure datasets. They presented an evolutionary particle swarm optimization learning-based method to optimally cluster N data points into K clusters.  Shen et al. (2005)  presented a mountain clustering based on an improved particle swarm optimization (MCBIPSO) algorithm. A mountain clustering method constructs a mountain function according to the density of the sample. The improved PSO algorithm is used to find all peaks of the mountain function. The simulation results show that the MCBIPSO algorithm is successful in deciding the density clustering centers of data samples.  Omran et al. (2006)  proposed a new dynamic clustering approach (DCPSO), based on particle swarm optimization for image segmentation. The proposed approach automatically determines the 'optimum' number of clusters and simultaneously clusters the data set with 123 minimal user interference. The algorithm starts by partitioning the data set into a relatively large number of clusters to reduce the effects of initial conditions. Using binary particle swarm optimization the "best" number of clusters is selected. The centers of the chosen clusters are then refined via the K-means clustering algorithm. The experiments show that the proposed approach generally found the optimum number of clusters on the tested images.  Cui and Potok (2006)  proposed a hybrid PSO based algorithm (PSO + K-means) to perform fast document clustering and to avoid being trapped in a local optimal solution as well. In this algorithm, the initial process starts by using the particle swarm optimization algorithm and then the result of the PSO is used as the initial seed of the K-means algorithm; the K-means algorithm can quickly locate the optima with a low distance average value. They reported that this hybrid PSO + K-means algorithm generated higher compact clustering than using either PSO or K-means alone.  Feng et al. (2006)  proposed an adaptive hyper-fuzzy partition particle swarm optimization clustering algorithm to optimally classify different geometrical structure data sets into correct groups. In this architecture, they used a hyper-fuzzy partition metric to improve the traditional commonly-used Euclidean norm metric clustering method. Since one fuzzy rule describes one pattern feature and implies the detection of one cluster center, it is encouraged that researchers decrease the number of fuzzy rules with the hyper-fuzzy partition metric. According to the adaptive particle swarm optimization, it was very suitable that they manage the clustering task for a complex, irregular, and high dimensional data set. They conclude that the proposed hyper-fuzzy PSO clustering algorithms are adaptive to outliers and achieve more accuracy than traditional K-means and fuzzy c-means clustering methods.  Hongwen and Rui (2006)  presented a neural network clustering methods-based particle swarm optimization for the large spatial datasets suitable for parallel processing. The outcome of the study clearly indicates that the proposed composite model and approach of this neural network clustering based PSO can be used as an attractive means for load characteristics to obtain the global load model for describing the nonlinear characteristics of the electric load. It will automatically produce the If-Then rules, whose premise parameters and consequent parameters are adjusted by using a Back-Propagation algorithm in combination with the least squares method. The simulation results also show the feasibility of the method.  Paterlini and Krink (2006)  introduced two algorithms for numerical optimization, namely particle swarm optimization (PSO) and differential evolution (DE). DE is clearly and consistently superior compared with the genetic algorithm (GA) for hard clustering problems.  Abraham et al. (2007)  introduced a method for clustering complex and linearly nonseparable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the particle swarm optimization (PSO) algorithm.  Ahmadi et al (2007)  reported that, when the data set is large and has a high dimension, the time single swarm is incapable of exploring the entire solution space. To solve such kinds of problems they proposed a multiple swarm based clustering algorithm based on the PSO. The proposed algorithm considers multiple cooperating swarms to find centers of clusters. In this algorithm a portion of the solution space is assigned to each swarm to search in that particular portion for increasing the chances of a good solution. The results of this algorithm show that it outperforms k-means clustering as well as conventional PSO-based clustering techniques.  Kao et al. (2007a)  applied the particle swarm optimization (PSO) algorithm to data clustering problems. Two reflex schemes are implemented on the PSO algorithm to improve the efficiency.  Kao et al. (2007b)  stated that PSO gives better clustering results, when it is applied in one dimension and for a small data set, but when it is applied to a large data set, it does not give good results. The reason behind this is that, when the search for a good solution reaches the search space boundary, the particles tend to stay there and do not move in other directions for a good solution. To overcome this problem a hybrid technique based on combining the K-means algorithm, Nelder-Mead simplex search  (Nelder and Mead 1965; Fan et al. 2004) , and particle swarm optimization, called K-NM-PSO, was proposed. The K-NM-PSO searches for cluster centers of an arbitrary data set as does the K-means algorithm, but it can effectively and efficiently find the global optima. This algorithm is both robust and suitable for handling data clustering and gives better results than other algorithms such as PSO, NM-PSO, K-PSO and K-means.  Marinakis et al. (2007)  presented a new stochastic nature-inspired methodology, which is based on the concept of particle swarm optimization and a greedy randomized adaptive search procedure (GRASP), for optimally clustering N objects into K clusters. They described a PSO algorithm for the solution of the feature selection problem and a GRASP for the solution of the clustering problem.  Junyan and Huiying (2007)  proposed the algorithm for mining web usage patterns based on PSO named RVPSO-K. Since web databases have high dimensions, a traditional clustering algorithm (K-means, FCM etc.) is not suitable for web data mining.  Das et al. (2008)  introduced a scheme for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring groups in the data. The proposed method was based on a modified version of the classical PSO algorithm, known as the Multi-Elitist PSO (MEPSO) model. It also employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. The use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. The performance of the proposed method was compared with six other clustering algorithms such as, GCUK-Genetic Clustering with an Unknown number of clusters K  (Maulik and Bandyopadhyay 2002) , the DCPSO-Dynamic Clustering PSO  (Omran et al. 2006 ), an ordinary PSO based kernel-clustering method, and the kernel K-means and a kernelized version of the subtractive clustering  (Kim et al. 2005) . They concluded that the algorithm gives similar and in some cases better results than the other compared algorithms using a test-bed of five datasets. They also concluded that Euclidean distance is suitable when data is hyper spherical and linearly separable. For a nonlinear and complex data set, it does not produce good results and they observed that the final classification accuracy and the mean number of classes found by the MEPSO deteriorate considerably when the feature space dimensionality exceeds 40.  Cui et al. (2008)  stated that the performance of HPSO using PSO and KM  (Cui et al. 2005 ) could be improved for document clustering by transforming the high dimensional vector document matrix into a lower dimension matrix. They report significant improvements in total clustering runtime for two dimensionality reduction techniques Latent Semantic Indexing-LSI  (Deerwester et al. 1990 ) and Random Projection-RP  (Kaski 1998) . Moreover, the clustering accuracy of the LSI-HPSO and RP-HPSO clustering algorithms is comparable to the one that uses the full term space.  Marinakis et al. (2008)  proposed a new hybrid algorithm, which is based on the concepts of PSO and GRASP, for optimally clustering N objects into K clusters. The proposed algorithm is a two-phase algorithm, which combines a multi-swarm constriction PSO algorithm for the solution of the feature selection problem and a GRASP algorithm for the solution of the clustering problem.  Díaz et al. (2008)  applied a derivative of the PSO algorithm to partitional clustering of a real-world data set obtained from a Water Supply Company. This PSO with its self-adaptive feature was compared to partitional clustering algorithms such as PAM and CLARA  (Kaufman and Rousseeauw 1990)  and results showed that the PSO efficiency was superior.  Zhang et al. (2008b)  proposed an improved ant colony optimization (IACO) and hybrid particle swarm optimization (HPSO) method for spatial clustering with obstacle constraints (SCOC). They applied IACO to obtain the shortest obstructed distance, which is an effective method for arbitrary shape obstacles, and 123 then they developed a novel HPKSCOC based on HPSO and K-medoids to cluster spatial data with obstacles.  Kao and Lee (2009)  presented a dynamic data clustering algorithm based on K-means and combinatorial particle swarm optimization, called KCPSO. Unlike the traditional K-means method, KCPSO does not need a specific number of clusters given before performing the clustering process and is able to find the optimal number of clusters during the clustering process.  Herrera et al. (2009)  applied a variant of Particle Swarm Optimization (PSO), to distinguish between perioperative practices and associate them with some unknown relevant facts. The PSO algorithm was compared to Partitioning Around Medoids (PAM)  (Kaufman and Rousseeauw 1990)  and in terms of efficiency; PSO efficiency was superior.  Marinakis et al. (2009)  proposed a new hybrid algorithm for clustering, which is based on the concepts of the bumble bees mating optimization (BBMO) and greedy randomized adaptive search procedure (GRASP). The proposed algorithm is a two-phase algorithm, which combines the BBMO algorithm for the solution of the feature selection problem and a GRASP algorithm for the solution of the clustering problem.  Lu et al. (2009)  presented a subspace clustering method to cluster documents, called Text Clustering via Particle Swarm Optimizer (TCPSO), which extends the particle swarm optimizer for variable weighting (PSOVW),  Luo and Wang (2009) , to handle the problem of clustering documents. Compared with the clustering quality of the six algorithms, TCPSO shows the ability to greatly improve the quality of text clustering.  Chen et al. (2009)  proposed a text clustering method algorithm based on K-means clustering and the PSO algorithm called OK-PSO. The K-means, DSOM-FS-FCM  (Hu et al. 2008)  and OK-PSO clustering approaches are applied on text datasets to compare the performance. Experimental results illustrate the efficiency of the proposed OK-PSO method.  Luo and Wang (2009)  introduced a data streams clustering algorithm based on Grid  (O'Callaghan et al. 2002; Li et al. 2009 ) and the PSO optimized clustering in order to get a more precise clustering efficiency. Experiments show that this algorithm is more efficient than the CluStream algorithm  (Aggarwal et al. 2003) .  Binwahlan et al. 2009  introduced a novel text summarization model based on swarm intelligence (PSO). The main purpose of the proposed model is for scoring sentences, while emphasizing how to deal with the text features fairly based on their importance. Results show that the proposed model creates better summary results than produced by the MS Word summarizer.  Sharma and Omlin (2009)  proposed the use of an adaptive heuristic PSO algorithm for finding cluster boundaries directly from the code vectors obtained from a self-organizing map (SOM).  Kiranyaz et al. (2009 Kiranyaz et al. ( , 2010a)) ) proposed two techniques. The first proposed technique, called Fractional Global Best Formation (FGBF), is designed to alleviate the premature convergence. The second technique called Multi-Dimensional Particle Swarm Optimization (MDPSO), is designed to achieve a dynamic clustering where the optimum number of clusters is also determined within the process. These techniques were used to find the optimal number of clusters in multidimensional data and to remove the common drawbacks of PSO family methods.  Li and Qian (2010)  proposed the hybrid P-HPSO clustering algorithm and used the Principle Component Analysis (PCA) to lower the data dimensionality with hybrid PSO and K-means algorithms (van der Merwe and Engelbrecht 2003) to cluster data. The results indicated that it could process the clustering analysis efficiently for the high-dimensional data.  Zhang et al. (2010)  adapted the Quantum-behaved Particle Swarm Optimization (QPSO)  (Sun et al. 2004a, b)  to improve the project indicating function to the PP (Projection Pursuit) model  (Friedman and Tukey 1974; Qiang and Xiaoyong 2006) , which is a multiple data processing method to find the best projection to project high dimension data to a lower dimension. They used the K-means clustering method to cluster the projected lower dimension data. The improved PP clustering model was proposed and shows the effectiveness and feasibility on different data types.  Peng et al. (2010)  present the text clustering algorithm that uses cluster intelligent optimization algorithms (PSO) to find the initial focal point to resolve the input sequences of the fuzzy clustering algorithm (FCM). Clustering results of experiments proved relatively stable; the text in each test set showed fairly good results.  Zhang and Jiang (2010)  proposed a text clustering method, called Text Clustering via Particle Swarm Optimizer (TCPSO) to solve the Chinese text clustering problem  (Lu et al. 2009 ). The simulation results on a text dataset shows that this approach effectively improves the quality of clustering and gets better results compared with the K-means algorithm, but is inferior to that on English text clustering because of the features of the Chinese words and the complex grammars.  Cai et al. (2010)  presented schemes for outlier detection in conjunction with clustering using PSO. They concluded that the combined modifications to PSO can speed up the optimization process to the point that it can quickly generate very efficient clustering solutions in large datasets. The running results of the system show that it was feasible and valuable to apply this method to mining the outliers in high-dimensional spectrum data.  Kiranyaz et al. (2010b)  presented a personalized long-term continuous electrocardiogram (ECG) classification system. It is based on multi-dimensional particle swarm optimization (MDPSO) to find out the true number of clusters and fractional global best formation (FGBF) to avoid local optima. To help professionals to quickly and accurately diagnose any latent heart disease, the proposed systematic approach produced results that were consistent with the manual labels with 99.5% average accuracy.  Marini and Walczak (2011)  proposed a discrete binary-coded implementation of the PSO algorithm to identify and select variables with a significant clustering tendency. In particular, to cope with the problems that can arise when performing variable selection on data sets with a high number of variables, and to account for the possibility of using the method also for purely exploratory purposes, an unsupervised approach was used, where the fitness was computed based on hierarchical clustering models. The characteristics of the method were shown on two simulated data sets and on a real megavariate data set coming from the analysis of genomic microarrays. They conclude that in all cases, PSO allowed them to explore different subspaces and to discover meaningful structures in the analyzed data. Based on these considerations, it is apparent that the use of PSO can represent a very promising option to deal with the problem of finding groups in a multivariate data set in a relatively straightforward and reliable way. This approach could be very useful to cope with biomarker identification in the fields of genomics, proteomics or metabolomics. Lotfi  Shahreza et al. (2011)  introduced an unsupervised algorithm based on a combination of a Self-Organizing Map (SOM)  (Xiang et al. 2003)  and Particle Swarm Optimization (PSO) for detection of anomalies like those in forest fires. In comparison with some other methods, this hybrid algorithm obtained good results, especially when abnormal cases are rare.  Lu et al. (2011)  presented a particle swarm optimizer to solve the variable weighting problem in projected clustering of high-dimensional data, called PSOVW. The PSOVW employs a K-means weighting function, which calculates the sum of the within-cluster distances for each cluster along relevant dimensions preferentially to irrelevant ones. It also proposed a normalized representation of variable weights in the objective function, which greatly facilities the search process. This algorithm makes use of particle swarm optimization to get near-optimal variable weights for the given objective function. They conclude that the experimental results on synthetic and real-life datasets, including an application to document clustering, show that it can greatly improve clustering accuracy for high-dimensional data and is much less sensitive to the initial cluster centroids. Moreover, the application to text clustering shows that the effectiveness of PSOVW is not confined to Euclidean distance. The reason is that the weights are only affected by the values of the objective function and weight updating in PSOVW is independent of the similarity measure. Other similarity measures can thus be used for different applications: the extended Jaccard  (Pang-ning et al. 2006)  coefficient for text data, for instance.  Sun et al. (2011)  presented a comparative analysis of particle swarm optimization (PSO), self-organizing hierarchical particle swarm optimizer (HPSO) and self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients (HPSO-TVAC) for data clustering. Through experiments on six well-known benchmarks, they found that the HPSO and the HPSO-TVAC algorithms had better performance than the PSO algorithm in most cases, and that all the clustering algorithms using PSO have good performance for large-scale data and high-dimensional data, especially the two algorithms proposed in this paper. Furthermore, they also observed that the convergence of the HPSO and the HPSO-TVAC algorithms is better when using a suitable fitness function  (Esmin et al. 2008) .  Kiranyaz et al. (2011)  presents the stochastic approximation-driven (SAD) MD-PSO. This used the simultaneous perturbation stochastic approximation (SPSA)  (Spall 1992 ) to drive the multi-dimensional extension of PSO, the MD-PSO  (Kiranyaz et al. 2009) . Experiments with and without SAD approve the superiority of SAD MD-PSO in terms of global convergence.  Yang and Du (2011)  proposed a particle swarm optimization (PSO)-based dimensionality reduction approach to improve support vector machine (SVM)-based classification for high-resolution hyperspectral imagery  (Bruzzone and Carlin 2006) . In experiments, SVM classification accuracy using PSO-selected bands was much higher than using all the original bands or dimensionalityreduced data.  Zhang and Mahfouf (2011)  introduced a framework for data-driven fuzzy modeling, named HFM-DSMO. As a multi-objective optimization algorithm, it used MPSO  (Zhang and Mahfouf 2006) , a modification of the PSO algorithm. This proposed modeling approach was successfully applied within the context of high-dimensional industrial data, the manufacturing of heat-treated alloy steels.  Chuang et al. (2011)  stated that the PSO, applied to clusters in multi-dimensional space, has shown outstanding performance. However, the rate of convergence when searching for global optima is still not sufficient  (Kao et al. 2008) . For this reason, they combined chaotic map particle swarm optimization (CPSO) with an accelerated convergence rate strategy  (Chuanwen and Bompard 2005; Xiang et al. 2007 ). This technique allows the ACPSO algorithm to cluster arbitrary data better than previous algorithms. Results of the conducted experimental trials on a variety of data sets taken from several real-life situations demonstrate that ACPSO was superior to the K-means, PSO, NM-PSO, CPSO, K-PSO and K-NM-PSO algorithms  (Kao et al. 2008) .  Padma and Komorasamy (2012)  presented a comparative study between the Quantum Clustering method (QC)  (Yeang et al. 2001 ) and some clustering methods within the same framework. In the experiments they used the Singular Value Decomposition (SVD) method to reduce the dataset's dimensionality and they conclude that the results show the effectiveness of the QC. Table  1  summarizes the above research contributions and Table  2  summarizes their contributions in different problem domains and applications.

Discussion
In the last decade, nature-inspired techniques are emerging as viable tools and alternatives to more traditional clustering techniques. Among the many nature-inspired techniques, clustering with particle swarm optimization techniques has found success in solving clustering problems. PSO can be used to find centroids of a user specified number of clusters. It is suitable for clustering complex and linearly non-separable datasets. In order to improve the efficiency of PSO algorithms, researchers have proposed different variants of the PSO algorithm and have developed new ideas such as adaptive heuristics, different fitness functions, kernel-induced similarity measures, and evolution of swarm generations etc.   The PSO still requires much investigation to improve performance and other key features that would make such algorithms suitable techniques for clustering high-dimensional data. Some future works and research trends to address the clustering of high-dimensional data are as follows: developing of multiple swarm cluster algorithms; dynamic PSO clustering algorithms; multi-objective PSO clustering algorithms; developing new fitness and measure functions; integration with feature selection and other techniques for dimensionality reduction; developing new PSO variants algorithms for feature selection phase and clustering algorithm analysis phase; developing and the use of multi distance measures; developing and the use of new similarity measures; developing and use of a multi similarity measure with multiple swarm algorithms; sensitivity analysis of PSO parameters; new strategies to find the optimal number of the clusters without any prior knowledge; and the applications to different real-world problems.

Conclusion
This paper has presented a review of previous research in the area of Particle Swarm Optimization, PSO Variants and its application to Clustering High-Dimensional Data. Research shows that, PSO applied to clusters in multi-dimensional space has shown outstanding performance. However, the rate of convergence when searching for global optima is still not sufficient. Furthermore, the modified PSO and its hybridization with other algorithms such as K-means, KFC, GA, ACO, etc. and/or the combinations with other techniques such as dimensionality reduction and feature selection can be successfully applied to address the problem of clustering high-dimensional data and gives better results. The implementation of such algorithms for High-Dimensional Data Clustering results in better cluster formation, which finally leads to better predictions and analysis of data. Some of the reviewed algorithms based on PSO can be also applied for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of clusters. A survey of the literature available in this topic has been given to provide a comprehensive insight into these topics.