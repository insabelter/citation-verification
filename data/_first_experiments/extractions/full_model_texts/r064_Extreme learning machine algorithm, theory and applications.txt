Introduction
As early as in the 1940s, mathematician Pitts and psychologist McCulloch have put forward neurons mathematical model (MP model) from the mathematical logic view  (McCulloch and Pitts 1943)  which opened the prelude of artificial neural network research. Neural network with parallel and distributed information processing network structure has a strong nonlinear mapping ability and adaptive self-learning, robustness and fault tolerance characteristics. In dealing with small samples, nonlinear adaptive information performance issues ELM has many unique advantages. Its unique nonlinear adaptive information processing capacity can overcome the traditional artificial intelligence methods for intuitive, such as pattern recognition, speech recognition, unstructured information processing deficiencies. Coupled with its solid theoretical basis and simple network structure model make the neural network in the fields of pattern recognition, image processing, sensors, signal processing and automatic control have significant results  (Ding et al. 2011a (Ding et al. ,b, 2012;; Quteishat and Lim 2008; Zhang and Wang 2009; Ding and Jin 2013) . It is widely used in the fields of expert system  (Markowska-Kaczmar and Trelak 2005) , pattern recognition  (Mohamed 2011) , intelligent control  (Ding et al. 2011a,b) , combinatorial optimization  (Kahramanli and Allahverdi 2009)  and prediction  (Hagan et al. 2002) . At present, there are many common kinds of neural network model, such as BP network  (Ding et al. 2011a,b; Feng et al. 2009 ), RBF network  (Ding et al. 2011a,b) , Hopfield network, CMAC cerebellar model, ART adaptive resonance theory  (LeCun and Bengio 1995; Benqio 2009; Carpenter and Grossberg 2003; Li et al. 2013) , etc. The powerful computing capabilities of the neural network are achieved through the propagation of information between neurons. According to the direction of the neural network internal information transfer, the neural network can be divided into two categories: feedforward neural network and feedback type neural networks. Extreme learning machine (ELM) described in this paper is for single hidden layer feedforward neural network which is one of feedforward neural networks. Feedforward neural network model has been extensively used in many fields due to its ability to approximate complex nonlinear mappings directly from the input samples. Among them, for a single hidden layer feedforward neural network learning ability the majority of studies focused on the input samples, divided into two aspects of the compact set and finite set.  Hornik (1991)  proved that if the activation function is continuous, bounded and nonconstant, then continuous mappings can be approximated in measure by neural networks over compact input sets. On this basis  Leshno et al. (1991)  improved the results and proved that feedforward networks with a non-polynomial activation function can approximate continuous functions. Through further study in a finite training set containing N different instance,  Huang and Babri (Huang and Babri 1998; Huang 2003)  shows that a nonlinear activation function single hidden layer feedforward neural network (SLFN) with at most N hidden nodes and can exactly approximate N distinct instances. Single hidden layer feed forward neural network has very strong learning ability, can approximate a complex nonlinear function and can solve the problems which cannot be solved by the traditional parameter learning method. However, the lack of fast learning method, often cannot meet the actual demand. To solve this problem, based on the above research conclusions of many scholars a new algorithm which is called the ELM has been proposed by  Huang et al. (2006) . As a new proposed learning algorithm can be easily solve the traditional feedforward neural network, such as BP network local minimum problem. ELM can adaptively set the hidden layer node number and randomly assign for the input weights and hidden layer biases, then the output layer weights obtained by the least square method, the whole process of a complete without iteration and improves the neural network generalization ability and learning ability. Since the extreme machine learning algorithm proposed, it has been widely used and became research focus of data mining, machine learning, image processing and other areas. In this paper, we make a review of extreme machine learning latest research progress about the algorithms, theory and applications. This paper is organized as follows. Section 2 introduces the basic idea and algorithm model of ELM. Section 3 further summarizes some improved ELM algorithms and its model. The applications of ELM in related fields are presented in Sect. 4. Conclusions and prospects are given in Sect. 5.

Extreme learning machine


Algorithm thought of ELM
Extreme learning machine is a learning algorithm for the single hidden layer feedforward neural networks used in classification and regression. The ELM used for single hidden layer feedforward neural network training can adaptively set the hidden layer node number and randomly assign for the input weights and hidden layer biases, the output layer weights obtained by the least square method, the whole learning process completed through one mathematical change without iteration. The training speed compared with the traditional BP algorithm based on gradient descent has been significantly improved (usually 10 times or more)  (Deng et al. 2010a,b) . In practical applications, firstly training on ELM and then predict. The training data set is mainly combined with the specific issues. The data sets include actual results and its related factors. During training, the influence factors and the corresponding results will be put into ELM for training, through an iteration to complete learning process. Then, with the trained ELM to predict, only need to input and the training data set is similar to the influencing factors. ELM model can be obtained the prediction results according to the memory. Extreme learning machine is an easy to use and effective algorithm for single hidden layer feedforward neural network. The traditional neural network learning algorithm (e.g. BP algorithm) need to set up lots of artificial network training parameters, and can easily lead to local optimal solution. ELM algorithm only need to set the number of hidden layer nodes, in the algorithm implementation process does not need to adjust the network input weights and hidden biases, and generates a unique optimal solution, with advantages of fast learning speed and generalization performance.

Model of ELM
For N arbitrary distinct samples (x i , t i ), where x i = [x i1 , x i2 , . . . , x in ] T ∈ R n and t i = [t i1 , t i2 , . . . , t im ] T ∈ R m , what's more (x i , t i ) ∈ R n × R m (i = 1, 2, . . . , N ), standard SLFNs with Ñ hidden nodes and activation function f (x) are mathematically models as Ñ i=1 β i f i (x j ) = Ñ i=1 β i f (a i • x j + b i ) = t j , j = 1, . . . , N (1) where a i = [a i1 , a i2 , . . . , a in ] T is the weight vector connecting the i th hidden node and the input nodes, and b i is the threshold of the ith hidden node.β i = [β i1 , β i2 , . . . , β im ] T is the weight vector connecting the ith hidden node and the output nodes.a i • x j represents the inner product of a i and x i , and the activation function usually choose "Sigmoid", "Sine", "RBF". The above Eq. (  1 ) can be written compactly as Hβ = T (2) where H (a 1 , . . . , a Ñ , b 1 , . . . , b Ñ , x 1 , . . . x N ) 123 = ⎡ ⎢ ⎣ f (a 1 • x 1 + b 1 ) • • • f (a Ñ • x 1 + b Ñ ) . . . • • • . . . f (a 1 • x N + b 1 ) • • • f (a Ñ • x N + b Ñ ) ⎤ ⎥ ⎦ N × Ñ , β = ⎡ ⎢ ⎣ β T 1 . . . β T Ñ ⎤ ⎥ ⎦ Ñ ×m , T = ⎡ ⎢ ⎣ t T 1 . . . t T N ⎤ ⎥ ⎦ N ×m H is called the hidden layer output matrix of the neural network; the ith column of H is the ith hidden node output with respect to inputs x 1 , x 2 , . . . , x N . Theorem proving  (Huang 2003) , as long as the number of hidden nodes is enough, when the activation function f (x) is infinitely differentiable at any interval the parameters of the network does not all need to adjust. When the training starts, SLFN randomly assigns to the input connection weights a and hidden layer node biases b, moreover, while the training process unchanged it can approximate any continuous function. Generally, in order to get good generalization performance, take Ñ N . When the input weights and hidden layer biases are determined in accordance with the random assignment, according to the input samples can get the hidden layer output matrix H . Therefore, training SFLN is converted into solving linear equations Hβ = T least squares solution. H (a 1 , . . . , a Ñ , b 1 , . . . , b Ñ ) β -T = min β H (a 1 , . . . , a Ñ , b 1 , . . . , b Ñ )β -T (3) The above Eq. (  3 ) least squares solution of the above liner system is β = H † T (4) In the Eq. (  4 ), H † represents Moore-Penrose  (Liang and Huang 2006 ) generalized inverse of the hidden layer output matrix H .Usually, the optimal solution β contains the following features: (1) According to β, the algorithm can gain the minimal training error; (2) Can get the optimal generalization capability of the minimum paradigm of the output connection weights and network; (3) β is unique. This can avoid producing the local optimal solution. In summary, given a training set (x i , t i ) ∈ R n × R m (i = 1, 2, . . . , N ),activation function f (x) and hidden node number Ñ , then the ELM algorithm can be can be summarized as follows 3 steps: Step 1: Definition of hidden layer node number Ñ , randomly assign input weights a i and hidden layer biases b i , (i = 1, 2, . . . , Ñ ). Step 2: Calculate the hidden layer output matrix H . Step 3: According to the Eq. (  4 ), calculate the output weight β.

Performance of ELM
To further demonstrate the performance of ELM, through experiments on ELM, support vector machines (SVMs) and BP neural network to compare the performance. ELM and BP execution environment for Matlab7.11 and the SVM execution environment for VC + + 6.0. ELM's source code can be downloaded directly from the profile of Huang. BP algorithm has been integrated in the neural network toolbox in Matlab and can directly execute experiment. SVM algorithm using the C language implementation of the SVM package: LibSVM. ELM and BP activation function to select a sigmoid function: f (x) = 1/(1 + exp(-x)) and the In the experiment we use several classification data sets in the UCI database and then compare experimental results. Datasets used in the experiment include: Segmen dataset, Satimage dataset and Diabetes dataset. The fundamental characteristics of the three datasets are as shown in Table  1 . For all samples in the experiment randomly divided into two groups for training and testing data sets. ELM and BP network hidden layer nodes and the number of SVM support vectors are the average of several experimental results. As shown in Table  2  are ELM, BP and SVM algorithm performance comparison. It can be seen that ELM is a very simple and fast neural network learning algorithm just need one iteration process. Compared with the BP algorithm, ELM can randomly initialize input connection weights and hidden layer neurons threshold by just tuning the number of hidden layer nodes Ñ .In practical applications, Ñ can be identified through constantly trying. Moreover, in a large number of experiments on standard UCI data sets show that, ELM has faster training speed and better generalization performance  (Han and Huang 2006)  than the BP algorithm and SVM method. To further validate the performance of the ELM, the article refers to the experimental results from literature  (Huang et al. 2006 ) to analysis. In the literature, Huang et al, do the experiment with small and medium real classification datasets and then compare ELM, BP and SVM classification results. The basic characteristics of the data set in Table  3 . Performance comparison of ELM, BP and SVM as in Table  4 . As well as the widely used neural network, ELM based on single hidden layer feedforward neural network due to its good learning capability and generalization performance, make it a 123 (1) ELM algorithm is based on the empirical risk minimization, without considering the structural risk and this may lead to over-fitting problems. (2) ELM directly calculates the least squares solution. The users cannot fine-tune according to the characteristics of the data sets. It is also poorly controllability. (3) When there are outliers in the data sets, the performance of model will be greatly affected, poor robustness.

Improvements on extreme learning machine
Since ELM was proposed by Huang et al.  (Lan et al. 2010 ) at 2004, many researchers began to study on ELM and put forward lots of improved algorithms. To make ELM in improving the speed of network training, at the same time also to avoid the based on gradient descent learning method for many problems, for example, local minimal, too much iterative times, termination conditions as well as the definition of learning rate parameter  (Huang et al. 2004 ). However, because the ELM is based on the empirical risk minimization principle, randomly select the input weights and hidden layer biases which may lead to non-optimal or unnecessary input weights and hidden layer biases. Compared with the based on gradient descent learning algorithm, ELM may need more hidden layer neurons, which reduces the computation rate and training effect of ELM. Therefore, in order to speed up the response rate of the training network, people put forward a lot of ELM improved algorithm.

Online extreme learning machine
Extreme learning machine algorithm usually used in solving multiple logistic regression and classification problems. In the applications, it reflects good generalization performance and fast learning speed. But in concrete applications it will also encounter some problems, such as small sample data set, prone to over-fitting phenomenon. After the ELM proposed  (Huang 2005 ) put forward sequential modification based on recursive least-squares (RLS) algorithm, which referred as online sequential extreme learning machine (OS-ELM). Based on OS-ELM, online sequential fuzzy extreme learning machine (Fuzzy-ELM) is also introduced to implement zero order TSK model and first order TSK model. The performance of OS-ELM and Fuzzy-ELM are evaluated and compared with other popular sequential learning algorithms, and compared with other sequential learning algorithm shows that the proposed OS-ELM produces better generalization performance at very fast learning speed.  Rong et al. (2009)  combined with previous work and find an online sequential fuzzy extreme learning machine (OS-Fuzzy-ELM) developed for function approximation and classification problems. The equivalence of a Takagi-Sugeno-Kang (TSK) fuzzy inference system (FIS) to a generalized single hidden layer feedforward network is shown first, which is then used to develop the OS-Fuzzy-ELM algorithm. This results in a FIS that can handle any bounded non-constant piecewise continuous membership function. Furthermore, the learning in OS-Fuzzy-ELM can be done with the input data coming in a one-by-one mode or a chunk-by-chunk mode with fixed or varying chunk size. In OS-Fuzzy-ELM, all the antecedent parameters of membership functions are randomly assigned first, and then, the corresponding consequent parameters are determined analytically. Performance comparisons of OS-Fuzzy-ELM with other existing algorithms are presented using realworld benchmark problems in the areas of nonlinear system identification, regression, and classification. To solve the problem of extreme learning machine (ELM) online training with sequential training samples, a new algorithm called selective forgetting extreme learning machine (SF-ELM) is proposed by  Zhang (2011)  and applied to chaotic time series prediction. The SF-ELM adopts the latest training sample and weights the old training samples iteratively to insure that the influence of the old training samples is weakened. The output weight of the SF-ELM is determined recursively during on-line training procedure according to its generalization performance. In comparison with on-line sequential extreme learning machine, the SF-ELM has better performance in the sense of computational cost and prediction accuracy.

Pruned extreme learning machine
Extreme learning machine represents one of the recent successful approaches in machine learning, particularly for performing pattern classification. One prominent advantage of ELM is its short training time. The number of hidden layer nodes can be randomly selected and analyzed to determine. This reduces the calculation time while learning speed fast. However, ELM also exist some shortcomings such as over-fitting problems.  Rong et al. (2008)  address the architectural design of the ELM classifier network, since too few/many hidden nodes employed would lead to under-fitting/over-fitting issues in pattern classification. A pruned-ELM (P-ELM) algorithm was proposed as a systematic and automated approach for designing ELM classifier network. P-ELM uses statistical methods to measure the relevance of hidden nodes. Beginning from an initial large number of hidden nodes, irrelevant nodes are then pruned by considering their relevance to the class labels. As a result, the architectural design of ELM network classifier can be automated. Empirical study of P-ELM on several commonly used classification benchmark problems and with diverse forms of hidden node functions show that the proposed approach leads to compact network classifiers that generate fast response and robust prediction accuracy on unseen data, comparing with traditional ELM and other popular machine learning approaches. An algorithm for pruning ELM networks by using regularized regression methods was put forward by  Martinez-Martinez et al. (2011) , thus obtaining a suitable number of the hidden nodes in the network architecture. Beginning from an initial large number of hidden nodes, irrelevant nodes are then pruned using ridge regression, elastic net and lasso methods; hence, the architectural design of ELM network can be automated. Empirical studies on several commonly used regression benchmark problems show that the proposed approach leads to compact networks that generate competitive results compared with the ELM algorithm.

Some other improved ELM algorithms
ELM may need higher number of hidden neurons due to the random determination of the input weights and hidden biases, a hybrid learning algorithm was proposed by  Zhu et al. (2005)  which uses the differential evolutionary algorithm to select the input weights and Moore-Penrose (MP) generalized inverse to analytically determine the output weights. This approach is able to achieve good generalization performance with much more compact networks. Consider about the improved ELM algorithm, many researchers combine ELM and other data processing methods to propose a new learning model which is applied to the related fields, and achieved very good results. In order to solve the ELM non-optimal performance and the over fitting issue, a hybrid approach is proposed based on group search optimizer strategy to select input weights and hidden biases for ELM algorithm by  Silva (2011) . In this method, need to evaluate the influence of different forms of handling members that fly out of the search space bounds. And in the benchmark data sets for processing can be obtained than traditional ELM better generalization performance. Because of the random choice of input weights and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. An effectiveness improved algorithm of ELM was proposed by  Wang and Cao (2011)  called EELM that makes a proper selection of the input weights and biases before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM.  Fernandez-Navarro et al. (2011)  propose a methodology for training a new model of artificial neural network called the generalized radial basis function (GRBF) neural network. This model is based on generalized Gaussian distribution, which parameterizes the Gaussian distribution by adding a new parameter τ . For example, when GRBF takes a value of τ = 2, it represents the standard Gaussian radial basis function. The model parameters are optimized through a modified version of the extreme learning machine (ELM) algorithm. The centers of each GRBF were taken randomly from the patterns of the training set and the radius and τ values were determined analytically, taking into account that the model must fulfill two constraints: locality and coverage. A thorough experimental study is presented to test its overall performance.

Applications of ELM
Neural network is widely used in artificial intelligence, data mining and other applications. Single hidden layer feedforward neural network has a strong learning ability. Extreme learning methods proposed to improve the disadvantage of a single hidden layer feedforward neural network, improve the learning ability and generalization performance. Due to ELM algorithm does not require large-scale operations, there is no need to adjust the hidden layer of the network and compared with the traditional computational intelligence techniques, such as BP algorithm the whole learning process needs only one iteration and the training speed improved more than 10 times. The algorithm is different from the traditional neural network methods. It can randomly select the hidden layer neurons number and weights, and analysis to determine the output weights of SLFN. The training speed is fast and with good generalization ability, which makes SLFN training time greatly reduced. Extreme learning machine is used in the fields such as signal processing  (Mao 2002) , automatic control  (Mao and Huang 2005) , image processing  (Huang et al. 2011) , market analysis, aviation and aerospace and medical diagnosis  (Shang et al. 2005) , etc. A new human face recognition algorithm based on bidirectional two dimensional principal component analysis (B2DPCA) and ELM is proposed by  Mohammed et al. (2011) . The method is based on curvelet image decomposition of human faces and a subband that exhibits a maximum standard deviation is dimensionally reduced using an improved dimensionality reduction technique. Discriminative feature sets are generated using B2DPCA to ascertain classification accuracy. Other notable contributions of the proposed work include significant improvements in classification rate, up to hundred folds reduction in training time and minimal dependence on the number of prototypes. Many researchers using the ELM to solve the compete phenomena in the whole nature and society, such as the winner-take-all (WTA) competition which is widely observed in both inanimate and biological media and society. Then,  Li et al. (2013)  presented a simple model which produces the WTA competition by taking advantage of selective positive-negative feedback through the interaction of neurons via p-norm. In order to solve the hidden layer neuron determination problem of regularized extreme learning machine applied to chaotic time series prediction, a new algorithm based on Cholesky factorization is proposed by  Zhang (2011) . First, an RELM-based prediction model with one hidden-layer neuron is constructed and then a new hidden-layer neuron is added to the prediction model in each training step until the generalization performance of the prediction model reaches its peak value. Thus, the optimal network structure of the prediction model is determined. In the training procedure, Cholesky factorization is used to calculate the output weights of RELM. The algorithm can be effectively used to determine the optimal network structure of RELM, and the prediction model trained by the algorithm has excellent performance in prediction accuracy and computational cost. A sign-bi-power activation function is proposed by  Li et al. (2012)  to accelerate Zhang neural network to finite-time convergence. The problem of this neural network is with the suggested activation functions never converge to the desired value in finite time, which may limit its applications in real-time processing. In addition, the proposed algorithm is applied to online calculating the pseudo-inverse of a matrix and nonlinear control of an inverted pendulum system. Both theoretical analysis and numerical simulations validate the effectiveness of proposed activation function. Recently, error minimized extreme learning machines (EM-ELMs) have been proposed by  Romero and Alquezar (2012)  as a simple and efficient approach to build SLFNs sequentially. They add random hidden nodes one by one and update the output weights incrementally to minimize the sum-of-squares error in the training set. Other very similar methods that also construct SLFNs sequentially had been reported earlier with the main difference that their hidden layer weights are a subset of the data instead of being random. These approaches are referred to as support vector sequential feed-forward neural networks (SV-SFNNs), and they are a particular case of the sequential approximation with optimal coefficients and interacting frequencies (SAOCIF) method. In this paper, it is shown that EM-ELMs can also be cast as a particular case of SAOCIF. In particular, EM-ELMs can easily be extended to test some number of random candidates at each step and select the best of them, as SAOCIF does. Moreover, it is demonstrated that the cost of the computation of the optimal output-layer weights in the originally proposed EM-ELMs can be improved if it is replaced by the one included in SAOCIF. In addition, there are still a lot of applications about ELM in other areas. An improved ELM method was proposed for soft sensing modeling by  Chang et al. (2007) . By adding a kind of sorting neurons into the hidden layer of the single hidden layer feedforward neural network (SLFN), a new SLFN structure was proposed. The method provided a new approach to build soft sensing models, and it was successfully applied to the soft sensing modeling the of cell concentration for the Nosiheptied fermentation process to realize the online prediction of the cell concentration.  Pan et al. (2010)  use the ELM to predicting reservoir permeability, through the contrast analysis with SVM, found in the reservoir permeability prediction of possibility and advantage. Yao and Han (2010) use the ELM for remote sensing image fusion. It gets the training sample regression relationship between the remote sensing image data quickly and efficiently while improving the real quality of a thermal infrared image data and provide convenience for the practical application of remote sensing.  Cai et al. (2010)  use the ELM method for lithology identification, and the establishment of the ELM lithology classification model, the results indicate that the application of ELM in the field the feasibility and effectiveness of the algorithm.  Li et al. (2012)  studies the decentralized kinematic control of multiple redundant manipulators for the cooperative task execution problem. They formulated the problem as a constrained quadratic programming problem and proposed a recurrent neural network with independent modules to solve the problem in a distributed manner. Each module in the neural network controls a single manipulator in real time without explicit communication with others and all the modules together collectively solve the common task. At present, the ELM is applied in more and more fields and plays a significant role. Especially in high dimensional complex data mining has prominent advantage. In the future, how to make better combinations between other learning methods and ELM and applications in the broader field are the issues of concern.

Conclusions and prospects
Extreme learning machine algorithm was proposed to solve the traditional single hidden layer feed forward neural network easily to appear the local minimum and over-fitting problems, and the network structure has also been greatly improved. However, the ELM algorithm still exists some shortcomings, this need further development and perfection. In view of the above shortages, ELM needs further improvement. Further research includes: (1) Do further refine and improve of the existing ELM algorithm. The ELM algorithm efficiency of learning and generalization performance has been recognized, but the number of neurons in hidden layer is numerous, the application is not very convenient also will affect the accuracy of the results. Therefore, further enhancing ELM model structure and the generalization performance of the algorithm is necessary. (2) The integration of other disciplines and other technology with ELM. In recent years, many researchers in view of ELM problems try to combine other kinds of disciplines algorithm with the ELM, presented better training model. In the future study, how to make the online learning, genetic algorithm, SVM and ELM together will be a very worthwhile to explore direction. (3) Expand ELM applications. Although ELM has obvious advantages in theory, but the actual application field is limited at present. Therefore, how to apply ELM to the daily life effectively is an important aspect in future research. How to use the ELM better use to address and resolve any regression and multi-classification problem is worth to explore and discover.