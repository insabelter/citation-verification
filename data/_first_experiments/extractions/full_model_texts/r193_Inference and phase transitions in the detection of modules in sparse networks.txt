In many networks, ranging from online communities to food webs, metabolic networks, and genetic regulatory networks, there are communities or modules that play distinct functional roles. A fundamental problem is to detect these communities and understand what role they play in the network's structure and dynamics. In social networks, these communities are often assortative, meaning that there is a higher density of connections within communities than between them, and many approaches to detecting these communities have been proposed (see e.g. 
[1] ). In other networks, however, these modules may consist of nodes with few connections to each other, but which connect to the rest of the network in similar ways. In this Letter we analyze a random generative model for sparse modular networks, known as the stochastic block model. It provides a useful playground for theoretical ideas and the analysis of algorithms, and is a popular model for functional modules in real networks. Using the cavity method developed in the physics of disordered systems  [2, 3]  we exactly analyze the detectability of these modules in the limit of large sparse networks. As a function of the parameters, we compute the phase diagram and locate the associated phase transitions. We distinguish between a detectable phase where it is possible to learn the model's parameters and the group assignments of the nodes, and a non-intuitive undetectable phase where learning is impossible because the network's topology does not retain enough information about the original group memberships. The existence of a phase where a certain class of algorithms is unable to detect communities was previously predicted  [4] , but its location was only found approximately (and its size overestimated). In addition, unlike previous works based on finding a ground state, i.e., minimizing a cost function associated with a group assignment  [4, 5] , our analysis is more general as it relies on the properties of the entire Boltzmann distribution of group assignments. We also unveil a transition from an algorithmically "hard" phase, where, we believe, no polynomial algorithm for learning the groups and parameters exists, to an "easy" phase where polynomial algorithms do exist. In the latter phase, we show that Belief Propagation (BP)  [6]  works on large networks in essentially linear time as a function of their size. BP was previously proposed for community detection  [7]  without, however, the ability to learn the parameters of the underlying model. Our approach also provides a natural measure of the significance of the modules in the network, since it computes the marginal probability that a given node belongs to a given group. If the network does not contain any modules, our method correctly infers this fact by making these marginals uniform. This is an aspect missing in the vast majority of the present approaches to community detection. Our theoretical understanding and algorithm are also applicable to real world networks, as we discuss briefly at the end of this paper (and in detail elsewhere). Moreover our approach is not restricted by the details of the generative model, and is easily generalized to more elaborate models (e.g., those of  [8] ). Stochastic block models. We consider networks of N nodes. Each node i has a hidden label t i ∈ {1, . . . , q}, specifying which of q groups it is a member of. These labels are chosen independently, where n a is the probability that a given node has label a ∈ {1, . . . , q} (normalized so that q a=1 n a = 1). If N a is the number of nodes in each group, we have n a = lim N →∞ N a /N . Once the group assignment is chosen, the model generates a graph G as follows. For each pair of nodes i, j with i < j, we put an edge between i and j independently with probability p ti,tj , leaving them unconnected with probability 1 -p ti,tj . We call p ab the affinity matrix. Since we are interested in the sparse case where p ab = O(1/N ), we will use the rescaled affinity matrix c ab = N p ab and assume that c ab = O(1) in the limit N → ∞. In our setting, the adjacency matrix A ij of the graph is the only information available to us. Our goal is to learn the parameters q, {n a }, {p ab } of the block model, as well as the true group assignments {t i }. Special cases of this model have often been considered in the literature. Planted partitioning, when n a = 1/q, c ab = c out for a = b and c aa = c in with c in > c out , is a classical problem in computer science and has been used as a benchmark for community detection  [1, 4, 7, 9, 10] . Planted coloring, where n a = 1/q, c aa = 0, and c ab = cq/(q -1), is a fundamental problem in constraint optimization  [3] , and was studied using the cavity method in  [11] . Bayesian inference for block models. Bayesian inference has been applied to community detection before. However, except for some very specific generative models  [12] , the likelihood function must be computed approximately, either through Monte Carlo sampling (e.g.  [13] ) or variational methods  [10] . The crucial contribution of our work is that the quantities that follow from Bayesian inference can be computed exactly in the thermodynamic limit using the cavity method, or on real finite networks using the BP algorithm in time roughly linear in the size of the network. The probability that the model parameters take a given set of values {θ} = (q, {n a }, {c ab }), conditioned on the topology of the network G, is P ({θ} | G) = P ({θ}) P (G) {ti} P (G, {t i } | {θ}) . (1) The sum is over all possible group assignments {t i }, where t i ∈ {1, . . . , q} for each node i. The prior P ({θ}) includes all graph-independent information about the values of the parameters. We will assume there is no such information available and hence this prior is uniform. In that case, maximizing P ({θ} | G) over {θ} is equivalent to maximizing the sum {ti} P (G, {t i } | {θ}). The function P (G, {t i } | {θ}) is called the likelihood. It is the probability that the model would produce the group assignment {t i } and the network G, assuming that its parameters are {θ}. We can write the likelihood exactly for many different generative models; for the stochastic block model defined above, it is P (G, {t i } | {θ}) = i n ti i<j p Aij ti,tj (1 -p ti,tj ) 1-Aij . Thus P ({θ} | G) is proportional to the partition sum Z({θ}) of a generalized Potts model, with Hamiltonian H({t i } | {θ}) = - i log n ti - i<j A ij log c ti,tj + (1 -A ij ) log 1 - c ti,tj N . (2) There is a strong O(1) interaction between connected nodes, and a weak O(1/N ) one between unconnected nodes. The log n ti play the role of local fields, enforcing the prior distribution {n a } on group assignments. Inferring the parameters {θ} is equivalent to minimizing the free energy f ({θ}) = -log Z({θ})/N associated with  (2) . If f ({θ}) has a non-degenerate minimum, then, from the saddle point method, {θ} is with high probability exactly the set of parameters used in the generation of the network. In that case, inferring the parameters of the underlying model is possible. Assuming that we know, or have learned, the correct parameters {θ}, how should we determine the group assignment of the nodes? The most likely assignment {t i } is the ground state of the Hamiltonian (2). However, if we want to find an assignment {t i } that maximizes the number of correctly labeled nodes, we need to follow a different strategy. Namely, we should compute the marginal distribution ν i (t i ) = {tj } j =i µ({t j } j =i , t i ) of the label of each node i, where µ is the Boltzmann distribution of (2). The most probable group assignment for node i is then t * i = argmax ti ν i (t i ). It can be proven in general  [14]  that this marginalization maximizes the number of correctly labeled nodes in the thermodynamic limit, and that it is a better choice than using the ground state of (2). Furthermore, a configuration chosen according to the Boltzmann distribution has, asymptotically, the correct group sizes and the correct number of edges between each pair of groups, while for the ground state this is not true; finding the minimum bisection, for instance, creates the illusion of two groups even in a completely random graph  [15] . Moreover marginalization is algorithmically easier than searching for the ground state. The expected number of correctly labeled nodes can be estimated as i ν i (t * i ), even without knowing the original assignment. Belief Propagation. We could estimate the free energy using Monte Carlo (MC) sampling, and we do this for comparison. But a faster algorithm is Belief Propagation (BP), known in physics as the cavity method  [2, 3] . It is exact in the thermodynamic limit as long as the network is locally treelike, and as long as connected correlations decay rapidly as a function of topological distance. To derive the BP equations  [3, 6] , one introduces "messages" ψ i→j ti and ψ j→i tj for each pair of nodes (i, j). These are conditional marginals in the cavity method. For instance, ψ i→j ti is the probability that i would be in group t i if j were removed from the network. Assuming conditional independence between the neighbors of each node and neglecting lower order terms, the messages must be a fixed point of a consistency equation, ψ i→j ti = 1 Z i→j n ti e -ht i k∈∂i\j t k c t k ti ψ k→i t k (3) for each edge (i, j). Here ∂i is the set of i's neighbors, the field h ti = 1 N k t k c t k ti ψ k t k summarizes the influence of the non-edges, and Z i→j is a normalizing factor. FIG.  1 : Learning for q = 2 groups with na = 1/2, average degree c = 3, and ǫ = cout/cin = 0.15. If we initialize it in the ordered region, i.e., with ǫ0 < 0.37, our algorithm infers the correct value of ǫ. Inset: the free energy as a function of ǫ. Note the minimum at ǫ = 0.15, and the paramagnetic region for ǫ > 0.37. We start with random messages, and iterate (3) until we reach a fixed point. This typically takes just a few iterations, and each step takes linear, O(N ), time. The marginals corresponding to the BP fixed point are ν i (t i ) = (1/Z i ) n ti e -ht i j∈∂i tj c tj ti ψ j→i tj , and the free energy is . For more details, see  [3, 6] . Requiring that f BP ({θ}) is stationary we update the parameters to their most-likely values given the fixed point f BP ({θ}) = - 1 N i log Z i + 1 N (i,j)∈E log Z ij - c 2 c ′ ab = (i,j)∈E c ab (ψ i→j a ψ j→i b + ψ i→j b ψ j→i a )/(Z ij n a n b N ) , and n ′ a = i ν i (a)/N . Starting with a suitable initial value {θ 0 }, we compute {θ ′ } and iterate until convergence (see Fig.  1 ), as in the expectation-maximization algorithm  [16] . To learn the number of groups q, we run the algorithm with several values of q ′ . The free energy f BP decreases with q and then stays constant for q ′ ≥ q. Phase diagrams. For illustration we use the case of planted partitions and colorings, n a = 1/q, c ab = c out for a = b, and c aa = c in . We observe three different cases governing the free energy landscape f BP {θ}. In the "paramagnetic" phase, the free energy is constant in the vicinity of the true value of {θ}. Learning is impossible, and the marginals are ν i (t i ) = 1/q for all nodes. In this case the overlap between the original assignment and the one resulting from BP marginalization, defined as Q({t i }, {q i }) = max π∈Sq (1/N ) i δ ti,π(qi) -max a n a 1 -max a n a , (4) (where S q is the permutation group) is zero, and the original assignment is undetectable. Generalizing  [11, 17] , one can show there is essentially no difference between a graph produced by the block model and a completely random graph of the same average degree; the free energy of the two ensembles is asymptotically identical. In the ordered phase, f BP has an attractive global minimum at the true value of {θ}, and BP rapidly infers the correct parameters. This is illustrated in Fig.  2 . As ǫ = c out /c in varies from 0 (q completely separated groups) to 1 (a pure random graph), we observe a continuous phase transition from an ordered phase with positive overlap to a paramagnetic phase with zero overlap. Thus there is a second-order transition from a detectable to an undetectable phase. A third situation arises if f BP {θ} has both a paramagnetic fixed point and the ordered fixed point at the true {θ}. In this case, the two phases co-exist and the detectability transition is first-order; see Fig.  2  on the right. The phase transition is located by comparing the free energies of the two phases. However, even if the ordered fixed point has a lower free energy, it is not easy to find it unless the initial messages are close to the true group assignment. All but an exponentially small set of initial messages will lead to the paramagnetic fixed point. This situation is typical of mean-field first-order phase transitions. In fact, recent results about random optimization problems show that finding the lower-free-energy phase in this case is an extremely hard problem  [11, 18] . Only when the paramagnetic phase is no longer locally stable does inference become easy. We can compute the location of the transition to this easily-detectable phase analytically by analyzing how a small random perturbation to the paramagnetic fixed point propagates as the BP equations are iterated  [11, 19] . It follows that for |c in -c out | > q √ c , (5) the original group assignment is dynamically attractive and hence many algorithms, e.g. MC or BP, will converge to it. Note that it is typically still hard to compute the ground state of (  2 ), even though we can compute the marginals, and therefore the optimal estimate of the group assignment, asymptotically exactly. On the other hand, if  (5)  is not satisfied then community detection is either impossible, or at best as hard as solving the hardest known optimization problems. When c out < c in the phase transition is of first-order for q > 4, as can be retrieved from data presented in  [19] . However, the detectable but hard region is so narrow that is is quite unlikely to appear in realistic situations. Real-world networks. Our algorithm is not restricted to large random networks; it is applicable to real networks as well. We tested it on the "Karate Club" network  [20] , a common benchmark for community detection. For q = 2, BP leads to two different fixed points. One corresponds to the actual known division into two A continuous phase transition between a detectable and a non-detectable phase arises at the critical point given by  (5) . Middle: The 4-group community detection benchmark of  [9]  with c = 16, with the same phenomenology. The results agree well with MC simulations, except very close to the critical point where finite-size effects are stronger. Right: A planted coloring problem with q = 5 and cin = 0, c = cout(1 -1/q). Both the ordered fixed point (green +s, obtained by initializing in the actual group assignment) and the paramagnetic one (blue ×s, obtained by initializing the algorithm in a random configuration) exist between c d and cs. The difference ∆f (red) between the paramagnetic and ordered free energies shows that modules are in principle detectable as soon as c > cc when ∆f > 0. It is in practice impossible to find the corresponding fixed point, and detection become feasible only after the spinodal point cs given by (  5 ). groups. The other has a smaller free energy and thus a larger likelihood, and splits the network into high-degree nodes and low-degree nodes as found in  [8] . These two fixed points correspond to two local minima of f BP for q = 2, and depending on the initial value {θ 0 } BP converges to one or the other. For q > 2, our algorithm converges to fixed points with yet lower values of f BP . For q = 4 the best fixed point corresponds to a splitting of the two actual groups into high-degree and low-degree subgroups. The results obtained with MC, which can be easily equilibrated for such a small network, are almost identical to those of BP in terms of the parameters and marginals, and identical in terms of the estimated group assignments. This demonstrates that BP is a useful approach even on real, finite networks that are far from trees. Conclusion. We have presented a principled and asymptotically exact analysis of the detection of communities in networks generated by the stochastic block model. There is a strict limit on detectability due to a transition from a phase where the free energy landscape lets us infer the model's parameters, to a phase where it does not. In some cases the communities are detectable, but the problem is hard because the attractive region around the correct fixed point is exponentially small. Our analysis comes with an associated learning algorithm, which for large sparse networks generated from the model is able to learn the number of groups, their exact sizes, and the affinity matrix p ab . Our approach and our algorithm are easily generalized to other local generative models, and we will investigate its performance on a variety of real-world networks in the future.