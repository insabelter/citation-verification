<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Application of pattern recognition tools for classifying acute coronary syndrome: an integrated medical modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2013-09-18">18 September 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nader</forename><surname>Salari</surname></persName>
							<email>nader.salari.1344@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University Putra Malaysia</orgName>
								<address>
									<settlement>Serdang</settlement>
									<region>Selangor</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Biostatistics and Epidemiology</orgName>
								<orgName type="department" key="dep2">School of Public Health</orgName>
								<orgName type="institution">Kermanshah University of Medical Sciences</orgName>
								<address>
									<settlement>Kermanshah</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University Putra Malaysia</orgName>
								<address>
									<settlement>Serdang</settlement>
									<region>Selangor</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Biostatistics and Epidemiology</orgName>
								<orgName type="department" key="dep2">School of Public Health</orgName>
								<orgName type="institution">Kermanshah University of Medical Sciences</orgName>
								<address>
									<settlement>Kermanshah</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shamarina</forename><surname>Shohaimi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University Putra Malaysia</orgName>
								<address>
									<settlement>Serdang</settlement>
									<region>Selangor</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University Putra Malaysia</orgName>
								<address>
									<settlement>Serdang</settlement>
									<region>Selangor</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farid</forename><surname>Najafi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Biostatistics and Epidemiology</orgName>
								<orgName type="department" key="dep2">School of Public Health</orgName>
								<orgName type="institution">Kermanshah University of Medical Sciences</orgName>
								<address>
									<settlement>Kermanshah</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Biostatistics and Epidemiology</orgName>
								<orgName type="department" key="dep2">School of Public Health</orgName>
								<orgName type="institution">Kermanshah University of Medical Sciences</orgName>
								<address>
									<settlement>Kermanshah</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meenakshii</forename><surname>Nallappan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University Putra Malaysia</orgName>
								<address>
									<settlement>Serdang</settlement>
									<region>Selangor</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University Putra Malaysia</orgName>
								<address>
									<settlement>Serdang</settlement>
									<region>Selangor</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Isthrinayagy</forename><surname>Karishnarajah</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University Putra Malaysia</orgName>
								<address>
									<settlement>Serdang</settlement>
									<region>Selangor</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Application of pattern recognition tools for classifying acute coronary syndrome: an integrated medical modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-09-18">18 September 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">095F2611C66261782F8EC3D756BF74BF</idno>
					<note type="submission">Received: 18 December 2012 Accepted: 4 September 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-03-14T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Acute coronary syndrome</term>
					<term>Artificial intelligence</term>
					<term>Clinical decision support systems</term>
					<term>Classification</term>
					<term>Diagnosis None=1</term>
					<term>Other antiplatelet agent=2</term>
					<term>Clopidogrel=3</term>
					<term>Ticlopidine=4 Calcium channel blockers Absence=1</term>
					<term>Presence=2 Nominal Predominantly presenting symptom Asymptomatic=1</term>
					<term>Fatigue=2</term>
					<term>Chest pain=3</term>
					<term>Dyspnoea=4</term>
					<term>Other symptoms=5</term>
					<term>Syncope=6</term>
					<term>Cardiac arrest-Aborted sudden death= 7</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objective: The classification of Acute Coronary Syndrome (ACS), using artificial intelligence (AI), has recently drawn the attention of the medical researchers. Using this approach, patients with myocardial infarction can be differentiated from those with unstable angina. The present study aims to develop an integrated model, based on the feature selection and classification, for the automatic classification of ACS. Methods: A dataset containing medical records of 809 patients suspected to suffer from ACS was used. For each subject, 266 clinical factors were collected. At first, a feature selection was performed based on interviews with 20 cardiologists; thereby 40 seminal features for classifying ACS were selected. Next, a feature selection algorithm was also applied to detect a subset of the features with the best classification accuracy. As a result, the feature numbers considerably reduced to only seven. Lastly, based on the seven selected features, eight various common pattern recognition tools for classification of ACS were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The performance of the aforementioned classifiers was compared based on their accuracy computed from their confusion matrices. Among these methods, the multi-layer perceptron showed the best performance with the 83.2% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>The results reveal that an integrated AI-based feature selection and classification approach is an effective method for the early and accurate classification of ACS and ultimately a timely diagnosis and treatment of this disease.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Acute coronary syndrome (ACS) is caused by insufficient blood supply to the heart muscle which itself is mostly caused by the rupture of an atherosclerotic plaque resulting in a partial or complete blockage of coronary arteries <ref type="bibr" target="#b0">[1]</ref>. ACS is generally classified into three coronary arteries-related conditions: ST elevation myocardial infarction (STEMI), non ST elevation myocardial infarction (NSTEMI), and unstable angina (UA) <ref type="bibr" target="#b1">[2]</ref>.</p><p>ACS is one of the most common problems among patient admitted to the emergency departments. According to a conservative estimate, at least 6 million patients present to emergency departments with suspected ACS each year in the United States. In spite of the high frequency of this presentation, accurate diagnosis of ACS remains still challenging and requires a novel approach <ref type="bibr" target="#b2">[3]</ref>. It is estimated that there is a 2-5% chance of misdiagnosis among patients with suspected ACS, which is potentially lifethreatening <ref type="bibr" target="#b3">[4]</ref>. Therefore, developing an automatic diagnostic system, based on the available clinical data, can be an effective solution for reducing this risk.</p><p>Past literature has attempted to identify automatic predictions that classify the three types of ACS using pattern recognition or machine learning approaches <ref type="bibr" target="#b3">[4]</ref>. These methods have been developed on the basis of major clinical features of the ACS such as: ECG and Troponin level. Due to the overwhelming number of available features for each patient such as: age, weight, ECG, blood pressure, and medical history, it is quite challenging to select a subset of features that reliably contribute the most to the classification of ACS subtypes.</p><p>Pattern recognition algorithms have been widely used to classify UA from MI <ref type="bibr" target="#b4">[5]</ref>. While such studies have used various features, they have not classified ACS based on both the ECG findings and Troponin level. On the other hand, according to the World Health Organization, the diagnosis criteria of MI are the combinations of at least two of these three major factors: (1) typical clinical manifestations of infarction (i.e.; chest pain), <ref type="bibr" target="#b1">(2)</ref> change in marker's pattern and (3) a typical ECG pattern involving the ST-segment changes on ECG <ref type="bibr" target="#b5">[6]</ref>. In the current study, both the ECG findings and Troponin level have been used for ACS classification. Figure <ref type="figure" target="#fig_0">1</ref> demonstrates the algorithm for diagnosis of patients suspected to have ACS.</p><p>Artificial neural networks (ANNs) are powerful and effective tools for the classification and prediction of diseases. These methods are capable of constructing a nonlinear mapping between the input and output. Several studies have used ANNs for classification of ACS data. Harrison et al. <ref type="bibr" target="#b3">[4]</ref> used Multilayer Perceptron (MLP), which is a common type of ANNs, for differentiating UA from MI by selecting 13 out of 40 features. They have achieved a good predictive performance using ECG findings while excluding Troponin level.</p><p>Similar to Harrison et al.'s study, Forberg et al. <ref type="bibr" target="#b6">[7]</ref> considered only the ECG information for classifying ACS patients. In their study, the performance of ANNs and logistic regression were compared to the physicians' decisions. The results showed a relatively higher efficacy of logistic regression as compared to the ANN. Moreover, Colak et al. [8] showed a good efficacy of eight learning algorithms for ANNs in detecting ACS based on clinical data. This being said, one of the main limitations of employing ANNs for classification of ACS is the lack of explanations of the findings. This issue was addressed by comparing artificial datasets with real clinically recorded ACS data <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>Other artificial intelligence expert systems have also been used in the detection and classification of heart disease. For instance, Adeli et al. <ref type="bibr" target="#b9">[10]</ref> proposed a fuzzy expert system for classifying patients into five different groups: healthy, typical angina, atypical angina, non-angina, and asymptomatic. This system also uses clinical data such as ECG and blood indices for clinical decision making. The results obtained this way were comparable to the diagnosis of clinicians. Overall, it seems that all of the current ACS classification methods have been designed to discriminate MI from UA patients.</p><p>The present study aims to improve and extend the classification approach to discriminate among all three types of ACS: UA, NSTEMI, and STEMI. The classification methods that are introduced in this study were selected from eight well-known pattern recognition algorithms such as the Generalized Linear Models (GLMs), Adaptive Network Fuzzy Interface System (ANFIS), radial basis functions (RBF), k-nearest neighbor (k-NN), MLP, Naive Bayes, iterative dichotomiser-3 (ID3), and Baggin-ID3. Moreover, a feature selection algorithm based on the k-NN classifier was used to remove the redundant features of the dataset thereby increasing the efficacy of the proposed classification approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset technical information</head><p>For patients admitted with a tentative diagnosis of ACS to Imam Ali Hospital (i.e. the main center for cardiovascular care in Kermanshah, Iran) was completed the Euro Heart Survey on ACS. This questionnaire was designed by the European Society of Cardiology and has shown reliability and consistency: it was first conducted in 25 countries (in <ref type="bibr">[2000]</ref><ref type="bibr">[2001]</ref> and again in 32 European countries <ref type="bibr" target="#b10">[11]</ref>. All patients admitted with a tentative diagnosis of ACS to Imam Ali hospital in during 2010-2011 were included. According to the standard protocol of European ACS registry, all patients with unstable angina as well as those suspected of acute myocardial infarction were differentiated using elevating the cardiac markers: troponin, CK, and CK-MB and more than one of the suggestive characteristics such as (i) symptoms of myocardial ischemia, (ii) the development of new Q waves, and (iii) ST-T abnormalities suggestive for ischemic origin <ref type="bibr" target="#b11">[12]</ref>. A total number of 809 patients were enrolled in this study. They were divided into four different groups based on the ACS including: STEMI, NSTEMI, UA, and other. Similar to with previous studies, follow up data were collected within a year for every patients. The forms were completed by the attending physician. A data collection officer reviewed and checked each form for the probable missing data.</p><p>For each subject, 266 clinical factors were collected consisting of both numeric and nominal features. Based on interviews with cardiologists as well as the references in the literature, 40 seminal attributes for classifying ACS were selected. These factors along with the values and data types are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>In the current study, we utilized both numerical and categorical variables. The numerical variables were re-scaled to [-1, 1], by min-max normalization technique. The re-scaling was carried out in order to deal with the inconsistencies between different features. This transformation technique has two important advantages. The main advantage of min-max normalization lies in its ability to rescale the values so that they fall within a predetermined range. In addition, it reserves the relationships between the initial data <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern recognition methods</head><p>Different classification methods for modeling ACS data were applied to achieve different classifiers for classifying of new subjects. The classifying performance of these classifiers was compared with respect to their performance in classification prediction. These classification methods, described in the following section, were selected from different tools including GLMs, ANFIS, RBF, k-NN, MLP, Naive Bayes, ID3, and Bgging-ID3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized linear models</head><p>GLMs are powerful methods in applied statistical, which generalizes the ordinary linear models <ref type="bibr" target="#b13">[14]</ref>. In this approach, the output variable y is modeled by linear combination of input variables x i (features):</p><formula xml:id="formula_0">y ¼ ∑ i b i :x i<label>ð1Þ</label></formula><p>Assuming a probability function for the variables, the statistical mean of the output may have a certain link function as shown in Table <ref type="table" target="#tab_2">2</ref>  <ref type="bibr" target="#b13">[14]</ref>. Finally, using the generalized least square method, the unknown parameters of the model are estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-nearest neighbor</head><p>k-NN is known as a very simple and popular classification algorithm. k-NN classifier, for each new sample, finds the k neighbors nearest to the new sample from the training data. Euclidean distance or correlation measure is usually used to find these neighbors. </p><formula xml:id="formula_1">μ = Xb Inverse Gaussian μ -2 = Xb Poisson Log (μ) = Xb Gamma μ -1 = Xb</formula><p>The new sample is then assigned to the class which has the most abundance in the neighboring samples <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilayer perceptron</head><p>MLPs are the most common structures of the ANNs, which can be used for both regression and classification problems. MLP is known as a feed forward neural network trained by Back Propagation algorithm with one or more layers between input and output layer. Feed forward means that the data flows in one direction from the input to the output layer. In addition, back-propagation refers to the method for computing the gradient of the error function with respect to the weights for a feed-forward network. MLP consists of neurons which are connected to each other with some weights. Each neuron sums its inputs from the neurons of the previous layer and passes the sum through a sigmoidal or S-shaped activation function <ref type="bibr" target="#b15">[16]</ref>. It has been shown that an MLP with one hidden layer can produce enough complexity to map any input and output data <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Radial basis functions</head><p>RBF networks can be interpreted as feed-forward networks consisting of an input layer, a hidden layer and an output layer <ref type="bibr" target="#b16">[17]</ref>. In hidden layer each neuron consists of an activation function which is a radial basis kernel function (typically a Gaussian function). The output of the radial basis activation function is inversely proportional to the distance between its input and the center of the neuron. Although the structure of RBF networks resembles that of MLPs, their input-output mappings and training algorithms are basically different. RBFs are typically trained using a hybrid algorithm in two steps <ref type="bibr" target="#b17">[18]</ref>. In the first step, the hidden layer is trained (i.e. determining the radial basis centers and the spreads) by an unsupervised learning method.</p><p>In the next step, the output layer is trained (i.e. Predicting the target outputs) by a supervised learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive network fuzzy interface system</head><p>ANFIS is an integrated neural-fuzzy network based on neural network and fuzzy rules <ref type="bibr" target="#b18">[19]</ref>. The structure of this network is similar to an MLP; however, its neurons have different functions. Indeed, it is a special case of an adaptive network.</p><p>In adaptive networks there are two types of neurons (nodes): (i) fixed nodes which perform simple addition and multiplication, and (ii) adaptive neurons which have adaptive parameters and need to be estimated based on the input and the output data. In effect, this approach is generally a regression method which is used as a classifier in the classification problem. Thus, a tremendous performance from this classifier should not be expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naive Bayes</head><p>Applying Naive Bayes classifier, each new sample is assigned to the most probable class based on the Bayes decision making. The probability functions of the classes are empirically estimated from the training data. In spite of the low computational complexity, this method has a relatively high performance <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative dichotomiser-3</head><p>Decision trees are powerful and effective approaches to create a classification model. This method is a flow-chart-like tree structure, where a tree is constructed by the "if-then" rules (i.e. A logical sequence of questions) extracted from the training data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. A new case can be classified by starting at the root of the tree and moving through it until a leaf is encountered. Decision trees have become one of the most widely applied methods among numerous classification approaches, because these are white box models with easy-to-interpret results. In addition, its construction does not need any domain knowledge or parameter setting and thus is appropriate for exploratory knowledge discovery <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. ID3 is one of the major algorithms of decision tree which was used in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bagging-ID3</head><p>Bagging (Bootstrap aggregating) is a popular approach proposed by Bremen <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> which is considered as an ensemble meta-algorithm to build classification models. This resampling-based technique can be incorporated into various classification algorithms or regression methods. This incorporation resulted in reducing the variance associated with the prediction models, and thereby improves the predictive performance of these models. Bagging consists of creating numerous bootstrap replicates of the learning set by drawing "B" simple random samples with replacement (bootstrap samples) from the learning set and using these as new learning sets. Then, the considered prediction model is applied to each "B" bootstrap sample (i.e. new learning sets). To construct the final model, the results (i.e. the "B" built models) subsequently are combined into an ensemble by averaging for regression and simple voting for classification <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>.</p><p>In fact, the true strength of bagging approach is for unstable models, such as decision trees and neural networks. Unstable models are sensitive to small alterations in the dataset. Hence, training the same model on two slightly different training sets might result in substantially different models (i.e. The models with different parameters similar overall accuracies) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>. Thus, bagging can be a good solution to overcome this problem. In order to overcome unstably of the ID3, classifier bagging can be incorporated into the ID3 and emerge Bagging-ID3 classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature selection</head><p>Feature selection is one of the important steps in a classification problem. In reality, there are usually many redundant features which do not have any contributions in discriminating classes. Moreover, redundant features increase the complexity of the classification algorithm. Thus, they may have an effect on the performance of the model and may decrease its accuracy as well.</p><p>There are two main approaches for performing dimensionality reduction of high dimensional data <ref type="bibr" target="#b17">[18]</ref>. The first approach is feature extraction, which focuses on transforming the existing features into a lower dimensional space. Most feature extraction methods have been based on two major linear techniques: principal components analysis and Fisher's linear discriminant analysis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. Although they can considerably reduce the number of features, the resulting new features are still a function of the initial features. Thereby, it is usually impossible to find a physical interpretation of these new features. The second approach is feature selection, which is also called feature subset selection in the pattern recognition literature. The goal of the feature selection approach is to find an "optimal" subset of features that maximizes information content or predictive accuracy.</p><p>In classification problems, feature selection finds a subset of features which generates the best discrimination among classes. Some discrimination indexes can be used for this purpose <ref type="bibr" target="#b3">[4]</ref>. Since these indexes are easy to calculate, the whole subset searching procedure can be performed quickly. However, these indexes are independent of the classification algorithm and thus the selected subset may not be the best choice for the classification task. In proposing model by Peng and Jinjin <ref type="bibr" target="#b29">[30]</ref>, a genetic algorithm-based strategy for feature selection in heart disease classification is used. In this approach, the optimal subset of features is found using GA.</p><p>We utilized a procedure for feature selection to yield a subset of features with the best classification accuracy. To this end, a k-NN classifier for the classification and the elimination algorithm for feature selection was employed: 0-Set k to 0</p><p>1. k ← k+1 and S ←{f 1 , f 40 } 2. For i = 1, l where l is the size of current selected subset, S, do the following steps a. S i = S- <ref type="bibr" target="#b30">[31]</ref> b. Perform the classification task with current Si and k and repeat it 100 times using different randomly selected training and test data c. acc i = average of all accuracy values from previous step 3. Select the best subset: S ←S i *, where i* = argmax acc i 4. Go back to 2 until l = 1 5. Go back to 1 until k = 13 6. Ending this algorithm is optimized for both the selected subset of features and the parameter k of k-NN classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance assessment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model validation</head><p>Model validation is one of the most important steps in the model building process <ref type="bibr" target="#b31">[32]</ref>. Cross-validation is the most popular resampling-based model validation method <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>The various types of cross-validation method include: data holdout, repeated random subsampling, k-fold, and leave-one-out <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. In the current study, repeated random subsampling cross-validation method was adopted for the model validation. The dataset was split into two sets of training and test (i.e. two-way data splitting method). The training set was used to find the model's parameters and the test set was used to evaluate the generalizability performance of the final model. The process of train-test was repeated 50 to 1000 times (i.e. adopted according to the used model) using randomly selected training and test sets. Finally, the estimate of the overall error rate was derived by averaging all the separate error rate estimates produced from different iterations. Cross-validation method can help avoid two important issues in pattern recognition problems: (i) overfitting of the final model (i.e. the final model is unable to generalize unseen data) and (ii) the error rate estimate will be overly optimistic (i.e. lower than the true error rate) <ref type="bibr" target="#b30">[31]</ref>. It should be noted that in order to select the model and estimate the error rate simultaneously, three-way data splits technique should be applied during the cross validation process <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. In other words, the data should be divided into three disjoint sets namely training, validation, and test sets.</p><p>In this procedure, the training set was used for learning, i.e. to optimize the tuning parameters of the model (e.g. In MLP, in order to determine the optimal weights and the bias with the back-propagation rule). The validation set was used to optimize the regularization parameters of the model (e.g. In MLP, in order to determine the optimal number of hidden units and a stopping point of the algorithm). The test set was used only to estimate the error rate of the final model (fully-tuned model). After assessing the final model based on the test set, the model must not be further tuned. Table <ref type="table" target="#tab_3">3</ref> presents the data splitting method and also the number of repetitions (based on model's computation complexity) for each classifier method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model performance evaluation criteria</head><p>There are a number of criteria used to quantify the performance of a model <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. The performance of the final model can be evaluated by estimating the model accuracy rate. The evaluation operation is generally performed by comparing the predicted class labels with the actual class labels.</p><p>A matrix called Confusion Matrix (CM) is used to show the performance of a model for certain problems <ref type="bibr" target="#b31">[32]</ref>. If we have C classes, the CM is C×C matrix whose elements CM ij show the misclassified number of samples from class I into class j. Therefore, the rows and columns of this matrix show the actual and predicted class labels, respectively. In Table <ref type="table" target="#tab_4">4</ref>, part A shows a summing CM, the underlined number (i.e. corresponding to the predicted class of 4 and actual classes of 3) indicates that there are 11 samples from class 3 misclassified as class 4. Consequently, the smaller off-diagonal elements are the better performance of the classifier. When there are only two classes, other indexes such as sensitivity and specificity are usually used instead of CM.</p><p>A common index for evaluating the performance of a classifier is accurate which is calculated from the CM as follows:</p><formula xml:id="formula_2">Acc ¼ ∑ i C:M ii ∑ i ∑ j C:M ij<label>ð2Þ</label></formula><p>If the elements of this matrix are divided by the actual number of each class (i.e. which is equal to the sum of each row), each element (i,j) of the resulting matrix would show the prediction probability</p><formula xml:id="formula_3">p c Ã i c i Þ j À</formula><p>This conditional probability indicates the </p><formula xml:id="formula_4">p À c i jc Ã j Þ ¼ p À c Ã j jc i Þ:p c i ð Þ ∑ k p À c Ã j jc k Þ:p c k ð Þ<label>ð3Þ</label></formula><p>where p(c i ) is the prior probability of the class C i . The off-diagonal elements of these matrixes, CM, APM, or CPM, for the perfect ideal classifier are zero. The APM and CPM corresponded with CM is presented in Table <ref type="table" target="#tab_4">4</ref>, part B and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head><p>Table <ref type="table" target="#tab_5">5</ref> shows the distribution of ACS subtypes (Classes' names) in the ACS dataset. The feature selection algorithm was implemented for different odd values of k. The accuracy plots are shown in Figure <ref type="figure" target="#fig_1">2</ref> for the different odd values of k. As  displayed in Table <ref type="table" target="#tab_6">6</ref>, k = 7 provided the best accuracy for the seven features. As it was expected, Troponin and ECG were presented with these selected features which can be a validation of feature selection algorithms <ref type="bibr" target="#b5">[6]</ref>. It should be noted that, a limitation of the study (as in most medical studies) was the relatively limited sample size problem. Accordingly, in this study, all data were used in the feature selection process.</p><p>After obtaining the optimal features for the classification tasks, these features in all the classifiers were used and their performances were compared with APM and overall classification accuracy values. The resulted APMs from the GLM method with four different distributions are presented in Table <ref type="table" target="#tab_7">7</ref>. The values reported in this table are the mean and the standard deviation of each element. The overall classification accuracy values of these methods are shown in Table <ref type="table" target="#tab_8">8</ref>. Table <ref type="table" target="#tab_8">8</ref> shows that GLMs with normal distribution presented the best classification performance (i.e. 68.49± 3.93) among other distributions. However, Table <ref type="table" target="#tab_7">7</ref> shows that most of the samples from NSTEMI class were misclassified into STEMI class. Furthermore, most of the samples from "others" class were misclassified into UA class. Only the samples of STEMI and UA classes were classified correctly with an acceptable probability rate. Table <ref type="table" target="#tab_9">9</ref> presents the results of APMs obtained from the other aforementioned classification algorithms. To obtain these findings, the algorithms were repeated several times with different random selections of train-test or train-validation-test data based on the descriptions available in Table <ref type="table" target="#tab_3">3</ref>. For further clarification, for instance, the process of modelbuilding for the MLP classifier was described in details.</p><p>In the MLP classifier, the seven selected features and the four class labels were considered as input and output nodes respectively. At first, an MPL with "N" hidden nodes was considered. In the next step, three-way data splits technique and also repeated random sub-sampling cross-validation method were used. This is, the data set was divided into training, validation, and test sets. The train set was used for determining optimal weights with back -propagation rules, while incorporating the validation set. Validation  set was used to determine the optimal number of neurons in the hidden layer, as well as to avoid over-fitting (determine a stopping point for the back propagation algorithm). When the best "weights" were found, the performance of this network was calculated based on the classification error on the validation set. It should be mentioned that in order to make the final network unbiased, the train-validation-test process is repeated 100 times with different randomly selected starting values. Accordingly, the average of the 100 error values (based on the validation set) was considered as the final classification performance of the MLP with N hidden nodes. These steps were also done for different number of hidden nodes (from 2 to 13). At the end of this process, the best MLP having minimum average error value was determined as the final model. As was mentioned earlier, the validation set was used to select the final model; consequently, in order to achieve unbiased error rate estimation of the final model the testing set was used. In fact, once final model was chosen, its real accuracy is assessed on the test set. The optimum number of hidden layer neurons was determined 9 for MLP.</p><p>The overall classification accuracy of all the methods is shown in Table <ref type="table" target="#tab_10">10</ref>. The MLP followed by the 7-NN method had the best classification performance with overall accuracies of 83.24 ± 3.17% and 82.92 ± 2.45%, respectively. It should be mentioned that the priority of the k -NN method over other classification methods (except MLP) may be due to this fact that, the k-NN classifier takes the advantage of the feature selection k-NN-based method.</p><p>As we expected, Bagging-ID3 generated better results than ID3 due to the fact that it is actually a modified version of ID3. The results of this classifier were close to MLP showing its capability in our data classification task. On the other hand, the performance of 7-NN classifiers was also very close to MLP. By looking at the best resulting APMs belonging to the MLP classifier, it can be concluded that, firstly, the samples of "others" class were very similar to the samples of UA class because most of these samples were misclassified as UA class. It should be noted that this problem was caused by the fact that the sample percentage of this class (or correspondingly its prior probability) was smaller than UA (see Table <ref type="table" target="#tab_5">5</ref>). However, this problem is not crucial because the risk of this misclassification is not harmful for the patient. Secondly, a large percentage of NSTEMI samples were misclassified as STEMI and UA classes. This problem may be caused by its low prior probability or its similarity to the classes, especially to  the STEMI class. A misclassification of NSTEMI sample as STEMI class is not risky for the patients because the patients continue to remain under monitoring. However, being misclassified to the UA class could be harmful for the patient because the patient might be discharged. Nevertheless, this problem is not crucial in our MLP classifier.</p><p>Since the prior probability of classes we used are not the same, it is more appropriate to interpret the correctness of the classifier decision. This means that we should know the correct probability of a decision which assigns a sample to a class c Ã i . For this purpose, we can use CPM which is defined in this section. As mentioned earlier, each element (i,j)of CPM indicates the probability of a sample classified as c Ã i , actually belonging to c i or p c i jc Ã</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>j</head><p>. The CPM of MLP classifier is presented in Table <ref type="table" target="#tab_11">11</ref>.</p><p>It is observed that in UA class the 98.7% of the decisions were correctly made. Therefore, if the classifier assigns a patient to UA class, we should not worry about the risk that the patient has STEMI or NSTEMI. In other words, the risk of discharging an MI patient as a UA case was too low 1.35%.</p><p>For a better comparison of accuracy and correctness between all methods studied in this research, the bar graphs of diagonal elements of all APMs and CPMs were shown in Figures <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>, respectively. These Figures show the probabilities of p c Ã i jc i À Á</p><p>(See Figure <ref type="figure" target="#fig_2">3</ref>) and p c i jc Ã i À Á (See Figure <ref type="figure" target="#fig_3">4</ref>). It can be seen that the performance of MLP  classifier was significantly better than the rest. However, both accuracy and correctness measures for "others" and NSTEMI were not high enough which means that the classifier failed to model these regions of data. This problem can be solved by acquiring either more samples or new clinical features which can distinguish them more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Accuracy improvement strategies play a key role in correctly classifying ACS patients, which ultimately saves valuable time and prevents potential misdiagnoses. Artificial intelligencebased approaches are powerful strategy, which can be used to this end. The current study proposed an integrated artificial intelligence-based method in order to discriminate among different types of ACS: UA, STEMI, and NSTEMI, with greater accuracy than current methods. A k-NN-based feature selection algorithm was used to find a subset of the features with the best classification accuracy. As a result, the feature numbers considerably reduced to only seven. Finally, eight different common pattern recognition methods were used to classify the subtypes of ACS based on the seven selected features. The performance of the classifiers was then compared based on their accuracy computed from their confusion matrices. The MLP and 7-NN methods showed the highest accuracy was 83.24% and 82.92%, respectively. The GLM and ANFIS methods, on the other hand, showed the lowest overall classification accuracy of 68.5% and 71.3%, respectively. Overall, MLP showed the best performance between these classifiers. Although MLP classifier is slightly more accurate than k-NN classifier, k-NN has some advantages such as simple implementability, understandability and interpretability; hence, future research is needed to further elucidate this model. In summary, early accurate classification of ACS by the incorporation of an AI-based feature selection with an AI-based classifier demonstrated promising results that can be used in the clinical field to timely diagnose and treat ACS patients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 A</head><label>1</label><figDesc>Figure 1 A diagnostic algorithm of classification of ACS based on ECG changes and Troponin level.</figDesc><graphic coords="2,127.28,552.25,340.76,162.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Classification accuracy plots versus the number of selected features in k-NN classifier for different odd values of k (k=3, 5, 7, 9, 11 and 13).</figDesc><graphic coords="11,123.65,298.77,350.84,406.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Bar graph of diagonal elements of APM for all methods, each bar corresponds to the accuracy probability (i.e. p(c Ã i c i j )) of class c i .</figDesc><graphic coords="15,155.68,96.27,283.87,138.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Bar graph of diagonal elements of CPM for all methods, each bar corresponds to the correctness probability (i.e. p(c i c Ã i )) of class c i .</figDesc><graphic coords="15,155.68,573.56,283.87,130.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Detailed description of recorded clinical features of our ACS data</figDesc><table><row><cell>Label Feature name</cell><cell>Value</cell><cell>Scale</cell></row><row><cell>Sex</cell><cell>Male=1, Female=2</cell><cell>Nominal</cell></row><row><cell>Age</cell><cell>[-1,1]</cell><cell>Ratio</cell></row><row><cell>Living place</cell><cell>Urban=1, Rural=2</cell><cell>Nominal</cell></row><row><cell>(rural or urban)</cell><cell></cell><cell></cell></row><row><cell>Body Mass Index</cell><cell>[-1,1]</cell><cell>Ratio</cell></row><row><cell>History of prior myocardial</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>infarction</cell><cell></cell><cell></cell></row><row><cell>History of prior angina</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>pectoris</cell><cell></cell><cell></cell></row><row><cell>History of congestive heart</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>failure</cell><cell></cell><cell></cell></row><row><cell>History of stroke</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>History of chronic renal</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>failure</cell><cell></cell><cell></cell></row><row><cell>History of chronic lung</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>disease</cell><cell></cell><cell></cell></row><row><cell>Prioritize PCI</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>Prior CABG</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>Smoking status</cell><cell>Never=1, Former=2, Current=3</cell><cell>Ordinal</cell></row><row><cell>Diabetes mellitus</cell><cell>Non-diabetic=1, Newly diagnosed =2, Diabetic</cell><cell>Ordinal</cell></row><row><cell></cell><cell>(dietary control) =3, Diabetic (oral medication) =4,</cell><cell></cell></row><row><cell></cell><cell>Diabetic (oral MEDs + insulin) =5, Diabetic (insulin) =6</cell><cell></cell></row><row><cell>History of hypertension</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>History of</cell><cell>Absence=1, Presence=2</cell><cell>Nominal</cell></row><row><cell>hypercholesterolemia</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Detailed description of recorded clinical features of our ACS data (Continued)</figDesc><table><row><cell>32</cell><cell>CKMB mass elevated</cell><cell>Absence=-1, Presence=1</cell><cell>Nominal</cell></row><row><cell>33</cell><cell>Total Cholesterol value</cell><cell>[-1,1]</cell><cell>Ratio</cell></row><row><cell>34</cell><cell>Serum creatinine value</cell><cell>[-1,1]</cell><cell>Ratio</cell></row><row><cell>35</cell><cell>Glucose value</cell><cell>[-1,1]</cell><cell>Ratio</cell></row><row><cell>36</cell><cell>Hemoglobin value</cell><cell>[-1,1]</cell><cell>Ratio</cell></row><row><cell>37</cell><cell>Killip class</cell><cell>Class I=1, Class II=2, Class III=3, Class IV= 4</cell><cell>Ordinal</cell></row><row><cell>38</cell><cell>ECG rhythm</cell><cell>Sinus rhythm=1, Atrial fibrillation=2, Pacemaker=3, Other=4</cell><cell>Ordinal</cell></row><row><cell>39</cell><cell>ECG QRS annotation</cell><cell>Normal=1, RBBB=2, LBBB=3, Other=4</cell><cell>Ordinal</cell></row><row><cell>40</cell><cell>ECG STT changes</cell><cell>Normal=1, Other=2, ST depression=3, ST elevation=4</cell><cell>Ordinal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Different probability distribution and their corresponding link function used in GLMs</figDesc><table><row><cell>Distribution</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>The used data splitting methods and number of repetitions for each classifier method</figDesc><table><row><cell>NO.</cell><cell>Classifier</cell><cell>Data splitting method</cell><cell>Number of repetitions</cell></row><row><cell>1</cell><cell>ANFIS</cell><cell>Three-way</cell><cell>50</cell></row><row><cell>2</cell><cell>MLP</cell><cell>Three-way</cell><cell>100</cell></row><row><cell>3</cell><cell>RBF</cell><cell>Three-way</cell><cell>1000</cell></row><row><cell>4</cell><cell>Bagging ID3</cell><cell>Three-way</cell><cell>1000</cell></row><row><cell>5</cell><cell>ID3</cell><cell>Two-way</cell><cell>1000</cell></row><row><cell>6</cell><cell>GLM</cell><cell>Two-way</cell><cell>1000</cell></row><row><cell>7</cell><cell>k-NN</cell><cell>Two-way</cell><cell>1000</cell></row><row><cell>8</cell><cell>Naive Bayes</cell><cell>Two-way</cell><cell>1000</cell></row></table><note><p><p><p>probability that the classifier assigns a sample of class</p>C i to class c Ã j . Therefore, p c Ã i c i Þ j À is</p>the accuracy of the classifier for class C i . This matrix is called accuracy probability matrix (APM). Another useful probability measure is p c i jc Ã j which indicates that the probability of a sample classified as c Ã j actually belongs to C i . Similarly, p c i jc Ã j shows the classification correctness of the classification C i called correctness probability matrix (CPM) whose elements can be calculated simply from APM by the following relation:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>An example of CM, APM, and CPM</figDesc><table><row><cell>Actual class</cell><cell>Predicted class</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Class1</cell><cell>Class2</cell><cell>Class3</cell><cell>Class4</cell></row><row><cell>A: CM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Class1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1 1</cell></row><row><cell>Class2</cell><cell>0</cell><cell>4 9</cell><cell>1</cell><cell>5</cell></row><row><cell>Class3</cell><cell>0</cell><cell>6</cell><cell>1 2</cell><cell>1 1</cell></row><row><cell>Class4</cell><cell>3</cell><cell>0</cell><cell>7</cell><cell>9 5</cell></row><row><cell>B: APM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Class1</cell><cell>0.08</cell><cell>0.00</cell><cell>0.08</cell><cell>0.85</cell></row><row><cell>Class2</cell><cell>0.00</cell><cell>0.89</cell><cell>0.02</cell><cell>0.09</cell></row><row><cell>Class3</cell><cell>0.00</cell><cell>0.21</cell><cell>0.41</cell><cell>0.38</cell></row><row><cell>Class4</cell><cell>0.02</cell><cell>0.00</cell><cell>0.07</cell><cell>0.91</cell></row><row><cell>C: CPM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Class1</cell><cell>0.01</cell><cell>0.00</cell><cell>0.00</cell><cell>0.01</cell></row><row><cell>Class2</cell><cell>0.00</cell><cell>0.83</cell><cell>0.18</cell><cell>0.00</cell></row><row><cell>Class3</cell><cell>0.02</cell><cell>0.01</cell><cell>0.19</cell><cell>0.02</cell></row><row><cell>Class4</cell><cell>0.97</cell><cell>0.16</cell><cell>0.63</cell><cell>0.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Class sample distribution in the ACS dataset</figDesc><table><row><cell>Class label</cell><cell>Class name</cell><cell>Frequency</cell><cell>Percent</cell></row><row><cell>1</cell><cell>STEMI</cell><cell>224</cell><cell>27.69</cell></row><row><cell>2</cell><cell>NSTEMI</cell><cell>128</cell><cell>15.82</cell></row><row><cell>3</cell><cell>UA</cell><cell>417</cell><cell>51.55</cell></row><row><cell>4</cell><cell>Other</cell><cell>40</cell><cell>4.94</cell></row><row><cell></cell><cell>Total</cell><cell>809</cell><cell>100.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Final selected features resulted from the feature selection algorithm</figDesc><table><row><cell>Label</cell><cell>Variable name</cell></row><row><cell>4</cell><cell>Body Mass Index</cell></row><row><cell>10</cell><cell>History of Chronic lung disease</cell></row><row><cell>27</cell><cell>Chronic Home MEDs: Calcium channel blockers</cell></row><row><cell>30</cell><cell>Systolic blood pressure</cell></row><row><cell>31</cell><cell>Troponin I elevated</cell></row><row><cell>36</cell><cell>Hemoglobin value</cell></row><row><cell>40</cell><cell>ECG STT changes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>The result of APM from the GLMs method with four different distributions</figDesc><table><row><cell>GLM with</cell><cell>Actual class</cell><cell>Predicted class</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>STEMI</cell><cell>NSTEMI</cell><cell>UA</cell><cell>Others</cell></row><row><cell>Normal Dist.</cell><cell>STEMI</cell><cell>86.82 ± 4.59</cell><cell>1.55 ± 2.11</cell><cell>11.64 ± 4.28</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>60.52 ± 13.38</cell><cell>10.52 ±10.24</cell><cell>28.96 ± 8.36</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>UA</cell><cell>4.60 ± 2.17</cell><cell>12.16 ± 7.29</cell><cell>83.25 ± 7.23</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>Others</cell><cell>11.16 ± 11.28</cell><cell>7.76 ± 9.43</cell><cell>81.07 ±13.77</cell><cell>0.00 ± 0.00</cell></row><row><cell>Inv. Gaussian Dist.</cell><cell>STEMI</cell><cell>88.06 ± 4.47</cell><cell>0.5 ± 1.01</cell><cell>11.44 ± 4.44</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>69.26 ± 8.25</cell><cell>2.06 ± 2.86</cell><cell>28.68 ± 8.19</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>UA</cell><cell>4.72 ± 2.03</cell><cell>17.42 ± 9.52</cell><cell>77.86 ± 9.44</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>Others</cell><cell>12.39 ±11.3</cell><cell>8.57 ± 11.27</cell><cell>79.04 ±15.53</cell><cell>0.00 ± 0.00</cell></row><row><cell>Poisson Dist.</cell><cell>STEMI</cell><cell>87.78 ± 4.35</cell><cell>0.69 ± 1.32</cell><cell>11.54 ± 4.29</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>66.22 ±10.82</cell><cell>4.88 ± 6.70</cell><cell>28.90 ± 8.30</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>UA</cell><cell>4.82 ± 2.18</cell><cell>13.99 ± 7.73</cell><cell>81.19 ± 7.75</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>Others</cell><cell>12.23 ±11.21</cell><cell>7.83 ± 9.99</cell><cell>79.95 ±14.86</cell><cell>0.00 ± 0.00</cell></row><row><cell>Gamma Dist.</cell><cell>STEMI</cell><cell>87.78 ± 4.47</cell><cell>0.55 ± 1.1</cell><cell>11.67 ± 4.44</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>68.62 ± 9.03</cell><cell>2.49 ± 3.79</cell><cell>28.89 ± 8.24</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>UA</cell><cell>4.77 ± 2.10</cell><cell>15.66 ± 0.66</cell><cell>79.57 ± 8.63</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>Others</cell><cell>12.24 ±11.22</cell><cell>7.80 ± 9.74</cell><cell>79.96 ±14.17</cell><cell>0.00 ± 0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Overall classification accuracy values for GLMs with different distribution functions</figDesc><table><row><cell>Distribution</cell><cell>Mean</cell><cell>Std.</cell></row><row><cell>Normal</cell><cell>68.49</cell><cell>3.93</cell></row><row><cell>Inverse Gaussian</cell><cell>64.83</cell><cell>4.71</cell></row><row><cell>Poisson</cell><cell>66.73</cell><cell>3.99</cell></row><row><cell>Gamma</cell><cell>41.99</cell><cell>4.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc>The APM for different classifier methods</figDesc><table><row><cell>Classifier</cell><cell>Actual class</cell><cell>Predicted class</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>STEMI</cell><cell>NSTEMI</cell><cell>UA</cell><cell>Others</cell></row><row><cell>ANFIS</cell><cell>STEMI</cell><cell>88.72 ± 4.82</cell><cell>8.14 ± 4.15</cell><cell>1.57 ± 1.80</cell><cell>1.57 ± 1.85</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>31.21 ± 9.05</cell><cell>40.04 ± 9.19</cell><cell>23.33 ± 7.33</cell><cell>5.42 ± 4.53</cell></row><row><cell></cell><cell>UA</cell><cell>2.72 ± 1.91</cell><cell>17.43 ± 6.01</cell><cell>78.02 ± 5.93</cell><cell>1.84 ± 1.91</cell></row><row><cell></cell><cell>Others</cell><cell>5.79 ± 9.30</cell><cell>22.69 ±16.86</cell><cell>67.22 ±17.83</cell><cell>4.30 ± 6.71</cell></row><row><cell>7-nn</cell><cell>STEMI</cell><cell>94.24 ± 3.22</cell><cell>4.20 ± 2.84</cell><cell>1.56 ± 1.52</cell><cell>0.00 ± 0.00</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>31.07 ± 7.64</cell><cell>47.14 ± 8.72</cell><cell>21.78 ± 7.41</cell><cell>0.02 ± 0.22</cell></row><row><cell></cell><cell>UA</cell><cell>2.05 ± 1.32</cell><cell>3.64 ± 1.84</cell><cell>94.20 ± 2.23</cell><cell>0.11 ± 0.33</cell></row><row><cell></cell><cell>Others</cell><cell>7.44 ± 7.79</cell><cell>7.28 ± 7.77</cell><cell>85.23 ± 10.31</cell><cell>0.05 ± 0.80</cell></row><row><cell>Native Bayes</cell><cell>STEMI</cell><cell>83.22 ± 4.78</cell><cell>2.59 ± 1.96</cell><cell>11.99 ± 3.94</cell><cell>2.19 ± 2.59</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>20.06 ± 6.32</cell><cell>47.59 ± 7.40</cell><cell>28.98 ± 7.35</cell><cell>3.36 ± 2.96</cell></row><row><cell></cell><cell>UA</cell><cell>0.04 ± 0.20</cell><cell>7.16 ± 6.58</cell><cell>86.17 ± 7.23</cell><cell>6.64 ± 4.15</cell></row><row><cell></cell><cell>Others</cell><cell>0.84 ± 2.88</cell><cell>7.85 ±10.09</cell><cell>80.99 ±12.58</cell><cell>10.32 ± 8.85</cell></row><row><cell>ID3</cell><cell>STEMI</cell><cell>84.55 ± 5.81</cell><cell>13.3 ± 5.61</cell><cell>1.92 ± 1.88</cell><cell>0.23 ± 0.73</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>26.59 ± 7.58</cell><cell>46.05 ± 9.00</cell><cell>25.09 ± 7.91</cell><cell>2.28 ± 2.99</cell></row><row><cell></cell><cell>UA</cell><cell>0.94 ± 0.94</cell><cell>6.05 ± 2.84</cell><cell>88.08 ± 3.76</cell><cell>4.93 ± 2.64</cell></row><row><cell></cell><cell>Others</cell><cell>3.09 ± 5.56</cell><cell>12.41 ± 10.70</cell><cell>78.63 ±13.60</cell><cell>5.87 ± 8.03</cell></row><row><cell>Bagging-ID3</cell><cell>STEMI</cell><cell>91.77 ± 3.94</cell><cell>6.54 ± 3.51</cell><cell>1.65 ± 1.73</cell><cell>0.03 ± 0.23</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>30.59 ± 7.41</cell><cell>46.93 ± 7.29</cell><cell>22.32 ± 6.85</cell><cell>0.16 ± 0.72</cell></row><row><cell></cell><cell>UA</cell><cell>1.13 ± 0.85</cell><cell>3.70 ± 1.83</cell><cell>94.07 ± 2.36</cell><cell>1.09 ± 1.05</cell></row><row><cell></cell><cell>Others</cell><cell>3.21 ± 4.87</cell><cell>9.84 ± 8.05</cell><cell>85.00 ± 9.50</cell><cell>1.95 ± 4.30</cell></row><row><cell>RBF (7 neurons)</cell><cell>STEMI</cell><cell>84.99± 4.98</cell><cell>1.76 ± 1.81</cell><cell>13.18 ± 4.74</cell><cell>0.07 ± 0.37</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>30.67±17.98</cell><cell>34.88 ±17.88</cell><cell>33.98 ±10.00</cell><cell>0.47 ± 1.31</cell></row><row><cell></cell><cell>UA</cell><cell>2.34 ± 2.34</cell><cell>2.18 ± 1.73</cell><cell>95.41 ± 2.35</cell><cell>0.07 ± 0.27</cell></row><row><cell></cell><cell>Others</cell><cell>5.40± 8.81</cell><cell>8.37 ± 9.10</cell><cell>84.88 ±12.10</cell><cell>1.34 ± 3.81</cell></row><row><cell>MLP (9 neurons)</cell><cell>STEMI</cell><cell>93.78 ± 5.05</cell><cell>2.67 ± 2.72</cell><cell>3.38 ± 4.07</cell><cell>0.17 ± 0.68</cell></row><row><cell></cell><cell>NSTEMI</cell><cell>29.93 ±10.41</cell><cell>48.33 ±10.21</cell><cell>21.43 ± 9.20</cell><cell>0.31 ± 1.14</cell></row><row><cell></cell><cell>UA</cell><cell>0.73 ± 1.26</cell><cell>2.98 ± 2.11</cell><cell>96.21 ± 2.55</cell><cell>0.08 ± 0.34</cell></row><row><cell></cell><cell>Others</cell><cell>3.07 ± 6.56</cell><cell>9.76 ± 12.30</cell><cell>82.54 ±15.10</cell><cell>4.62 ± 9.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10</head><label>10</label><figDesc>Overall classification accuracy of all the methods</figDesc><table><row><cell>Classifiers</cell><cell>Accuracy (%)</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>Std.</cell></row><row><cell>GLM with normal Dist.</cell><cell>68.49</cell><cell>3.93</cell></row><row><cell>ANFIS</cell><cell>71.31</cell><cell>3.73</cell></row><row><cell>7-NN</cell><cell>82.92</cell><cell>2.45</cell></row><row><cell>Naive Bayes</cell><cell>75.51</cell><cell>4.14</cell></row><row><cell>ID3</cell><cell>76.33</cell><cell>2.83</cell></row><row><cell>Bagging-ID3</cell><cell>81.12</cell><cell>2.43</cell></row><row><cell>RBF</cell><cell>78.42</cell><cell>3.59</cell></row><row><cell>MLP</cell><cell>83.24</cell><cell>3.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 The</head><label>11</label><figDesc>CPM of MLP method (with 9 neurons)</figDesc><table><row><cell>Actual class</cell><cell>Predicted class</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>STEMI</cell><cell>NSTEMI</cell><cell>UA</cell><cell>Others</cell></row><row><cell>STEMI</cell><cell>92.28</cell><cell>30.70</cell><cell>0.40</cell><cell>1.88</cell></row><row><cell>NSTEMI</cell><cell>1.50</cell><cell>28.32</cell><cell>0.94</cell><cell>3.42</cell></row><row><cell>UA</cell><cell>6.19</cell><cell>40.92</cell><cell>98.65</cell><cell>94.19</cell></row><row><cell>Others</cell><cell>0.03</cell><cell>0.06</cell><cell>0.01</cell><cell>0.51</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>http://www.tbiomed.com/content/10/1/57</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank <rs type="person">Dr. Hossein Moayedi</rs> for his support during the study.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declared that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors' contributions</head><p>In this study all authors have had key contribution and also, read and approved the final manuscript.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clinical diagnosis of acute coronary syndrome in patients with chest pain and a normal or non-diagnostic electrocardiogram</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goodacre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Crowder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pitcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emerg Med J</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">866</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of the acutely dyspneic elderly patient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moayedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinics in geriatric medicine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="307" to="325" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of the elderly patient with acute chest pain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinics in geriatric medicine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="327" to="349" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Artificial neural network models for prediction of acute coronary syndromes using clinical data from the time of presentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of emergency medicine</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="431" to="439" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying the severity of an acute coronary syndrome by mining patient data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Odeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Davidsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Annual Workshop of the Swedish Artificial Intelligence Society</title>
		<imprint>
			<publisher>Blekinge Institute of Technology</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ESC Guidelines for the management of acute coronary syndromes in patients presenting without persistent ST-segment elevation The Task Force for the management of acute coronary syndromes (ACS) in patients http://www.tbiomed</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-P</forename><surname>Bassand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agewall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Caso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dudek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gielen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huber</surname></persName>
		</author>
		<ptr target="com/content/10/1/57presentingwithout" />
	</analytic>
	<monogr>
		<title level="j">Eur Heart J</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2999" to="3054" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>persistent ST-segment elevation of the European Society of Cardiology (ESC)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">In search of the best method to predict acute coronary syndrome using only the electrocardiogram from the emergency department</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Forberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bjork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Edenbrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ekelund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Electrocardiol</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="58" to="63" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel approach for classification of ECG arrhythmias: type-2 fuzzy clustering neural network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Özbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="6721" to="6726" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring new possibilities for case-based explanation of artificial neural network ensembles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ekelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Edenbrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bjork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Forberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="75" to="81" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fuzzy expert system for heart disease diagnosis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neshat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International MultiConference of Engineers and Computer Scientists</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Hong Kong</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The second Euro Heart Survey on acute coronary syndromes: characteristics, treatment, and outcome of patients with ACS in Europe and the Mediterranean Basin in 2004</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mandelzweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Boyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Danchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Filippatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hasdai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marrugat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur Heart J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2285" to="2293" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Prediction of risk of death and myocardial infarction in the six months after presentation with acute coronary syndrome: prospective multinational observational study (GRACE)</title>
		<author>
			<persName><forename type="first">Kaa</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">H</forename><surname>Dabbous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Pieper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Eagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>De Werf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Avezum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Flather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1091">2006. 1091</date>
			<biblScope unit="volume">333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Data mining: concepts and techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>Burlington, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An introduction to generalized linear models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting coronary artery disease using different artificial neural network models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kocaturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barutçu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anadolu Kardiyol Derg</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="249" to="254" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Clinical applications of artificial neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dybowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural networks for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pattern analysis for machine olfaction: a review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gutierrez-Osuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="189" to="202" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ANFIS: adaptive-network-based fuzzy inference system</title>
		<author>
			<persName><forename type="first">Jsr</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="665" to="685" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting corporate financial distress based on integration of decision tree classification and logistic regression</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="11261" to="11272" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reusable components in decision tree induction algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suknovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Delibasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jovanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vukicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Becejski-Vujaklija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Stat</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="127" to="148" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4. 5: programs for machine learning</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>CRC Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On aggregating belief decision trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vannoorenberghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information fusion</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification and regression trees, bagging, and boosting</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="303" to="329" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dealing with noise in defect prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering (ICSE), 2011 33rd International Conference</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An experimental study on diversity for bagging and boosting with linear classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Skurichina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information fusion</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="245" to="258" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Improvement of Naive Bayesian Classifier Based on the Strategy of Fuzzy Feature Selection with the Dual Space, Wireless Communications, Networking and Mobile Computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jinjin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WiCom 2007</title>
		<meeting><address><addrLine>Shanghai</addrLine></address></meeting>
		<imprint>
			<publisher>International Conference</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="5532" to="5534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Estimating and Comparing Classifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dougherty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="157" to="176" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selecting representative data sets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Borovicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jirina</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kordik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jirina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intech: Advances in Data Mining Knowledge Discovery and Applications</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal classifier selection and negative bias in error rate estimation: an empirical study on high-dimensional prediction</title>
		<author>
			<persName><forename type="first">A-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical research methodology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Resampling methods: concepts, applications, and justification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Practical Assessment, Research &amp; Evaluation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Application of pattern recognition tools for classifying acute coronary syndrome: an integrated medical modeling</title>
		<author>
			<persName><forename type="first">R ;</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><surname>Salari</surname></persName>
		</author>
		<idno type="DOI">10.1186/1742-4682-10-57</idno>
	</analytic>
	<monogr>
		<title level="j">International joint Conference on artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="1995">1995. 2013</date>
			<pubPlace>Montreal, Quebec, Canada</pubPlace>
		</imprint>
	</monogr>
	<note>Theoretical Biology and Medical Modelling</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
