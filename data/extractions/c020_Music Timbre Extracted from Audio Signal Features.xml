<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retraction Retracted: Music Timbre Extracted from Audio Signal Features Mobile Information Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-06-16">16 June 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ying</forename><surname>Mo</surname></persName>
							<email>00071027@wzu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Music College</orgName>
								<orgName type="institution" key="instit2">Wenzhou University</orgName>
								<address>
									<postCode>325035</postCode>
									<settlement>Wenzhou Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retraction Retracted: Music Timbre Extracted from Audio Signal Features Mobile Information Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-16">16 June 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">9FE72C68960D1CDB9A88BFDA711D993B</idno>
					<idno type="DOI">10.1155/2022/1349935</idno>
					<note type="submission">Received 1 August 2023; Accepted 1 August 2023; Received 15 April 2022; Revised 18 May 2022; Accepted 3 June 2022;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-03-11T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Among the basic elements of music, timbre is one of the most important elements of sound, and it is also the main basis for distinguishing one pronunciation from another. People usually have the ability to "listen and argue" because everyone's pronunciation is different. However, the existing audio extraction technology has low efficiency and low accuracy. )erefore, this paper aims to discuss the algorithm that can make music timbre feature extraction more accurate and efficient. For audio signal feature extraction, this paper proposed an audio feature based on harmonic components to describe the harmonic structure information in the audio signal spectrum. )e algorithm in this paper extracts timbre features from the sound data of Western musical instruments and national musical instruments and analyzes the recognition accuracy. )e experimental results showed that the classification accuracy of the four feature extractors is above 92%, among which B has the worst effect, with an accuracy of 92.42%, and D has the best classification effect, with an accuracy of 99.15%, which shows that the feature extraction algorithm designed in this paper combined with the traditional feature extraction algorithm can achieve better results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tis article has been retracted by Hindawi following an investigation undertaken by the publisher <ref type="bibr">[1]</ref>. Tis investigation has uncovered evidence of one or more of the following indicators of systematic manipulation of the publication process:</p><p>(1) Discrepancies in scope <ref type="bibr" target="#b2">(2)</ref> Discrepancies in the description of the research reported <ref type="bibr" target="#b3">(3)</ref> Discrepancies between the availability of data and the research described (4) Inappropriate citations (5) Incoherent, meaningless and/or irrelevant content included in the article (6) Peer-review manipulation Te presence of these indicators undermines our confdence in the integrity of the article's content and we cannot, therefore, vouch for its reliability. Please note that this notice is intended solely to alert readers that the content of this article is unreliable. We have not investigated whether authors were aware of or involved in the systematic manipulation of the publication process.</p><p>Wiley and Hindawi regrets that the usual quality checks did not identify these issues before publication and have since put additional measures in place to safeguard research integrity.</p><p>We wish to credit our own Research Integrity and Research Publishing teams and anonymous and named external researchers and research integrity experts for contributing to this investigation.</p><p>Te corresponding author, as the representative of all authors, has been given the opportunity to register their agreement or disagreement to this retraction. We have kept a record of any response received.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sound is one of nature's most common signals. Sound is produced by the vibration of objects. Its existence even predates the existence of living things. As long as there is the vibration and transmission medium of objects, there is the generation of sound signals. Since the birth of human beings, sound has always occupied the most important part of people's lives. For example, natural sounds convey the information of nature to people: if you hear the wind, it means the wind is coming; if you hear the sound of rain, it means it is raining; if you hear the sound of water, it means there are rivers and oceans nearby. Language is the most important communication tool in human society, as a signal for transmitting messages between people. It is convenient, natural, and efficient, and can accurately convey various messages. As for music, its content has risen to the height of human art. People can use different musical instruments to play music in various poses and sounds, and express their emotions such as happiness, anger, sorrow, and music with the help of music.</p><p>In the subject of sound, the component analysis of audio signals has always occupied the mainstream of audio signal analysis technology. Various components of the audio signals determine the theme emotion expressed by the music signals. By analyzing the characteristics of various components of the audio signal, the characteristic parameters of the music can be extracted, and the music signal can be classified, identified, and retrieved. )is is of great significance for establishing a high-performance and high-accuracy music retrieval database and implementing music classification algorithms based on music style, music content, and musical instruments.</p><p>)e main innovations of this paper are as follows: (1) feature extraction-this part mainly analyzes the characteristics of the audio signal on the basis of preprocessing and then extracts the characteristic curve of the audio signal to pave the way for the subsequent audio melody matching. <ref type="bibr" target="#b2">(2)</ref> Audio feature library construction-this part mainly studies the music in MIDI file format and uses the improved contour algorithm to build the audio feature library, which is used as the data source for melody matching. )is plays an</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><p>important role in the timbre analysis of various musical instruments in the text, and he can distinguish the timbres of various musical instruments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>)e timbre analysis is of great significance not only to music and musical instruments, but also to the current simultaneous interpretation, speech recognition, etc., so there are many studies on timbre analysis. <ref type="bibr">Kim et al.</ref> have done research on classifying musical instruments from polyphonic music, which he believes is a challenging but important task in music information retrieval <ref type="bibr">[1]</ref>. Tatar et al. introduce latent timbre synthesis, a new approach to audio synthesis using deep learning. )is synthesis method allows composers and sound designers to interpolate and extrapolate between the timbres of multiple sounds in the latent space of audio frames <ref type="bibr" target="#b2">[2]</ref>. Banerjee et al. believe that the analysis of sound signals in the linear deterministic approach reached a new dimension and developed many well-equipped software to precisely measure and control the basic parameters of sound, such as pitch, intensity, and rhythm <ref type="bibr" target="#b3">[3]</ref>. Rossetti and Manzolli believe that analyzing electroacoustic music is a challenging task that can be solved by different strategies. He proposed to use representations of complex dynamical systems (such as phase space graphs) in music analysis to reveal the timbre characteristics that arise in acoustic music based on particle techniques <ref type="bibr" target="#b4">[4]</ref>.</p><p>It can be seen that most of the timbre analysis uses audio signal extraction technology and deep learning technology. )ere are also many studies on the extraction of audio signal features. An overview and benchmarking of Sharan et al. test various audio signal representation techniques for classification using CNNs, including methods for processing signals of different lengths and combining multiple representations to improve classification accuracy <ref type="bibr" target="#b5">[5]</ref>. Santosh et al. believe that speech, music, and audio signals are essential for communication (e.g., sharing information) and entertainment. )is automatic processing of signal processing reduced expert and/or human intervention <ref type="bibr" target="#b6">[6]</ref>. Baxter believes that audio metering and monitoring can be described as the ability to audibly or visually determine certain characteristics of an audio signal. For example, loudness measurements are accumulated over a period of time, and it may be difficult for a mixer to detect changes in audio by ear through control room speakers, but the changes obtained through LKFS meters are noticeable <ref type="bibr" target="#b7">[7]</ref>. However, it can also be clearly found that most of the current audio signal extraction technology is limited to signal extraction, and the accuracy in removing noise and identifying timbre is not high enough, so this article will conduct in-depth research on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Music Tone and Its Principle</head><p>3.1. Principle of Hearing. )e process of human perception of audio information is carried out through the hearing of the human ear. )e process of hearing includes from sound vibrations to changes in electrical potential and the release of chemicals, to the emergence of nerve impulses, and finally to central information processing. )erefore, to understand the human perception and cognitive mechanism of audio information, it is necessary to start from the physiological structure of the human ear and the process of auditory perception.</p><p>)e ear is an important human sense. )e function of the ear is to first receive external sounds and then convert the received sounds into neural signals that humans can recognize. Sound perception refers to the internal processing of the received sound through the brain and finally converts it into semantics that humans can understand <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>. )e human ear consists of three parts: the outer ear, the middle ear, and the inner ear. )e cochlea of the ear is the human auditory sense; the inner ear is responsible for position determination and balance; the outer ear is responsible for sensing external sounds; the middle ear is responsible for transmitting external sounds to the inner ear; the inner ear is responsible for converting the incoming sound energy into human nerve stimulation. )e structure of the human ear is shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>)e perception process of the human ear is inseparable from the brain, which is a very sensitive organ. )e eardrum vibrates when sound waves from the external environment travel through the air in the external auditory canal to the eardrum. )e vibration of the tympanic membrane is transmitted by the ossicles through the middle ear. During the transmission process, the ossicles vibrate the cochlear fluid, which in turn causes the basilar membrane to vibrate and finally generates traveling waves. Different sounds produce different traveling waves. When the organ of Corti with the basement membrane vibrates, electrical potentials appear on the hair cells. )e characteristic of this potential is that the frequency and waveform are the same as the external stimulus sound. )is potential also stimulates the nerve fibers in the lower part of the hair cells, generating action potentials. )is action potential causes a change in the electrical potential between the auditory nerve and the hair cell, which in turn creates a chemical reaction. A certain substance produced by the chemical reaction will stimulate the nerve endings and finally transmit the excitement generated by the nerve endings to the nerve center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tone.</head><p>)e basic knowledge of music theory is essential, and all music production is closely related to this knowledge. )ese music theory knowledge can help us better understand the elements in music. Music melody includes many elements: melody, rhythm, beat, speed, dynamics, range, pitch, sound intensity, sound value, timbre, etc. <ref type="bibr" target="#b10">[10]</ref>. )e common physical quantities of sound, such as loudness, pitch, and timbre, are shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>Timbre: the timbre of the sound. )e difference in timbre is mainly due to the different inherent properties of objects. Different timbres can be combined into a different very nice music, and the timbre can also determine the image of the music.</p><p>From a disciplinary point of view, the concept of timbre contains multiple attributes such as physical and psychological. In different academic fields, the definition of timbre and its impact will also be different. From the point of view of physical acoustics, timbre is a certain property of sound produced by hearing, and it is a kind of vibration wave propagating in the medium. )e timbre usually heard is a composite tone composed of the fundamental tone and an overtone produced by the sounding body. )erefore, the structure, material, shape of the sounding body, the number of overtones above the fundamental sound generated by these factors of the sounding body, and the amplitude of each overtone are the fundamental reasons that affect the timbre. )erefore, in physical acoustics, the difference in timbre depends on the distribution and intensity of each overtone <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. From a psychological point of view, timbre is a key factor in creating an auditory linkage between performers and listeners. )e performer can convey the musical mood and musical image of the musical work to the audience through the timbre, so that the audience can produce synesthesia in the auditory through the unique timbre perception. )is synesthesia effect directly affects people's emotions and subjective cognition, which is an important basis for music    </p><formula xml:id="formula_0">O t i O t i (a) O t i O t i (b) O t i O t i (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><p>therapy. Music therapy is one of the important methods and means of psychotherapy, which is to relieve emotions and cure diseases through music <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>. It uses various forms of musical activities, including listening, singing, playing, rhythm, and other means to stimulate and hypnotize people and stimulate physical responses to sound, so that people can achieve health goals. Among them, the choice of different timbres and different sound qualities has an essential and even decisive influence on the treatment. It is mentioned in "Huangdi Neijing" that the melodious, quiet, honest, and solemn tone is as broad and firm as "earth" and can enter the spleen.</p><p>In the theory of musicology, timbre is an auditory property, and the timbre of a musical tone refers to the sound property produced by a musical instrument when a note is played. When different musical instruments play notes of the same pitch, strength, and time value, the listeners will perceive the sound perception of different attributes, that is, the difference in timbre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tone Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Tone Classification of a Single Instrument.</head><p>)ere are many kinds of musical instruments, and understanding each musical instrument and its classification will help us learn and train timbres. )is paper discusses the classification of musical instrument timbres into the two categories of Chinese national musical instruments and Western musical instruments. In the classification of traditional Chinese national musical instruments, the "Introduction to Ethnic Art" of the Chinese Academy of Arts divides musical instruments into wind instruments, stringed instruments, plucked instruments, and percussion instruments according to the performance of the instruments themselves and the differences in their playing methods.</p><p>In the orchestration method, according to the principle of orchestration, various Western musical instruments divide the modern symphony orchestra into different musical instrument groups according to their own materials, structure, sounding methods, and performance skills: woodwinds, brass, percussion, plucked, bowed, and keyboards. Some of the Western and ethnic musical instruments are shown in Figure <ref type="figure">3</ref>.</p><p>To sum up, to have a clearer understanding of the classification of musical instrument timbres, the author divides the common Western musical instruments in the table through first-level classification, second-level classification, and third-level classification. )e sound classification of each musical instrument is shown in Tables <ref type="table" target="#tab_3">1</ref> and<ref type="table" target="#tab_4">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Tone Classification of Musical Instrument</head><p>Combinations. In this paper, according to the classification of a single instrument timbre, the instrument combination timbre is defined as the combination of two or more musical instrument timbres. )e definition of instrument combination is extremely simple, but the form of instrument combination tone is very rich, there are two instruments playing in unison at the same time, and there are also band formations formed by a combination of multiple instruments. According to the division of musical instrument timbre types in this section, the timbres of musical instruments can also be divided into two types: national orchestra and Western orchestra. )e national orchestra includes four categories of wind instruments, stringed instruments, plucked instruments, and percussion instruments. Western orchestras are also known as symphony orchestras. Western musical instruments can also be divided into four groups according to the origin and timbre of the instruments-woodwind, brass, strings, and percussion. )e combination forms of the two bands are extremely rich. With the development of composition technology and the development of cultural exchanges, Chinese and Western instruments also appear in many musical works at the same time. Due to the wide variety of Chinese and Western musical instruments, the combinations are endless. In this article, the author takes Western musical instruments and Western orchestras as the main content and conducts research and discussion on musical instrument timbre perception training <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Classification of Human Voices.</head><p>In the classification of vocal timbre, according to the bel canto method, the timbre difference of human voice can be divided into children's voice, male voice, and female voice according to age and gender; according to the division of the range, it can be divided into soprano, alto, and bass; in the field of mixed chorus, it can be divided into soprano, mezzo-soprano, tenor, and bass. After synthesizing various vocals, it is divided into children's voices, male voices, and female voices as a whole. To sum up, most scholars divide human voices into children's voices, male voices (tenor, baritone, bass), and female voices (soprano, mezzo-soprano, and alto) after synthesizing the above classification basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Audio Feature Extraction Based on Harmonic Components</head><p>)is chapter proposes an audio feature based on harmonic components to describe the harmonic structure information in the audio signal spectrum. )is chapter first introduces the commonly used frequency-domain features and then introduces the human brain's perception characteristics of harmonic signals. In particular, the research results in psychoacoustics on the use of fundamental frequency, spectral peaks, and timbre in the harmonic signal by the human brain to distinguish different audio signal types. On this basis, a harmonic dictionary is proposed, which describes the harmonic structure in the spectrum by constructing harmonic atoms composed of fundamental frequency, formant, and overtone energy decay rates in the frequency domain. ear of the human ear. )rough the resonance of the cilia, the mechanical vibrations are converted into nerve impulses.</p><p>After the impulses reach the brain, they "hear" the sound through a series of higher-level perceptions <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. Since ciliary resonance reflects that the human ear can convert time-domain signals into frequency-domain signals, researchers have proposed a variety of frequency-domain features that reflect spectral characteristics. )e discrete Fourier transform (DFT) can transform a sequence of timedomain samples of an audio signal into the frequency domain, converting the audio signal x m of the mth frame length of N sampling points to the frequency domain; X m can be calculated by the following formula:</p><formula xml:id="formula_1">X m (k) � 􏽘 N n�1</formula><p>x m (ne) -2iπnk/N .</p><p>(1)</p><formula xml:id="formula_2">Among them, n (1 ≤ n ≤ N) and k (1 ≤ k ≤ K)</formula><p>are the timedomain and frequency-domain sampling indices, respectively, and K is the DFT length. On this basis, there are various spectral features used to describe the distribution characteristics of the spectrum:</p><p>(1) Bandwidth. )e bandwidth of an audio signal describes whether the frequency distribution of the signal is more dispersed or relatively concentrated and is defined as follows:</p><formula xml:id="formula_3">BW m � ��������������������� 􏽐 K k�1 k -SC m 􏼁 2 X m (k) 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 2 E m 􏽶 􏽴 . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>)e bandwidth of speech is generally 300∼3400 Hz, and the bandwidth of music is usually much larger </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><p>than that of speech, which can be as high as 22 KHz <ref type="bibr" target="#b19">[19]</ref>. (2) Sub-band energy ratio. )e spectral distribution of audio signals generated by different sound sources is different. For example, the spectral energy of speech signals is mainly concentrated in the low-frequency part, while the spectral distribution of music signals is relatively average. )erefore, by dividing the frequency band into several sub-bands, and separately calculating the energy ratio of each sub-band to the entire spectrum, the distribution characteristics of the spectral energy can be roughly described.</p><formula xml:id="formula_5">SER m,i � 􏽐 H i k�L i X m (k) 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 2 E m . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Among them, H i and L i , respectively, represent the upper and lower frequency of the ith sub-band, and the sub-band bandwidth H i -L i can be divided into equal or unequal lengths. (3) Sub-band spectral flux. Sub-band spectral flow refers to the cumulative change of the corresponding intensity of adjacent frequencies in each sub-band of the spectrum, which can be used to detect the frequency components of sudden changes in each subband, and is defined as</p><formula xml:id="formula_7">SF m,i � 1 H i -L i 􏽘 H i k�L i 􏽢 X m (k + 1) -􏽢 X m (k) 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌.<label>(4)</label></formula><p>Among them, 􏽢 X m (k) refers to the normalized spectral signal. )e normalization is to avoid the scale difference between different frames due to different energies. It is achieved by converting the spectral energy into a decibel scale and normalizing it to unit energy, that is,</p><formula xml:id="formula_8">􏽢 X m (k) � 10log 10 X m (k) ����������������� 􏽐 k k�1 10log 10 X m (k) 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 􏼌 2 􏽱 . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>(4) Spectral roll-off point.</p><p>)e spectral roll-off point is defined as at frequency RP m , the sum of the spectral amplitudes less than this frequency accounts for 85% of the sum of the entire spectral amplitudes, that is,</p><formula xml:id="formula_10">􏽘 RP m k�1 X m (k) � 0.85 􏽘 K k�1 X m (k). (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>)e spectral roll-off point describes the energy ratio of the low-frequency part and the overall shape of the spectrum. (5) Linear prediction coefficient (LPC) and linear spectrum pair (LSP). Linear prediction coefficient refers to the method of describing the vocal tract model that produces speech using linear prediction analysis <ref type="bibr" target="#b20">[20]</ref>, as shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>Usually, an all-pole model is used to describe the vocal tract model, as</p><formula xml:id="formula_12">H p (z) � G A p (z) � G 1 -􏽐 p i�1 a i z -i . (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>Among them, G is the gain, A p (z) is the transfer function of the p-order linear filter, and a i , i � 1, ... , p represents the linear prediction coefficient, that is, the filter parameter. )e prediction coefficient is usually obtained by minimizing the prediction error, and the solution algorithms include covariance method, autocorrelation method, and Levinson-Durbin grid method. Among them, the Levinson-Durbin algorithm is the most commonly used <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>Line spectrum pair is a feature based on linear prediction and a deduction parameter of linear prediction coefficient. In harmonic signals, this feature describes the distribution characteristics of formants (i.e., peaks of spectral envelope). )e line spectrum pair feature treats the channel as a cascade of p + 1 resonant cavities, which represent the resonant frequencies of the resonators when the excitation energy is at a local minimum or a local maximum, respectively. In speech signal processing, it corresponds to the resonant frequency of the vocal tract when the glottis is fully closed or fully opened.</p><p>)e recurrence relation of the transfer function can be obtained by the Levinson-Durbin algorithm,</p><formula xml:id="formula_14">A p+1 (z) � A p (z) -k p+1 z -(P+1) A p z -1 􏼐 􏼑.<label>(8)</label></formula><p>Among them, the reflection coefficients k p+1 � 1 and k p � -1 correspond to the boundary conditions when the glottal door is closed and opened, respectively, and P(z) and Q(z) are used to represent A p+1 (z) when k p+1 � 1 and k p � -1 are, respectively,</p><formula xml:id="formula_15">P(z) � A p (z) -z -(P+1) A p z -1 􏼐 􏼑,<label>(9)</label></formula><formula xml:id="formula_16">Q(z) � A p (z) + z -(P+1) A p z -1 􏼐 􏼑,<label>(10)</label></formula><formula xml:id="formula_17">A p (z) � 1 2 [P(z) + Q(z)]. (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>)e roots of equations ( <ref type="formula" target="#formula_15">9</ref>) and ( <ref type="formula" target="#formula_16">10</ref>) both lie on the unit circle of the z-plane and alternate, representing P(z) and Q(z) in the form of factorization, respectively,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><formula xml:id="formula_19">P(z) � 1 -z -1 􏼐 􏼑 􏽙 p/2 i�1 1 -2cosw i z -1 + z -2 􏼐 􏼑, Q(z) � 1 + z -1 􏼐 􏼑 􏽙 p/2 i�1 1 -2cosθ i z -1 + z -2 􏼐 􏼑,<label>(12)</label></formula><p>where roots w i and θ i satisfy</p><formula xml:id="formula_20">0 &lt; w 1 &lt; θ 1 &lt; . . . &lt; w p/2 &lt; θ p/2 &lt; π. (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>)e factorization coefficients w i and θ i appear in pairs and reflect the spectral resonance frequencies, so they are called line spectral pairs. Because of its good quantization and interpolation properties, it is widely used in the research on vocoders for speech coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Harmonic Dictionary.</head><p>Humans can hear sounds with frequencies between 20 and 20 KHz. )e objective description indicators of sound waves include frequency (fundamental frequency), harmonic components, sound pressure, and amplitude. Human perception of sound includes loudness, pitch, and timbre. Harmonic means that a signal can be decomposed into a fundamental frequency sine wave plus several other higher frequency sine waves, and each higher frequency is an integer multiple of the fundamental frequency. )e frequency components of these octaves are usually called overtones (Overtones), for example, the 2 times the frequency components of the fundamental frequency are called the second harmonic or the first overtone. Usually, the generation of harmonic signals is due to the resonance phenomenon when the excitation source passes through a resonant cavity. )e frequency of the excitation source corresponds to the fundamental frequency of the harmonic signal, and the resonance frequency of the resonant cavity is reflected in the frequency spectrum as the formant frequency. Taking speech as an example, the opening and closing cycle of the glottis determines the fundamental frequency, while the vocal tract as a resonant cavity determines the formant frequency. More commonly, many musical instruments such as violins, pianos, and guitars generate scales by resonating strings and resonating boxes with different vibration frequencies. )e vibration frequency of strings is determined by the length, thickness, and material of the strings. Instruments such as violins and guitars control their fundamental frequency by changing the length of the part of the string that can vibrate by pressing a finger on the string. Harmonic characteristics can be used to distinguish sound sources with and without resonant cavities. Sound sources with resonant cavities can generate harmonic signals, such as the resonant cavities of speech and music, which are the vocal tract and the resonance box, respectively; sound sources without resonant cavities produce nonharmonic signals, such as the sound of brakes and the sound of a river, and some researchers have proposed algorithms to distinguish the two types of signals. While DFT is capable of converting a time-domain audio signal to the frequency domain, this method of extracting frequencies is different from how humans perceive them. According to the research results of psychoacoustics, when people hear a sound with harmonic structure, they do not perceive the frequency of each single overtone in turn, but perceive the signal as a fundamental frequency as a whole. )e number of overtones, energy size, and overtone energy decay rate is perceived as timbres. )is perceptual fusion phenomenon is due to the brain's ability to use harmonic relationships to organize complex acoustic environments into independent acoustic targets. An intuitive example of this brain function is when two speakers have a difference in fundamental frequency, even if they speak at the same time, a person can easily distinguish two speakers.</p><p>Fundamental frequency refers to the greatest common divisor of each octave in a harmonic signal and is an objective measure. Pitch refers to the pitch perceived by people, which mainly depends on the fundamental frequency, intensity, and subjective feelings of people. )e fundamental frequency detection algorithm can be performed in the time domain or the frequency domain, usually using the method of directly finding the peak and trough positions, the autocorrelation function method, and the comb filter. )e methods in the frequency domain include cepstral method, maximum similarity method, and methods based on wavelet transform. Timbre involves many fields related to psychology and is related to human perception and various characteristics of sound, including the nature, material, and shape of the sound source, the number of overtones, the rate of energy decay, and the shape of the spectral envelope. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, the time-domain waveforms and power spectrograms of the clarinet and violin in a short time frame (16 kHz sampling rate, 32 ms frame length) are shown. )e overtone energy of the black tube attenuates quickly, the number of overtones is small, and its second formant appears around 1718 Hz. )e violin has more overtones, and the second formant appears around 3125 Hz. )ese factors together determine the timbre characteristics of the clarinet's low sound and the violin's clearer sound.</p><p>In this section, the signal spectrum is decomposed using the proposed harmonic dictionary and matching pursuit algorithm. Harmonic spectral component extraction technology is usually used in music signals, such as multifundamental frequency detection by decomposing the power spectrum of the signal into a series of base vectors.</p><p>)e power spectrum s of a short-duration audio frame can be represented as a linear combination of a set of basis vectors.</p><formula xml:id="formula_22">s � 􏽘 f max ,w,σ ( )∈A δ f max ,w,σ d f max ,w,σ + r. (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>Among them, d f max ,w,σ are the atom in the dictionary, δ f max ,w,σ are the scale corresponding to the atom, and the f max , w, σ parameters, respectively, represent the fundamental frequency, the center frequency, and the side lobe decay rate. A is the set of parameters for the selected atoms in the sparse representation, and r is the residual.</p><p>)e solution process of the sparse representation adopts an iterative process based on the matching pursuit algorithm. In the ith iteration, a basis vector d (f max ,w,σ) i is chosen, Mobile Information Systems multiplied by scale δ (f max ,w,σ) i and subtracted from the residual s i-1 of the spectrum.</p><formula xml:id="formula_24">s i � s i-1 -d f max ,w,σ ( ) iδ f max ,w,σ ( ) i . (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>)is iterative process involves two key issues: one is how to choose the basis vector d (f max ,w,σ) i , and the other is how to find its scale. For the first problem, since the harmonic structure is essential for audio classification, each harmonic basis vector can be represented as a linear sum of a set of instantaneous basis vectors. )erefore, the harmonic components should be extracted from the spectrum first, and then the nonharmonic components should be extracted. In addition, since the basis vector with the highest correlation with the residual spectrum reflects the most significant structure in the residual spectrum, in each iteration, the optimal basis vector should have a high correlation with the residual spectrum. )erefore, in each step of selection, a basis vector with a harmonic structure and a high correlation with the residual spectrum is selected.</p><p>To prevent the spectral aliasing phenomenon of the signals obtained after framing, there will be several repetition points in the two frames before and after the music signal, and only a small section in the middle is a different signal. At the same time, the signal needs to be windowed during the framing process. Moreover to prevent the occurrence of spectral aliasing, the window function can choose rectangular windows and Hamming windows. )e framing process is shown in Figure <ref type="figure" target="#fig_6">6</ref>.</p><p>A sparse representation of the audio spectrum can be obtained through the MP algorithm; that is, a set of selected basis functions and their scales are used to characterize the power spectrum of the original signal. Among them, the parameters of the basis function characterize the fundamental frequency, center frequency, and frequency multiplication attenuation rate of the spectrum, and the scale indicates the proportion of the basis function in the signal power spectrum. )us, the mean value of each parameter value weighted by the scale indicates the mean property of the signal power spectrum. In addition, the variance of each parameter can characterize the distribution range of each parameter. )erefore, by using the combination of the weighted mean and variance, the distribution of each parameter can be roughly depicted, reflecting the characteristics of the signal power spectrum.</p><p>It is assumed that the parameter set of the basis function selected by the sparse representation is (f max , w, σ) i , 1 ≤ i ≤ I, and I is the number of basis functions selected in the sparse representation. )e scale set is δ i , 1 ≤ i ≤ I. )en, the weighted parameter mean values are, respectively,</p><formula xml:id="formula_26">f max � 􏽐 I i�1 δ i f i max 􏽐 I i�1 δ i , w � 􏽐 I i�1 δ i w i 􏽐 I i�1 δ i , σ � 􏽐 I i�1 δ i σ i 􏽐 I i�1 δ i . (<label>16</label></formula><formula xml:id="formula_27">)</formula><p>)e variances of the parameters are, respectively,</p><formula xml:id="formula_28">Δf max � 1 I 􏽘 I i�1 f i max -f max 􏼐 􏼑 2 , Δw � 1 I 􏽘 I i�1 w i -w 􏼐 􏼑 2 , Δσ � 1 I 􏽘 I i�1 σ i -σ 􏼐 􏼑 2 . (<label>17</label></formula><formula xml:id="formula_29">)</formula><p>)e audio feature vector is represented as [f max , w, σ, Δf max , Δw, Δσ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Extraction Experiment</head><p>)is chapter mainly conducts simulation, verification, and analysis on the application performance of different classification models in musical instrument classification scenarios. Using the feature parameter extraction method in the previous chapter, the timbre feature parameter set of the existing music data source is extracted. Combined with pattern recognition technology, different classifiers are used for classification, training, and cross-validation of timbre feature parameter sets. And it compares and analyzes the classification results of different classification models, compares the advantages and disadvantages of each classifier, and lays the algorithm foundation for the design and implementation of the timbre analysis system. )e overall flow of simulation analysis is shown in Figure <ref type="figure" target="#fig_5">7</ref>.</p><p>After preparing the data source for the experimental simulation, first use the content of Chapter 2 to extract the timbre feature parameters of the data source in MATLAB and then convert the extracted timbre feature parameter  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><p>vector into the arf format that can be recognized by the Weka data mining tool text file. Based on the Weka platform, the feature parameter vector is classified, and some test data are provided to calculate the accuracy of the classification results. Finally, it is necessary to compare and analyze the classification results, summarize the advantages and disadvantages of different classification models, and lay the algorithm foundation for the design and implementation of the timbre analysis system in the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Tone Feature Data Collection.</head><p>To obtain comprehensive and accurate simulation results, the data sources used as learning samples and test samples in the simulation come from the Internet, recording equipment and musical instrument sound effects synthesized by software. All pieces are solo pieces played by a single instrument. To cover a wide range of music, this experiment selected eight instruments as sound sources, four of which were Western instruments: piano, violin, saxophone, and guitar; four oriental instruments are pipa, guzheng, flute, and erhu. In addition, among the eight musical instruments, violin, piano, guitar, pipa, guzheng, and erhu are percussion instruments, while flute and erhu are wind instruments, so that we can compare and analyze the timbre of musical instruments with similar pronunciation principles.</p><p>In the selection of music repertoire, the influence of performance style is also taken into account, and a considerable number of repertoires with fast, medium, and slow rhythms are selected for the repertoire played by each type of   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><p>instrument. In addition, to simplify the experimental data, this paper does not strictly distinguish the test data and the training learning samples, but inputs all data into the Weka tool platform for cross-validation.</p><p>)e audio file format selected in this article is the audio in MP3 format, which is the most common audio file and has a suitable size. In addition, due to the limitation of practical conditions, this paper adopts the method of dividing a track into music segments ranging from 10 s to 30 s to expand the database, which has two advantages: one is to verify the effect of playing time on the classification results; the other is to eliminate the influence of other factors except timbre on the experiment. Of course, the method of segmenting music may have a certain impact on the accuracy of the experimental results, so there may be a certain deviation between the data obtained by the classification accuracy rate and the actual accuracy rate at the end. However, this does not affect the horizontal comparison between the various classifiers, nor does it affect the algorithm selection of the timbre analysis system design.</p><p>When slicing music files, it is necessary to eliminate the silent sections in the track, because these silent sections do not contain data, so it does not make any sense for this experiment. )ese silent segments need to be filtered, and this experiment uses endpoint detection to eliminate silent segments. )e specific processing process of the endpoint detection algorithm is shown in Figure <ref type="figure" target="#fig_9">8</ref>. After deleting the silent segment, we use the MP3 cutting tool to cut the audio, and every 10 s-30 s there is a music segment. After cutting, the timbre, feature extraction can be performed.</p><p>A total of 950 pieces of music were used in this experiment, each piece being a solo piece for one instrument. A total of 8 musical instruments were collected in the experiment, including 4 Western musical instruments: piano, violin, saxophone, and guitar; 4 oriental musical instruments are pipa, guzheng, erhu, and dizi. Among the 950 pieces of music, there are 98 pieces for flute, 158 pieces for erhu, 122 pieces for piano, 122 pieces for guzheng, 100 pieces for guitar, 146 pieces for pipa, 132 pieces for saxophone, and 72 pieces for violin. )e data distribution is shown in Figure <ref type="figure">9</ref>.</p><p>)e simulation analysis is carried out on the Weka data mining platform. )e minimum distance classifier, decision tree classifier, SVM classifier, and BP neural network implemented by Weka are used to train and learn the timbre feature parameter vector set to establish different classification models. )e analysis method of the classification model is carried out by means of cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Simulation Based on Weka.</head><p>Weka is a completely opensource data mining work platform, designed and implemented by the University of Waikato based on Java language. As a data mining work platform, Weka collects a large number of machine learning algorithms capable of data mining tasks, which can be used directly or invoked in their own Java code. Weka includes tools for data preprocessing, classification, regression, clustering, rule association, and visualization. Weka is also suitable for secondary development of machine learning algorithms on its basis.</p><p>)e datasets that Weka can handle are datasets in the form of two-dimensional tables. )e row of the two-dimensional table represents the instance. In this experiment, one row represents the timbre feature vector (29 dimensions) of a piece of music; the columns of the two-dimensional table represent attributes, and Weka mines the relationship between attributes. )e file format that Weka can handle is ARFF (Attribute-Relation File Format) file. In this experiment, after the timbre feature parameter vector set is extracted, it needs to be converted into the corresponding ARFF file that Weka can recognize.</p><p>)e timbre feature parameter of a piece of music is a 29-dimensional feature vector, so when it is converted into an ARFF file, each feature vector has a total of 30 columns, that is, 30 attributes, because it is necessary to add a one-dimensional column of instruments to the feature vector. In this way, the ARFF file finally generated in this experiment is a 950 * 30 two-dimensional dataset. )e latest Weka3-6 requires running in a JVM with jdkl. 7 or above, and a Java environment needs to be configured before simulation with Weka.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Summary of Simulation Results</head><p>. )e experimental simulation of training and learning is carried out on the musical instrument timbre dataset, and four different audio feature extraction methods are used to classify musical instrument timbres, namely, MFCC and spectral features (SF). )e HCE features proposed in this paper and the spliced MFCC + HCE features are denoted as A, B, C, D for convenience in the text. )e two groups of feature parameters with different dimensions are compared and analyzed. Group A is the sample containing only the MFCC parameters, and group B is the sample containing all feature parameters. )e simulation results show that the classification accuracy of group B data is higher than that of group A, which shows that the parameters selected in this paper are all effective feature parameters. In this section, the simulation results of group B will be analyzed in detail. )e accuracy of the cross-validation results of the classifiers implemented by different algorithms is shown in Table <ref type="table" target="#tab_10">3</ref>.</p><p>Analyzing the overall classification situation, it can be seen from Figure <ref type="figure" target="#fig_10">10</ref> that the classification accuracy of the four feature extractors is above 92%. Among them, B has the worst effect, with an accuracy rate of 92.42%, while D has the best classification effect, with an accuracy rate of 99.15%. For </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><p>a single instrument, all 100 guitar samples are correctly classified in the four classifiers, indicating that the guitar timbre has obvious characteristics and is easy to distinguish; there are a few errors in the classification of piano and flute timbres in the four classifiers, as can be seen from the confusion matrix of each classification result in the previous section. Most of the wrong types of piano timbres are classified as guzheng, and the wrong classification of guzheng is also classified as piano, which shows that the timbres of piano and guzheng are relatively similar and easy to be confused. )e error situation of the flute is more complicated, indicating that the sound of the flute and valid information have not been found. )e violin and saxophone are also easy to be confused, indicating that the tone of the violin and the saxophone has something in common, and the two can bring a similar auditory experience. Different feature extraction models have different accuracy rates, which shows that the selection of the classification model has a great influence on the classification results. As can be seen from Table <ref type="table" target="#tab_10">3</ref> and<ref type="table">Figure</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head><p>best classification effect, with an accuracy rate of 99.15%, but its time consumption is relatively long, while the second algorithm A has certain advantages in time consumption. For these two classification algorithms, after a trade-off comparison, the classifier implemented by the D algorithm is the most suitable for timbre classification.</p><p>)ere are two main reasons: first, although the time overhead of the MFCC + HCE feature extraction algorithm is relatively small, its K value is an empirical value, and there is currently no scientific method for selecting the K value. In the process of training and learning, repeated parameter adjustment is required to ensure the accuracy of the classification model, and the C algorithm is not practical when the number of classification categories increases. Although the MFCC + HCE feature extraction algorithm takes a long time, the parameter adjustment of this algorithm is mainly to adjust the weight of the feature parameter vector itself, so it has a similar value method for the same mode, which is more stable than the C algorithm. Second, the generation of the classification model itself can be calculated as an offline method, the result is more important than the process, and the offline calculation is not sensitive to the time overhead. Considering the above two points, the classifier implemented by the MFCC + HCE feature extraction algorithm is the most suitable for musical instrument classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E T R A C T E D R E T R A C T E D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Musical instrument recognition is one of the important issues in the field of audio information retrieval, which mainly includes feature extraction and classification algorithms. From the perspective of timbre, this paper studies the identification of Western musical instruments. )e main work can be summarized as follows: this paper introduces four kinds of mathematical models commonly used in musical sound signal research. )e excitation source filter model is based on the sounding mechanism of the musical instrument, and the musical tone signal is modeled as the convolution of the excitation signal and the resonant body, which greatly simplifies the complex musical tone performance process. )e sine-plus-noise model utilizes the spectrum analysis characteristics of human hearing and regards the spectrum of the musical sound signal as the superposition of the sine component and the noise component. )e FM-AM model successfully describes the subtle changes in musical tone during performance. After summarizing and analyzing the existing models, this paper finds that an appropriate mathematical model can provide not only a concise and intuitive description for musical tone signals, but also an important basis for timbre research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: )e principle of human hearing.</figDesc><graphic coords="4,228.40,178.51,96.31,105.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Volume, tone, and timbre. (a) Volume level. (b) Pitch high and low. (c) )e timbre is different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Linear prediction model for speech production.</figDesc><graphic coords="7,312.86,77.11,65.87,57.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Black pipe and violin waveforms. (a) Black pipe time-domain waveform. (b) Violin time-domain waveform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overall flowchart of experimental simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Framing process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc>A) Preprocess the music signal. )e specific process is introduced in detail in the second chapter, namely, noise reduction, frame separation, etc. (B) Calculate the short-term energy E of each frame. (C) Calculate the zero-crossing rate (ZCR) of each frame. (D) Set the threshold of short-term energy and ZCR, and the short-term energy and ZCR exceeding the threshold value are judged as valid signals. (E) Delete the silent segment to obtain the output audio signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>10 ,Figure 9 :</head><label>109</label><figDesc>Figure 9: Distribution map of timbre feature dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Silence removal process.</figDesc><graphic coords="12,313.27,199.74,60.93,80.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Accuracy achieved by different feature extraction algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Classification of Western musical instruments.</figDesc><table><row><cell>Stringed instrument</cell><cell>Bowed string instrument Plucked instruments</cell><cell>Violin, viola, cello, double bass Harp</cell></row><row><cell>Wind instrument</cell><cell>Woodwind Brass instruments</cell><cell>Flute, clarinet, oboe, bassoon Trumpet, horn, trombone, large</cell></row><row><cell>Percussion</cell><cell>Has a fixed pitch No fixed pitch</cell><cell>Timpani, xylophone, carillon, row bell, etc. Snare drum, bass drum, triangle, cymbal, tambourine, gong, etc.</cell></row><row><cell>Keyboard instrument</cell><cell>-</cell><cell>Piano, celesta</cell></row><row><cell>Electric musical instrument</cell><cell>-</cell><cell>Electronic organ, electric piano, etc.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>National instruments.Figure 3: Some ethnic and Western musical instruments.</figDesc><table><row><cell>Mobile Information Systems</cell><cell></cell><cell>5</cell></row><row><cell></cell><cell>tribal instrument</cell><cell>western musical instruments</cell></row><row><cell></cell><cell>Whistleless musical instrument</cell><cell>Flute, pan flute, etc.</cell></row><row><cell>Wind instrument</cell><cell>Whistle instrument</cell><cell>Pipe, suona</cell></row><row><cell></cell><cell>Reed instrument</cell><cell>Sheng</cell></row><row><cell>Stringed instrument</cell><cell>-</cell><cell>Gaohu, jinghu, erhu, banhu, zhonghu, zuihu, etc.</cell></row><row><cell></cell><cell>Play musical instrument</cell><cell>Pipa, ruan, sanxian, yueqin, etc.</cell></row><row><cell>Plucked instruments</cell><cell>Flat musical instrument</cell><cell>Zither, guqin</cell></row><row><cell></cell><cell>Stringed instrument</cell><cell>Dulcimer</cell></row><row><cell></cell><cell>Drums</cell><cell>Drums, war drums, waist drums, long drums, etc.</cell></row><row><cell>Percussion</cell><cell>Gong class Cymbals</cell><cell>Big gong, small gong, cloud gong, etc. Large cymbals, small cymbals, cymbals, etc.</cell></row><row><cell></cell><cell>Bang class</cell><cell>Board, clapper, wooden fish, etc.</cell></row></table><note><p>9071, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2022/1349935, Wiley Online Library on [04/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2022/1349935, Wiley Online Library on [04/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</figDesc><table><row><cell>8</cell><cell>Mobile Information Systems</cell><cell>9071, 2022, 1,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2022/1349935, Wiley Online Library on [04/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</figDesc><table><row><cell>Mobile Information Systems</cell><cell>9</cell><cell>9071, 2022, 1,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2022/1349935, Wiley Online Library on [04/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</figDesc><table><row><cell>10</cell><cell>Mobile Information Systems</cell><cell>9071, 2022, 1,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Mobile Information Systems 11 9071, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2022/1349935, Wiley Online Library on [04/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Implementation of different feature extraction algorithms. Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2022/1349935, Wiley Online Library on [04/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</figDesc><table><row><cell>12</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mobile Information Systems</cell><cell>9071, 2022, 1,</cell></row><row><cell>Musical instrument</cell><cell></cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell></row><row><cell>Flute</cell><cell>Correct Mistake</cell><cell>91 7</cell><cell>86 12</cell><cell>89 9</cell><cell>94 4</cell></row><row><cell>Erhu</cell><cell>Correct Mistake</cell><cell>154 4</cell><cell>153 5</cell><cell>155 3</cell><cell>158 0</cell></row><row><cell>Piano</cell><cell>Correct Mistake</cell><cell>116 6</cell><cell>110 12</cell><cell>111 11</cell><cell>118 4</cell></row><row><cell>Guzheng</cell><cell>Correct Mistake</cell><cell>121 1</cell><cell>112 10</cell><cell>111 11</cell><cell>122 0</cell></row><row><cell>Guitar</cell><cell>Correct Mistake</cell><cell>100 0</cell><cell>100 0</cell><cell>100 0</cell><cell>100 0</cell></row><row><cell>Lute</cell><cell>Correct Mistake</cell><cell>144 2</cell><cell>123 23</cell><cell>137 9</cell><cell>146 0</cell></row><row><cell>Saxophone</cell><cell>Correct Mistake</cell><cell>132 0</cell><cell>128 4</cell><cell>132 0</cell><cell>132 0</cell></row><row><cell>Violin</cell><cell>Correct Mistake</cell><cell>66 6</cell><cell>66 6</cell><cell>72 0</cell><cell>72 0</cell></row><row><cell>Summary</cell><cell>Correct Mistake</cell><cell>924 26</cell><cell>878 72</cell><cell>279 141</cell><cell>942 8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>9071, 2022, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2022/1349935, Wiley Online Library on [04/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>Data sharing is not applicable to this article as no datasets were generated or analyzed during the current study.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>)e authors declare that they have no conflicts of interest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile Information</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Music Timbre Extracted from Audio Signal Features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<idno>ID 1349935</idno>
	</analytic>
	<monogr>
		<title level="j">Mobile Information Systems</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A single predominant instrument recognition of polyphonic music using CNN-based timbre analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engineering &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="590" to="593" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent timbre synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bisig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasquier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel study on perception cognition scenario in music using deterministic and non-deterministic approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nag</surname></persName>
		</author>
		<idno>ID 125682</idno>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and Its Applications</title>
		<imprint>
			<biblScope unit="volume">567</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of granular acousmatic music: representation of sound flux and emergence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rossetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Manzolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organised Sound</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="216" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Benchmarking audio signal representation techniques for classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berkovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3434</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Preface: special section: advances in speech, music and audio signal processing (articles 1-13)</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="294" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">For audio monitoring, seeing is believing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tv Technology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="12" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Doppler audio signal analysis as an additional tool in Evaluation of umbilical artery circulation</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Källén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Brännström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maršál</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultraschall in der Medizin</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="549" to="555" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Music similarity and retrieval: an introduction to audio-and web-based strategies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Reviews</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">270</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hybrid controller for mid-power audio application</title>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sensarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Power Electronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1200" to="1207" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feasibility research on break-out detection using audio signal in drilling film cooling holes by EDM</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia CIRP</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="566" to="571" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simulation of ocean surface temperature based on audio signal collection and accuracy of trade English translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhihao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arabian Journal of Geosciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Minding the gap: conceptualizing &quot;perceptualized&quot; timbre in music analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Engebretsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Leonardo Music Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="17" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human dissimilarity ratings of musical instrument timbre: a computational meta-analysis</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caramiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Depalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcadams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1745" to="1746" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analysis of timbre comparison between jeongak daegeum and sanjo daegeum</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Korea Entertainment Industry Association</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A corpus analysis of timbre semantics in orchestration treatises</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wallmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Music</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="585" to="605" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Musical perception assessment of people with hearing impairment: a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Simes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lüders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M D</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Audiology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On timbre in urban soundscapes: the role of fountains</title>
		<author>
			<persName><forename type="first">L</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Hermand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>D'autilia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploration of perceptual differences of virtually enhanced sound fields using timbre toolbox</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2499" to="2500" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tone timbre as hearing quality: visualization OF the condition and dynamic pattern</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Minaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Yashin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science and Innovations in Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vibro-acoustic modeling, numerical and experimental study of the resonator and its contribution to the timbre of Sarasvati veena, a South Indian stringed instrument</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Singru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vathsan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="540" to="555" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An empirical study of the timbre differences between gut core and metal core violin &quot;A&quot; strings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Homer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American String Teacher</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="36" to="37" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
