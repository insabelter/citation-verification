import pandas as pd
from IPython.display import display
from scipy.stats import fisher_exact
import numpy as np
import sys
import os

# Add the parent directory to the path to import from scripts
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from df_calculations import get_attribute_value_groups, eval_per_attribute_value

# ----------- Fisher's Exact Test for All Attribute Groups -----------
def _perform_fisher_exact_test_overall(correct_counts, false_counts, attribute_values):
    """
    Helper function to perform Fisher's exact test on a contingency table with multiple groups.
    
    Args:
        correct_counts: List of correct prediction counts for each attribute value
        false_counts: List of false prediction counts for each attribute value
        attribute_values: List of attribute values corresponding to the counts
    
    Returns:
        Dictionary with Fisher's exact test results or error information
    """
    if len(correct_counts) >= 2 and any(false_counts) and any(correct_counts):
        # Create contingency table with rows as outcomes (correct/false) and columns as groups
        contingency_table = [correct_counts, false_counts]
        
        try:
            # Fisher's exact test can handle larger contingency tables
            statistic, p_value = fisher_exact(contingency_table)
            
            return {
                'odds_ratio' if len(correct_counts) == 2 else 'statistic': float(round(statistic, 4)) if statistic != float('inf') else float('inf'),
                'p_value': float(round(p_value, 4)),
                'n_groups': len(attribute_values)
            }
        except ValueError as e:
            return {'error': str(e)}
    
    return None

def calc_fisher_exact_overall_total_sub_unsub(attribute_results, attribute_values):
    """
    Performs Fisher's exact tests on contingency tables for Total, Substantiated, and Unsubstantiated predictions
    across different attribute values. This performs an overall comparison across all attribute values,
    similar to the chi-squared test functions.
    
    Args:
        attribute_results: Dictionary with structure {attribute_value: {eval_predictions results...}}
        attribute_values: List of attribute values to include in the test
    
    Returns:
        Dictionary with Fisher's exact test results for Total, Substantiated, and Unsubstantiated
    """
    results = {}
    
    if len(attribute_values) < 2:
        return results  # Need at least 2 groups for Fisher's exact test
    
    # Prepare data for Total dataset (combining both labels)
    total_correct = []
    total_false = []
    
    # Prepare data for Substantiated label
    sub_correct = []
    sub_false = []
    
    # Prepare data for Unsubstantiated label  
    unsub_correct = []
    unsub_false = []
    
    # Extract data for each attribute value
    for attr_value in attribute_values:
        if attr_value in attribute_results:
            results_for_value = attribute_results[attr_value]
            
            # Total correct and false predictions (combining both labels)
            total_correct_count = (results_for_value['Substantiated']['True Classifications'] + 
                                 results_for_value['Unsubstantiated']['True Classifications'])
            total_false_count = (results_for_value['Substantiated']['False Classifications'] + 
                               results_for_value['Unsubstantiated']['False Classifications'])
            
            total_correct.append(total_correct_count)
            total_false.append(total_false_count)
            
            # Substantiated predictions
            sub_correct.append(results_for_value['Substantiated']['True Classifications'])
            sub_false.append(results_for_value['Substantiated']['False Classifications'])
            
            # Unsubstantiated predictions
            unsub_correct.append(results_for_value['Unsubstantiated']['True Classifications'])
            unsub_false.append(results_for_value['Unsubstantiated']['False Classifications'])
    
    # Perform Fisher's exact tests using helper function
    total_result = _perform_fisher_exact_test_overall(total_correct, total_false, attribute_values)
    if total_result:
        results['Total'] = total_result
    
    sub_result = _perform_fisher_exact_test_overall(sub_correct, sub_false, attribute_values)
    if sub_result:
        results['Substantiated'] = sub_result
    
    unsub_result = _perform_fisher_exact_test_overall(unsub_correct, unsub_false, attribute_values)
    if unsub_result:
        results['Unsubstantiated'] = unsub_result
    
    return results

def display_fisher_exact_overall_test_results(significance_results):
    """
    Display Fisher's exact test results (overall comparison) in a formatted table with color-coded p-values.
    
    Args:
        significance_results: Dictionary with structure {category: {test_results...}} 
                             where category is 'Total', 'Substantiated', or 'Unsubstantiated'
    
    Returns:
        pandas.DataFrame: The formatted results table
    """
    # Get Total results
    total_stat_value = "N/A"
    total_p_value = "N/A"
    total_stat_name = "Statistic"  # Default name
    if 'Total' in significance_results and 'error' not in significance_results['Total']:
        if 'odds_ratio' in significance_results['Total']:
            total_stat_value = f"{significance_results['Total']['odds_ratio']:.4f}" if significance_results['Total']['odds_ratio'] != float('inf') else "inf"
            total_stat_name = "Odds Ratio"
        elif 'statistic' in significance_results['Total']:
            total_stat_value = f"{significance_results['Total']['statistic']:.4f}" if significance_results['Total']['statistic'] != float('inf') else "inf"
            total_stat_name = "Statistic"
        total_p_value = f"{significance_results['Total']['p_value']:.4f}"
    
    # Get Unsubstantiated results
    unsub_stat_value = "N/A"
    unsub_p_value = "N/A"
    unsub_stat_name = "Statistic"  # Default name
    if 'Unsubstantiated' in significance_results and 'error' not in significance_results['Unsubstantiated']:
        if 'odds_ratio' in significance_results['Unsubstantiated']:
            unsub_stat_value = f"{significance_results['Unsubstantiated']['odds_ratio']:.4f}" if significance_results['Unsubstantiated']['odds_ratio'] != float('inf') else "inf"
            unsub_stat_name = "Odds Ratio"
        elif 'statistic' in significance_results['Unsubstantiated']:
            unsub_stat_value = f"{significance_results['Unsubstantiated']['statistic']:.4f}" if significance_results['Unsubstantiated']['statistic'] != float('inf') else "inf"
            unsub_stat_name = "Statistic"
        unsub_p_value = f"{significance_results['Unsubstantiated']['p_value']:.4f}"
    
    # Get Substantiated results
    sub_stat_value = "N/A"
    sub_p_value = "N/A"
    sub_stat_name = "Statistic"  # Default name
    if 'Substantiated' in significance_results and 'error' not in significance_results['Substantiated']:
        if 'odds_ratio' in significance_results['Substantiated']:
            sub_stat_value = f"{significance_results['Substantiated']['odds_ratio']:.4f}" if significance_results['Substantiated']['odds_ratio'] != float('inf') else "inf"
            sub_stat_name = "Odds Ratio"
        elif 'statistic' in significance_results['Substantiated']:
            sub_stat_value = f"{significance_results['Substantiated']['statistic']:.4f}" if significance_results['Substantiated']['statistic'] != float('inf') else "inf"
            sub_stat_name = "Statistic"
        sub_p_value = f"{significance_results['Substantiated']['p_value']:.4f}"
    
    # Use the most specific stat name available (prioritize "Odds Ratio" if any category has it)
    stat_column_name = "Odds Ratio" if any(name == "Odds Ratio" for name in [total_stat_name, unsub_stat_name, sub_stat_name]) else "Statistic"
    
    # Create single row of data
    table_data = [[
        total_stat_value,
        total_p_value,
        unsub_stat_value,
        unsub_p_value,
        sub_stat_value,
        sub_p_value
    ]]

    # Create DataFrame with dynamic column names
    columns = [f'Total {stat_column_name}', 'Total P-value',
               f'Unsubstantiated {stat_column_name}', 'Unsubstantiated P-value', 
               f'Substantiated {stat_column_name}', 'Substantiated P-value']
    df = pd.DataFrame(table_data, columns=columns)
    
    # Apply color styling to the DataFrame
    def color_p_values(val):
        """Color p-values based on significance level"""
        try:
            if val == "N/A":
                return ''
            p_val = float(val)
            if p_val <= 0.05:
                return 'background-color: darkgreen; color: white'
            elif p_val <= 0.1:
                return 'background-color: darkorange; color: white'
            else:
                return 'background-color: darkred; color: white'
        except (ValueError, TypeError):
            return ''
    
    # Apply styling only to p-value columns
    styled_df = df.style.map(color_p_values, subset=['Total P-value', 'Unsubstantiated P-value', 'Substantiated P-value'])
    display(styled_df)
    
    return df

# ------------ Permutation Test -------------
def _extract_metric_from_results(results, metric_path):
    """
    Helper function to extract a specific metric from eval_predictions results.
    
    Args:
        results: Results dictionary from eval_predictions_per_attribute_value
        metric_path: List describing path to metric (e.g., ['Accuracy'] or ['Substantiated', 'Recall'])
    
    Returns:
        Metric value or None if not found
    """
    metric_value = results
    for key in metric_path:
        if key in metric_value:
            metric_value = metric_value[key]
        else:
            return None
    return metric_value

def calc_permutation_test_total_sub_unsub(df, attribute, group_numbers_from=False, n_permutations=1000, seed=42):
    """
    Performs two-sided permutation tests for the difference of classification metric values between groups.
    Efficiently calculates all metrics (Total, Substantiated, Unsubstantiated) in a single permutation loop.
    
    Args:
        df: DataFrame with prediction results
        attribute: Column name of the attribute to shuffle
        group_numbers_from: Whether to group attribute values by their numeric part
        n_permutations: Number of permutations to perform
        seed: Random seed for reproducible results
    
    Returns:
        Dictionary with all permutation test results for Total, Substantiated, and Unsubstantiated metrics
    """
    # Set random seed if provided
    if seed is not None:
        np.random.seed(seed)
    
    # Calculate observed results per attribute value
    attribute_groups = get_attribute_value_groups(df, attribute, group_numbers_from)
    observed_results = eval_per_attribute_value(df, attribute, attribute_groups)
    attribute_values = [val for val in observed_results.keys() if val != 'Total']
    
    if len(attribute_values) < 2:
        return {}
    
    # Define all metrics to test
    metrics_to_test = {
        'Total': {
            'Accuracy': ['Accuracy'],
            'Balanced Accuracy': ['Balanced Accuracy']
        },
        'Substantiated': {
            'Precision': ['Substantiated', 'Precision'],
            'Recall': ['Substantiated', 'Recall'],
            'F1 Score': ['Substantiated', 'F1 Score']
        },
        'Unsubstantiated': {
            'Precision': ['Unsubstantiated', 'Precision'],
            'Recall': ['Unsubstantiated', 'Recall'],
            'F1 Score': ['Unsubstantiated', 'F1 Score']
        }
    }
    
    # Extract observed values for all metrics
    observed_metrics = {}
    for category, metric_dict in metrics_to_test.items():
        observed_metrics[category] = {}
        for metric_name, metric_path in metric_dict.items():
            observed_values = []
            for attr_value in attribute_values:
                if attr_value in observed_results:
                    metric_value = _extract_metric_from_results(observed_results[attr_value], metric_path)
                    if metric_value is not None:
                        observed_values.append(metric_value)
                    else:
                        observed_values = None
                        break
            
            if observed_values is not None:
                observed_diff = np.var(observed_values)
                observed_metrics[category][metric_name] = {
                    'observed_values': observed_values,
                    'observed_difference': observed_diff,
                    'permuted_differences': [],
                    'extreme_count': 0
                }
    
    # Perform permutations once and calculate all metrics
    df_copy = df.copy()
    
    for _ in range(n_permutations):
        # Shuffle the attribute values once
        shuffled_attributes = np.random.permutation(df_copy[attribute].values)
        df_copy[attribute] = shuffled_attributes
        
        # Recalculate metrics with shuffled attributes once
        permuted_attribute_groups = get_attribute_value_groups(df_copy, attribute, group_numbers_from)
        permuted_results = eval_per_attribute_value(df_copy, attribute, permuted_attribute_groups)
        
        # Extract all metrics from this single permutation
        for category, metric_dict in observed_metrics.items():
            for metric_name, metric_data in metric_dict.items():
                metric_path = metrics_to_test[category][metric_name]
                
                # Extract permuted metric values
                permuted_values = []
                for attr_value in attribute_values:
                    if attr_value in permuted_results:
                        metric_value = _extract_metric_from_results(permuted_results[attr_value], metric_path)
                        if metric_value is not None:
                            permuted_values.append(metric_value)
                        else:
                            permuted_values.append(0)  # Default if metric not found
                
                # Calculate permuted difference
                permuted_diff = np.var(permuted_values)
                
                # Store permuted difference
                metric_data['permuted_differences'].append(permuted_diff)
                
                # Count extreme values (two-sided test)
                if permuted_diff >= metric_data['observed_difference']:
                    metric_data['extreme_count'] += 1
    
    # Calculate final results for all metrics
    results = {}
    for category, metric_dict in observed_metrics.items():
        if metric_dict:  # Only add category if it has valid metrics
            results[category] = {}
            for metric_name, metric_data in metric_dict.items():
                p_value = metric_data['extreme_count'] / n_permutations
                average_difference = np.mean(metric_data['permuted_differences'])
                
                results[category][metric_name] = {
                    'p_value': p_value,
                    'observed_variance': float(round(metric_data['observed_difference'], 4)),
                    'average_variance': float(round(average_difference, 4)),
                    'difference of variances': float(round(metric_data['observed_difference'], 4) - float(round(average_difference, 4))),
                    'n_permutations': n_permutations,
                    'n_groups': len(attribute_values),
                    'observed_values': metric_data['observed_values']
                }
    
    return results

def display_permutation_test_results(significance_results):
    """
    Display permutation test results in a formatted table with color-coded p-values.
    
    Args:
        significance_results: Dictionary with structure {category: {metric_name: {test_results...}}}
                             where category is 'Total', 'Substantiated', or 'Unsubstantiated'
    
    Returns:
        pandas.DataFrame: The formatted results table
    """
    # Define the desired row order
    metric_order = ['Balanced Accuracy', 'Accuracy', 'Precision', 'Recall', 'F1 Score']
    
    # Collect all unique metric names across all categories
    all_metrics = set()
    for category_results in significance_results.values():
        for metric_name in category_results.keys():
            all_metrics.add(metric_name)
    
    # Use only metrics that exist in our data, in the specified order
    sorted_metrics = [metric for metric in metric_order if metric in all_metrics]
    
    # Create table data
    table_data = []
    
    for metric_name in sorted_metrics:
        row = [metric_name]
        
        # Add Total columns (Var Diff and P-value)
        if 'Total' in significance_results and metric_name in significance_results['Total']:
            total_results = significance_results['Total'][metric_name]
            row.append(f"{total_results['difference of variances']:.6f}")
            row.append(f"{total_results['p_value']:.4f}")
        else:
            row.extend(["N/A", "N/A"])
        
        # Add Unsubstantiated columns (Var Diff and P-value)
        if 'Unsubstantiated' in significance_results and metric_name in significance_results['Unsubstantiated']:
            unsub_results = significance_results['Unsubstantiated'][metric_name]
            row.append(f"{unsub_results['difference of variances']:.6f}")
            row.append(f"{unsub_results['p_value']:.4f}")
        else:
            row.extend(["N/A", "N/A"])
        
        # Add Substantiated columns (Var Diff and P-value)
        if 'Substantiated' in significance_results and metric_name in significance_results['Substantiated']:
            sub_results = significance_results['Substantiated'][metric_name]
            row.append(f"{sub_results['difference of variances']:.6f}")
            row.append(f"{sub_results['p_value']:.4f}")
        else:
            row.extend(["N/A", "N/A"])
        
        table_data.append(row)
    
    # Create DataFrame
    columns = [
        'Metric',
        'Total Var Diff', 'Total P-value',
        'Unsubstantiated Var Diff', 'Unsubstantiated P-value',
        'Substantiated Var Diff', 'Substantiated P-value'
    ]
    df = pd.DataFrame(table_data, columns=columns)
    df.set_index('Metric', inplace=True)
    
    # Apply color styling to the DataFrame
    def color_p_values(val):
        """Color p-values based on significance level"""
        try:
            if val == "N/A":
                return ''
            p_val = float(val)
            if p_val <= 0.05:
                return 'background-color: darkgreen; color: white'
            elif p_val <= 0.1:
                return 'background-color: darkorange; color: white'
            else:
                return 'background-color: darkred; color: white'
        except (ValueError, TypeError):
            return ''
    
    # Apply styling only to p-value columns
    p_value_columns = ['Total P-value', 'Substantiated P-value', 'Unsubstantiated P-value']
    styled_df = df.style.map(color_p_values, subset=p_value_columns)
    display(styled_df)
    
    return df

# ------------ Extract only p-values from all results ------------
def extract_p_values(significance_tests_results):
    """
    Extract only p-values from the significance tests results dictionary.
    
    Parameters:
    significance_tests_results: Dictionary containing significance test results with structure:
                               {attribute: {test_type: {group/label: {metric: {results...}}}}}
    
    Returns:
    Dictionary with the same structure but containing only p-values
    """
    p_values_only = {}
    
    for attribute, tests in significance_tests_results.items():
        p_values_only[attribute] = {}
        
        # Extract Fisher Exact p-values
        if 'Fisher Exact' in tests:
            p_values_only[attribute]['Fisher Exact'] = {}
            for group, labels in tests['Fisher Exact'].items():
                p_values_only[attribute]['Fisher Exact'][group] = {}
                for label, results in labels.items():
                    p_values_only[attribute]['Fisher Exact'][group][label] = float(results['p_value'])

        # Extract Fisher Exact Overall p-values
        if 'Fisher Exact Overall' in tests:
            p_values_only[attribute]['Fisher Exact Overall'] = {}
            for label, results in tests['Fisher Exact Overall'].items():
                p_values_only[attribute]['Fisher Exact Overall'][label] = float(results['p_value'])

        # Extract Chi-Squared p-values
        if 'Chi-Squared' in tests:
            p_values_only[attribute]['Chi-Squared'] = {}
            for label, results in tests['Chi-Squared'].items():
                p_values_only[attribute]['Chi-Squared'][label] = float(results['p_value'])
        
        # Extract Permutation Test p-values
        if 'Permutation Test' in tests:
            p_values_only[attribute]['Permutation Test'] = {}
            for label, metrics in tests['Permutation Test'].items():
                p_values_only[attribute]['Permutation Test'][label] = {}
                for metric, results in metrics.items():
                    p_values_only[attribute]['Permutation Test'][label][metric] = float(results['p_value'])
    
    return p_values_only

def reorganize_p_values_by_test_type(significance_results):
    reorganized = {
        'Fisher Exact': {},
        'Fisher Exact Overall': {},
        'Chi-Squared': {},
        'Permutation Test': {}
    }
    
    # Process each attribute
    for attribute_name, tests in significance_results.items():
        
        # Process Fisher Exact tests - keep attribute name and values separate
        if 'Fisher Exact' in tests:
            for group_name, categories in tests['Fisher Exact'].items():
                for category, p_value in categories.items():
                    # Create hierarchy: category -> attribute_name -> group_name -> p_value
                    if category not in reorganized['Fisher Exact']:
                        reorganized['Fisher Exact'][category] = {}
                    if attribute_name not in reorganized['Fisher Exact'][category]:
                        reorganized['Fisher Exact'][category][attribute_name] = {}
                    reorganized['Fisher Exact'][category][attribute_name][group_name] = p_value
        
        # Process Fisher Exact Overall tests
        if 'Fisher Exact Overall' in tests:
            for category, p_value in tests['Fisher Exact Overall'].items():
                if category not in reorganized['Fisher Exact Overall']:
                    reorganized['Fisher Exact Overall'][category] = []
                reorganized['Fisher Exact Overall'][category].append((attribute_name, p_value))
                
        # Process Chi-Squared tests
        if 'Chi-Squared' in tests:
            for category, p_value in tests['Chi-Squared'].items():
                if category not in reorganized['Chi-Squared']:
                    reorganized['Chi-Squared'][category] = []
                reorganized['Chi-Squared'][category].append((attribute_name, p_value))
        
        # Process Permutation Test
        if 'Permutation Test' in tests:
            for category, metrics in tests['Permutation Test'].items():
                if category not in reorganized['Permutation Test']:
                    reorganized['Permutation Test'][category] = {}
                
                for metric, p_value in metrics.items():
                    if metric not in reorganized['Permutation Test'][category]:
                        reorganized['Permutation Test'][category][metric] = []
                    reorganized['Permutation Test'][category][metric].append((attribute_name, p_value))
    
    return reorganized

# ------------ P-Value Corrections ------------
def holm_p_value_correction(p_values, alpha=0.05):
    # p_values: List of tuples (test_name, p_value)
    sorted_p_values = sorted(p_values, key=lambda x: x[1])

    rejected_null_hypotheses = []
    accepted_null_hypotheses = []
    one_accepted = False
    for i, (test_name, p_value) in enumerate(sorted_p_values):
        n = len(p_values)
        adjusted_p_value = min((n - i) * p_value, 1.0)
        if not one_accepted and adjusted_p_value <= alpha:
            rejected_null_hypotheses.append((test_name, adjusted_p_value))
        else:
            one_accepted = True
            accepted_null_hypotheses.append((test_name, adjusted_p_value))
    return rejected_null_hypotheses, accepted_null_hypotheses

def print_holm_correction_results_as_table(rejected_null_hypotheses, accepted_null_hypotheses, results, label_set):
    # Combine all results into one DataFrame
    all_data = []
    
    # Add rejected hypotheses
    for test_name, adjusted_p_value in rejected_null_hypotheses:
        original_p_value = dict(results[label_set])[test_name]
        all_data.append({
            'Test Name': test_name,
            'Original p-value': original_p_value,
            'Adjusted p-value': adjusted_p_value,
            'Significant': 'Yes'
        })
    
    # Add accepted hypotheses
    for test_name, adjusted_p_value in accepted_null_hypotheses:
        original_p_value = dict(results[label_set])[test_name]
        all_data.append({
            'Test Name': test_name,
            'Original p-value': original_p_value,
            'Adjusted p-value': adjusted_p_value,
            'Significant': 'No'
        })
    
    # Create and display DataFrame
    if all_data:
        df = pd.DataFrame(all_data)
        # Format p-values to 4 decimal places
        df['Original p-value'] = df['Original p-value'].apply(lambda x: f"{x:.4f}")
        df['Adjusted p-value'] = df['Adjusted p-value'].apply(lambda x: f"{x:.4f}")
        
        # Style the DataFrame with background colors and white text
        def color_significant(val):
            if val == 'Yes':
                return 'background-color: darkgreen; color: white;'
            elif val == 'No':
                return 'background-color: darkred; color: white;'
            return ''
        
        styled_df = df.style.map(color_significant, subset=['Significant'])
        display(styled_df)
    else:
        print("No test results available")

def generate_p_value_results_table(data, attribute_order=None):
    """
    Generate a table from p-value results data.
    
    Parameters:
    - data: Dictionary with label sets as keys and attributes with original/adjusted p-values as values
    - attribute_order: List specifying the order of attributes in rows. If None, uses alphabetical order.
    
    Returns:
    - pandas DataFrame with attributes as rows and label sets with original/adjusted as columns
    """
    
    # Extract all unique attributes and label sets
    all_attributes = set()
    label_sets = list(data.keys())
    
    for label_set_data in data.values():
        all_attributes.update(label_set_data.keys())
    
    # Use provided order or sort alphabetically
    if attribute_order is None:
        attributes = sorted(all_attributes)
    else:
        # Use provided order and add any missing attributes at the end
        attributes = [attr for attr in attribute_order if attr in all_attributes]
        remaining = sorted(set(all_attributes) - set(attribute_order))
        attributes.extend(remaining)
    
    # Sort label sets in specific order: "Total", "Substantiated", "Unsubstantiated"
    desired_order = ["Total", "Substantiated", "Unsubstantiated"]
    sorted_label_sets = [label for label in desired_order if label in label_sets]
    # Add any remaining label sets not in the desired order
    remaining_labels = [label for label in label_sets if label not in desired_order]
    sorted_label_sets.extend(sorted(remaining_labels))
    
    # Create column names
    columns = []
    for label_set in sorted_label_sets:
        columns.extend([f"{label_set} Original", f"{label_set} Adjusted"])
    
    # Initialize the table
    table_data = []
    
    for attribute in attributes:
        row = []
        for label_set in sorted_label_sets:
            if attribute in data[label_set]:
                original = data[label_set][attribute]['original']
                adjusted = data[label_set][attribute]['adjusted']
                row.extend([original, adjusted])
            else:
                row.extend([None, None])
        table_data.append(row)
    
    # Create DataFrame
    df = pd.DataFrame(table_data, columns=columns, index=attributes)
    
    return df